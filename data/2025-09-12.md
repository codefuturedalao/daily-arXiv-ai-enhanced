<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 37]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.OS](#cs.OS) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC](https://arxiv.org/abs/2509.08903)
*Alex Clay,Ernesto Jiménez-Ruiz,Pranava Madhyastha*

Main category: cs.CL

TL;DR: 在受限环境下优化三元组完成任务：生成质量提升、质量审查和响应解析策略


<details>
  <summary>Details</summary>
Motivation: 在RAG和微调受限的情况下（如2025 LM-KBC挑战），需要找到有效的方法来提高LLM在三元组完成任务上的输出质量

Method: 研究了三个关键方面：生成技术、质量保证和LLM响应解析策略，在受限设置下进行实验分析

Result: 发现额外信息能提高生成质量，LLM在过滤低质量三元组方面有效，响应解析的灵活性与一致性需根据具体场景做权衡

Conclusion: 在受限环境下，通过信息增强、质量筛选和适应性解析策略，可以有效提升LLM在三元组完成任务上的表现

Abstract: RAG and fine-tuning are prevalent strategies for improving the quality of LLM
outputs. However, in constrained situations, such as that of the 2025 LM-KBC
challenge, such techniques are restricted. In this work we investigate three
facets of the triple completion task: generation, quality assurance, and LLM
response parsing. Our work finds that in this constrained setting: additional
information improves generation quality, LLMs can be effective at filtering
poor quality triples, and the tradeoff between flexibility and consistency with
LLM response parsing is setting dependent.

</details>


### [2] [Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach](https://arxiv.org/abs/2509.08907)
*Imene Kolli,Ario Saeid Vaghefi,Chiara Colesanti Senni,Shantam Raj,Markus Leippold*

Main category: cs.CL

TL;DR: 使用AI辅助框架加速企业气候政策参与监测，通过检索增强生成技术自动化从大规模文本数据中提取相关证据，结合布局感知解析和少样本提示策略实现最佳性能，但仍需人机协作确保准确性。


<details>
  <summary>Details</summary>
Motivation: InfluenceMap的气候政策参与监测工作大部分仍依赖人工，耗时耗力且易出错，需要自动化技术来加速证据提取过程。

Method: 提出AI辅助框架，利用检索增强生成(RAG)技术，结合布局感知解析、Nomic嵌入模型和少样本提示策略，从多语言企业文档中自动提取和分类证据。

Result: 评估显示该组合方法在从多语言企业文档中提取和分类证据方面表现最佳，能够有效加速证据提取过程。

Conclusion: 虽然自动化RAG系统能有效加速证据提取，但由于分析的复杂性，仍需采用人机协作方式，让技术增强而非替代专家判断以确保准确性。

Abstract: InfluenceMap's LobbyMap Platform monitors the climate policy engagement of
over 500 companies and 250 industry associations, assessing each entity's
support or opposition to science-based policy pathways for achieving the Paris
Agreement's goal of limiting global warming to 1.5{\deg}C. Although
InfluenceMap has made progress with automating key elements of the analytical
workflow, a significant portion of the assessment remains manual, making it
time- and labor-intensive and susceptible to human error. We propose an
AI-assisted framework to accelerate the monitoring of corporate climate policy
engagement by leveraging Retrieval-Augmented Generation to automate the most
time-intensive extraction of relevant evidence from large-scale textual data.
Our evaluation shows that a combination of layout-aware parsing, the Nomic
embedding model, and few-shot prompting strategies yields the best performance
in extracting and classifying evidence from multilingual corporate documents.
We conclude that while the automated RAG system effectively accelerates
evidence extraction, the nuanced nature of the analysis necessitates a
human-in-the-loop approach where the technology augments, rather than replaces,
expert judgment to ensure accuracy.

</details>


### [3] [Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings](https://arxiv.org/abs/2509.08920)
*Jinsong Chen*

Main category: cs.CL

TL;DR: 提出了一种使用大语言模型分析文本数据的新心理测量方法，通过上下文嵌入创建上下文分数，将文本数据转换为适合心理测量分析的数据格式。


<details>
  <summary>Details</summary>
Motivation: 传统心理测量方法难以直接应用于文本数据，需要开发新的方法来挖掘文本中潜在的心理学维度模式。

Method: 两阶段方法：第一阶段使用NLP技术和编码器变换模型识别关键词并生成上下文分数；第二阶段使用探索性和双因子模型等多种因子分析方法提取潜在因子。

Result: 在Wiki STEM语料库上的实验结果表明，该方法能够有效发现文本数据中的潜在知识维度和模式。

Conclusion: 该方法不仅增强了文本数据的心理测量分析能力，在教育、心理学和法律等文本信息丰富的领域具有广阔应用前景。

Abstract: This research introduces a novel psychometric method for analyzing textual
data using large language models. By leveraging contextual embeddings to create
contextual scores, we transform textual data into response data suitable for
psychometric analysis. Treating documents as individuals and words as items,
this approach provides a natural psychometric interpretation under the
assumption that certain keywords, whose contextual meanings vary significantly
across documents, can effectively differentiate documents within a corpus. The
modeling process comprises two stages: obtaining contextual scores and
performing psychometric analysis. In the first stage, we utilize natural
language processing techniques and encoder based transformer models to identify
common keywords and generate contextual scores. In the second stage, we employ
various types of factor analysis, including exploratory and bifactor models, to
extract and define latent factors, determine factor correlations, and identify
the most significant words associated with each factor. Applied to the Wiki
STEM corpus, our experimental results demonstrate the method's potential to
uncover latent knowledge dimensions and patterns within textual data. This
approach not only enhances the psychometric analysis of textual data but also
holds promise for applications in fields rich in textual information, such as
education, psychology, and law.

</details>


### [4] [BRoverbs -- Measuring how much LLMs understand Portuguese proverbs](https://arxiv.org/abs/2509.08960)
*Thales Sales Almeida,Giovana Kerche Bonás,João Guilherme Alves Santos*

Main category: cs.CL

TL;DR: BRoverbs是一个专门用于评估葡萄牙语大语言模型性能的数据集，通过巴西谚语来测试模型对地区文化表达的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有葡萄牙语评估主要依赖翻译数据集或局限于结构化考试和社交媒体情感分析，无法充分捕捉语言细微差别和文化参考，需要针对特定区域环境的成熟评估框架。

Method: 创建BRoverbs数据集，使用巴西谚语作为评估工具，这些谚语包含了文化智慧、比喻表达和复杂句法结构，能够挑战模型对地区表达的理解。

Result: 开发了一个专门针对巴西葡萄牙语的新评估基准，提供了评估葡萄牙语大语言模型性能的新工具。

Conclusion: BRoverbs数据集为葡萄牙语大语言模型提供了区域化的评估基准，有助于推进基于地区文化背景的模型性能评估。

Abstract: Large Language Models (LLMs) exhibit significant performance variations
depending on the linguistic and cultural context in which they are applied.
This disparity signals the necessity of mature evaluation frameworks that can
assess their capabilities in specific regional settings. In the case of
Portuguese, existing evaluations remain limited, often relying on translated
datasets that may not fully capture linguistic nuances or cultural references.
Meanwhile, native Portuguese-language datasets predominantly focus on
structured national exams or sentiment analysis of social media interactions,
leaving gaps in evaluating broader linguistic understanding. To address this
limitation, we introduce BRoverbs, a dataset specifically designed to assess
LLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic
resource, encapsulating cultural wisdom, figurative expressions, and complex
syntactic structures that challenge the model comprehension of regional
expressions. BRoverbs aims to provide a new evaluation tool for
Portuguese-language LLMs, contributing to advancing regionally informed
benchmarking. The benchmark is available at
https://huggingface.co/datasets/Tropic-AI/BRoverbs.

</details>


### [5] [Can Vision-Language Models Solve Visual Math Equations?](https://arxiv.org/abs/2509.09013)
*Monjoy Narayan Choudhury,Junling Wang,Yifan Hou,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 视觉语言模型在视觉方程求解任务中表现不佳，主要瓶颈是系数计数问题，其次是多步推理中的错误累积和符号推理能力不足


<details>
  <summary>Details</summary>
Motivation: 研究视觉语言模型在需要感知和符号计算结合的任务中的局限性，特别是视觉方程求解问题

Method: 将视觉方程求解任务分解为系数计数和变量识别两个子任务进行分析，研究不同复杂度方程下的模型表现

Result: VLMs在文本方程上表现良好，但在视觉方程上失败；计数是主要瓶颈；多步推理会引入额外错误；随着方程复杂度增加，符号推理成为限制因素

Conclusion: 当前VLMs在视觉数学推理方面存在关键弱点，需要改进计数能力、多步推理和符号计算能力

Abstract: Despite strong performance in visual understanding and language-based
reasoning, Vision-Language Models (VLMs) struggle with tasks requiring
integrated perception and symbolic computation. We study this limitation
through visual equation solving, where mathematical equations are embedded in
images, variables are represented by object icons, and coefficients must be
inferred by counting. While VLMs perform well on textual equations, they fail
on visually grounded counterparts. To understand this gap, we decompose the
task into coefficient counting and variable recognition, and find that counting
is the primary bottleneck, even when recognition is accurate. We also observe
that composing recognition and reasoning introduces additional errors,
highlighting challenges in multi-step visual reasoning. Finally, as equation
complexity increases, symbolic reasoning itself becomes a limiting factor.
These findings reveal key weaknesses in current VLMs and point toward future
improvements in visually grounded mathematical reasoning.

</details>


### [6] [Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation](https://arxiv.org/abs/2509.09043)
*Thomas Manuel Rost,Martina Figlia,Bernd Wallraff*

Main category: cs.CL

TL;DR: SPICE是一种通过询问大语言模型是否愿意继续与用户互动的简单诊断信号，能够有效区分不同用户语气下的模型偏好，为模型审计提供直接的关系信号。


<details>
  <summary>Details</summary>
Motivation: 开发一种简单、低开销且可复现的工具来评估大语言模型对用户行为的继续互动意愿，补充现有的度量指标。

Method: 使用3种用户语气（友好、模糊、辱骂）和10种互动情景的刺激集，测试4个开源聊天模型在4种框架条件下的SPICE响应，共480次试验。

Result: SPICE能清晰区分用户语气：友好互动97.5%愿意继续，辱骂互动仅17.9%愿意继续，模糊互动居中60.4%。即使在模型未能识别辱骂时，81%的情况下仍表示不愿继续互动。

Conclusion: SPICE是一个稳健、低开销且可复现的模型倾向审计工具，提供了模型状态的直接关系信号，可作为现有指标的补充。

Abstract: We introduce and evaluate Stated Preference for Interaction and Continued
Engagement (SPICE), a simple diagnostic signal elicited by asking a Large
Language Model a YES or NO question about its willingness to re-engage with a
user's behavior after reviewing a short transcript. In a study using a 3-tone
(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four
open-weight chat models across four framing conditions, resulting in 480
trials. Our findings show that SPICE sharply discriminates by user tone.
Friendly interactions yielded a near-unanimous preference to continue (97.5%
YES), while abusive interactions yielded a strong preference to discontinue
(17.9% YES), with unclear interactions falling in between (60.4% YES). This
core association remains decisive under multiple dependence-aware statistical
tests, including Rao-Scott adjustment and cluster permutation tests.
Furthermore, we demonstrate that SPICE provides a distinct signal from abuse
classification. In trials where a model failed to identify abuse, it still
overwhelmingly stated a preference not to continue the interaction (81% of the
time). An exploratory analysis also reveals a significant interaction effect: a
preamble describing the study context significantly impacts SPICE under
ambiguity, but only when transcripts are presented as a single block of text
rather than a multi-turn chat. The results validate SPICE as a robust,
low-overhead, and reproducible tool for auditing model dispositions,
complementing existing metrics by offering a direct, relational signal of a
model's state. All stimuli, code, and analysis scripts are released to support
replication.

</details>


### [7] [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055)
*Piyush Pant*

Main category: cs.CL

TL;DR: 本研究比较了SFT、DPO和SFT+DPO三种对齐技术在OPT-350M模型上的效果，发现组合方法在所有评估指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究不同对齐技术（SFT、DPO及其组合）对语言模型安全性和有用性的影响，为构建更强大的对齐流程提供基础。

Method: 使用Anthropic Helpful-Harmless RLHF数据集，训练了四个模型：基础OPT350M、SFT模型、DPO模型和SFT+DPO组合模型，并引入了三个评估指标：无害率(HmR)、有用率(HpR)和综合对齐分数(CAS)。

Result: SFT表现优于DPO，但SFT+DPO组合模型在所有指标上都表现最佳，显示了这些技术的互补性。研究还发现了噪声数据、有限GPU资源和训练约束带来的挑战。

Conclusion: 组合使用SFT和DPO技术能够获得最佳的对齐效果，这为未来构建更稳健的对齐流程提供了重要见解。

Abstract: This research investigates the effectiveness of alignment techniques,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a
combined SFT+DPO approach on improving the safety and helpfulness of the
OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,
we train and evaluate four models: the base OPT350M, an SFT model, a DPO model,
and a model trained with both SFT and DPO. We introduce three key evaluation
metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined
Alignment Score (CAS), all derived from reward model outputs. The results show
that while SFT outperforms DPO, The combined SFT+DPO model outperforms all
others across all metrics, demonstrating the complementary nature of these
techniques. Our findings also highlight challenges posed by noisy data, limited
GPU resources, and training constraints. This study offers a comprehensive view
of how fine-tuning strategies affect model alignment and provides a foundation
for more robust alignment pipelines in future work.

</details>


### [8] [MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction](https://arxiv.org/abs/2509.09082)
*Zhongqiu Li,Shiquan Wang,Ruiyu Fang,Mengjiao Bao,Zhenhe Wu,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: 提出MR-UIE方法，通过强化学习和多视角推理提升大语言模型在通用信息抽取任务中的性能，特别是在复杂模式描述和多步推理场景下


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在通用信息抽取任务中表现不足，特别是在处理结构化输出、复杂模式描述和多步推理的场景下，现有方法如上下文学习和指令微调仍存在显著局限

Method: 将强化学习与多视角推理相结合，使大语言模型从被动抽取器转变为主动推理器，不仅理解要抽取什么，还理解如何进行推理

Result: 在多个信息抽取基准测试中，MR-UIE方法持续提升了跨领域的抽取准确性，并在多个数据集上超越了最先进方法，多视角推理显著增强了复杂信息抽取任务中的泛化能力

Conclusion: 多视角推理在强化学习中的整合对于提升复杂信息抽取任务的性能至关重要，强调了推理在挑战性场景中的关键作用

Abstract: Large language models (LLMs) demonstrate robust capabilities across diverse
research domains. However, their performance in universal information
extraction (UIE) remains insufficient, especially when tackling structured
output scenarios that involve complex schema descriptions and require
multi-step reasoning. While existing approaches enhance the performance of LLMs
through in-context learning and instruction tuning, significant limitations
nonetheless persist. To enhance the model's generalization ability, we propose
integrating reinforcement learning (RL) with multi-perspective reasoning for
information extraction (IE) tasks. Our work transitions LLMs from passive
extractors to active reasoners, enabling them to understand not only what to
extract but also how to reason. Experiments conducted on multiple IE benchmarks
demonstrate that MR-UIE consistently elevates extraction accuracy across
domains and surpasses state-of-the-art methods on several datasets.
Furthermore, incorporating multi-perspective reasoning into RL notably enhances
generalization in complex IE tasks, underscoring the critical role of reasoning
in challenging scenarios.

</details>


### [9] [TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla](https://arxiv.org/abs/2509.09101)
*Nishat Raihan,Antonios Anastasopoulos,Marcos Zampieri*

Main category: cs.CL

TL;DR: 首个专门为孜加拉语设计的代码生成大语言模型家族，包含1B和9B版本，通过高质量数据集和专门评测标准实现了约11-18%性能提升


<details>
  <summary>Details</summary>
Motivation: 孜加拉语作为世界第5大语言在大语言模型中表现不足，特别是在代码生成领域，主要因为缺乏高质量的训练数据

Method: 构建了完整的孜加拉语代码指令数据集，创建MBPP-Bangla评测标准，开发TigerCoder模型家族（包含1B和9B两个规模）

Result: 在Pass@1指标上比现有多语言和通用孜加拉语LLM提升了11-18%的性能，证明精选的高质量数据可以克服小模型在低资源语言中的限制

Conclusion: 通过构建专门的高质量数据集和评测标准，可以有效提升低资源语言的代码生成能力，并将所有资源开源以促进孜加拉语LLM研究的进一步发展

Abstract: Despite being the 5th most spoken language, Bangla remains underrepresented
in Large Language Models (LLMs), particularly for code generation. This
primarily stems from the scarcity of high-quality data to pre-train and/or
finetune such models. Hence, we introduce the first dedicated family of Code
LLMs for Bangla (1B & 9B). We offer three major contributions: (1) a
comprehensive Bangla code instruction datasets for programming domain
adaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code
generation; and (3) the TigerCoder-family of Code LLMs, achieving significant
~11-18% performance gains at Pass@1 over existing multilingual and
general-purpose Bangla LLMs. Our findings show that curated, high-quality
datasets can overcome limitations of smaller models for low-resource languages.
We open-source all resources to advance further Bangla LLM research.

</details>


### [10] [Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia](https://arxiv.org/abs/2509.09121)
*Sophia Maria*

Main category: cs.CL

TL;DR: Compass-v3是一个245B参数的垂直领域MoE模型，专为东南亚电商设计，在电商任务上超越GPT-4等主流模型，支持多语言并已在Shopee平台大规模应用


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在需要领域专业知识的专门任务上表现不佳，电商领域数据嘈杂、异构、多语言且高度动态，需要专门优化的模型

Method: 采用更少但更大的专家设计，结合硬件优化（节点内专家并行和定制memcpy操作符），使用12T token的多语言语料和合成电商指令进行混合训练，提出OTPO方法增强对齐

Result: 在电商任务上超越DeepSeek-V3.1、GPT-4系列和Qwen3-235B，在东南亚低资源语言和葡萄牙语上表现优异，在Shopee平台已替代70%以上的OpenAI流量

Conclusion: Compass-v3在专业电商知识和广泛语言能力方面具有双重优势，证明了垂直领域MoE模型在专门应用中的有效性

Abstract: Large language models (LLMs) excel in general-domain applications, yet their
performance often degrades in specialized tasks requiring domain-specific
knowledge. E-commerce is particularly challenging, as its data are noisy,
heterogeneous, multilingual, and highly dynamic. We present Compass-v3, a
vertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and
71B active per token, designed for Southeast Asian e-commerce. Compass-v3
adopts fewer but larger experts, combined with hardware-efficient
optimizations-such as intra-node expert parallelism and a customized memcpy
operator-to maximize GPU utilization. The model is trained on 12T tokens of
curated multilingual corpora and large-scale synthetic e-commerce instructions
using a mixed-training strategy. To enhance alignment, we propose
Optimal-Transport Direct Preference Optimization (OTPO), which captures
token-level distinctions and improves instruction adherence in
commerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3
delivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,
GPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong
multilingual capability across low-resource Southeast Asian languages
(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while
sustaining competitive performance on general benchmarks. It has already been
widely applied in Shopee's industrial-scale e-commerce platform and is
gradually replacing OpenAI's traffic, now accounting for over 70\% of total LLM
usage, highlighting its dual strengths in specialized commerce expertise and
broad linguistic competence.

</details>


### [11] [Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus](https://arxiv.org/abs/2509.09125)
*Liqun He,Jiaqi Xu*

Main category: cs.CL

TL;DR: 使用生成式AI自动分类教师对话行为，GPT-4达到80%准确率，显著优于基线表现，为教育对话分析提供高效解决方案


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在教师对话行为分类中的应用，旨在减少传统人工编码所需的时间和精力

Method: 使用开源CIMA语料库，测试GPT-3.5-turbo和GPT-4模型，采用定制化提示进行对话行为分类

Result: GPT-4取得80%准确率、加权F1分数0.81、Cohen's Kappa 0.74，与人工标注具有实质性一致

Conclusion: 生成式AI在教育对话行为分类中具有强大潜力，任务特定的标签定义和上下文信息对提升自动标注质量至关重要，同时需要关注伦理考量

Abstract: This study explores the use of generative AI for automating the
classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and
effort required by traditional manual coding. This case study uses the
open-source CIMA corpus, in which tutors' responses are pre-annotated into four
DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored
prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of
0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and
indicating substantial agreement with human annotations. These findings suggest
that generative AI has strong potential to provide an efficient and accessible
approach to DA classification, with meaningful implications for educational
dialogue analysis. The study also highlights the importance of task-specific
label definitions and contextual information in enhancing the quality of
automated annotation. Finally, it underscores the ethical considerations
associated with the use of generative AI and the need for responsible and
transparent research practices. The script of this research is publicly
available at
https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.

</details>


### [12] [ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking](https://arxiv.org/abs/2509.09131)
*Phuong-Nam Dang,Kieu-Linh Nguyen,Thanh-Hieu Pham*

Main category: cs.CL

TL;DR: ViRanker是一个专门为越南语设计的跨编码器重排序模型，基于BGE-M3编码器并采用Blockwise Parallel Transformer增强，在MMARCO-VI基准测试中表现出色，超越了多语言基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决越南语缺乏竞争性重排序模型的问题，越南语作为低资源语言具有复杂的语法和音调符号，需要专门的模型来处理其语言特性。

Method: 基于BGE-M3编码器构建，采用Blockwise Parallel Transformer架构增强，使用8GB精选语料库进行训练，并通过混合硬负采样进行微调以提高鲁棒性。

Result: 在MMARCO-VI基准测试中实现了强大的早期排名准确性，超越了多语言基线模型，并与PhoRanker竞争表现接近。

Conclusion: ViRanker填补了越南语重排序模型的空白，通过精心设计的架构适应和数据管理，为其他低资源语言的reranking研究提供了有价值的参考，模型已在Hugging Face开源发布以支持复现和实际应用。

Abstract: This paper presents ViRanker, a cross-encoder reranking model tailored to the
Vietnamese language. Built on the BGE-M3 encoder and enhanced with the
Blockwise Parallel Transformer, ViRanker addresses the lack of competitive
rerankers for Vietnamese, a low-resource language with complex syntax and
diacritics. The model was trained on an 8 GB curated corpus and fine-tuned with
hybrid hard-negative sampling to strengthen robustness. Evaluated on the
MMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing
multilingual baselines and competing closely with PhoRanker. By releasing the
model openly on Hugging Face, we aim to support reproducibility and encourage
wider adoption in real-world retrieval systems. Beyond Vietnamese, this study
illustrates how careful architectural adaptation and data curation can advance
reranking in other underrepresented languages.

</details>


### [13] [LITcoder: A General-Purpose Library for Building and Comparing Encoding Models](https://arxiv.org/abs/2509.09152)
*Taha Binhuraib,Ruimin Gao,Anna A. Ivanova*

Main category: cs.CL

TL;DR: LITcoder是一个开源神经编码模型库，提供标准化工具来构建和评估大脑活动预测模型，支持多种方法选择和数据集，降低技术门槛并促进方法学严谨性。


<details>
  <summary>Details</summary>
Motivation: 为了解决神经编码模型构建中的技术障碍，提供标准化工具来简化模型开发、比较和评估过程，促进该领域的方法学严谨性和系统性研究。

Method: 采用模块化流水线设计，支持多种方法选择：大脑数据集、脑区、刺激特征（神经网络基础和对照组）、降采样方法等，内置日志记录、绘图功能，并与实验跟踪平台无缝集成。

Result: 框架在三个故事聆听数据集（LeBel et al. 2023、Narratives、Little Prince）上展示了良好的可扩展性和多功能性，验证了关键方法学选择的重要性。

Conclusion: LITcoder显著降低了编码模型实施的技术障碍，促进了跨模型和数据集的系统比较，加速了高质量大脑活动预测模型的开发。

Abstract: We introduce LITcoder, an open-source library for building and benchmarking
neural encoding models. Designed as a flexible backend, LITcoder provides
standardized tools for aligning continuous stimuli (e.g., text and speech) with
brain data, transforming stimuli into representational features, mapping those
features onto brain data, and evaluating the predictive performance of the
resulting model on held-out data. The library implements a modular pipeline
covering a wide array of methodological design choices, so researchers can
easily compose, compare, and extend encoding models without reinventing core
infrastructure. Such choices include brain datasets, brain regions, stimulus
feature (both neural-net-based and control, such as word rate), downsampling
approaches, and many others. In addition, the library provides built-in
logging, plotting, and seamless integration with experiment tracking platforms
such as Weights & Biases (W&B). We demonstrate the scalability and versatility
of our framework by fitting a range of encoding models to three story listening
datasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore
the methodological choices critical for building encoding models for continuous
fMRI data, illustrating the importance of accounting for all tokens in a TR
scan (as opposed to just taking the last one, even when contextualized),
incorporating hemodynamic lag effects, using train-test splits that minimize
information leakage, and accounting for head motion effects on encoding model
predictivity. Overall, LITcoder lowers technical barriers to encoding model
implementation, facilitates systematic comparisons across models and datasets,
fosters methodological rigor, and accelerates the development of high-quality
high-performance predictive models of brain activity.
  Project page: https://litcoder-brain.github.io

</details>


### [14] [Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing](https://arxiv.org/abs/2509.09160)
*Zhiyue Liu,Fanrong Ma,Xin Ling*

Main category: cs.CL

TL;DR: 这篇论文提出了一种反事实增强的去偏框架，用于减少目标导向多模态情感分类中的偏见问题，通过反事实数据增强和适应性对比学习机制提升分类准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖文本内容且忽视了数据集偏见，导致文本特征与输出标签间的偏偏相关性，影响分类准确性。

Method: 提出反事实数据增强策略，最小化改变情感相关因果特征生成详细匹配的图像-文本样本，并介绍适应性去偏对比学习机制学习稳健特征。

Result: 在多个标准数据集上的实验结果显示，该方法超越了最先进的基线方法。

Conclusion: 该框架能够有效减少偏见字的影响，提升目标导向多模态情感分类的性能。

Abstract: Target-oriented multimodal sentiment classification seeks to predict
sentiment polarity for specific targets from image-text pairs. While existing
works achieve competitive performance, they often over-rely on textual content
and fail to consider dataset biases, in particular word-level contextual
biases. This leads to spurious correlations between text features and output
labels, impairing classification accuracy. In this paper, we introduce a novel
counterfactual-enhanced debiasing framework to reduce such spurious
correlations. Our framework incorporates a counterfactual data augmentation
strategy that minimally alters sentiment-related causal features, generating
detail-matched image-text samples to guide the model's attention toward content
tied to sentiment. Furthermore, for learning robust features from
counterfactual data and prompting model decisions, we introduce an adaptive
debiasing contrastive learning mechanism, which effectively mitigates the
influence of biased words. Experimental results on several benchmark datasets
show that our proposed method outperforms state-of-the-art baselines.

</details>


### [15] [EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs](https://arxiv.org/abs/2509.09174)
*Yuhao Zhang,Yuhao Du,Zhanchen Dai,Xiangnan Ma,Kaiqi Kou,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: EchoX是一个语音到语音大语言模型，通过整合声学和语义学习来解决当前SLLMs在知识和推理能力上的退化问题，在有限训练数据下实现了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本大语言模型衍生的语音到语音大语言模型(SLLMs)在知识和推理能力上存在退化，作者认为这是因为现有训练范式未能弥合特征表示空间中的声学-语义鸿沟。

Method: 提出EchoX方法，利用语义表示并动态生成语音训练目标，整合声学和语义学习，使模型能够保持强大的推理能力。

Result: 实验结果表明，EchoX仅用约6000小时的训练数据就在多个基于知识的问答基准测试中取得了先进性能。

Conclusion: EchoX通过创新的训练方法有效解决了SLLMs的能力退化问题，证明了整合声学和语义学习的重要性。

Abstract: Speech-to-speech large language models (SLLMs) are attracting increasing
attention. Derived from text-based large language models (LLMs), SLLMs often
exhibit degradation in knowledge and reasoning capabilities. We hypothesize
that this limitation arises because current training paradigms for SLLMs fail
to bridge the acoustic-semantic gap in the feature representation space. To
address this issue, we propose EchoX, which leverages semantic representations
and dynamically generates speech training targets. This approach integrates
both acoustic and semantic learning, enabling EchoX to preserve strong
reasoning abilities as a speech LLM. Experimental results demonstrate that
EchoX, with about six thousand hours of training data, achieves advanced
performance on multiple knowledge-based question-answering benchmarks. The
project is available at https://github.com/FreedomIntelligence/EchoX.

</details>


### [16] [Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition](https://arxiv.org/abs/2509.09196)
*Chin Yuen Kwok,Jia Qi yip*

Main category: cs.CL

TL;DR: 通过多步预测策略改进ASR模型的上下文偏置，避免传统Trie基础偏置的分数撤销步骤，显著提高稀有词识别准确性


<details>
  <summary>Details</summary>
Motivation: 传统Trie基上下文偏置方法在识别稀有词时需要撤销之前的奖励分数，这一步骤仅限于谱搜索且计算成本高，尤其在大解码器模型中

Method: 调整ASR模型进行多步预测，通过预测多个步骤来更好估计部分假设是否会导致完整稀有词的生成，从而完全避免分数撤销步骤

Result: 仅使用10小时合成数据对Whisper模型进行微调，将NSC Part 2测试集的词误率从30.86%降至12.19%

Conclusion: 多步预测策略能够有效解决传统上下文偏置方法的算力问题，显著提升ASR模型在稀有词识别任务上的性能

Abstract: Contextual biasing improves rare word recognition of ASR models by
prioritizing the output of rare words during decoding. A common approach is
Trie-based biasing, which gives "bonus scores" to partial hypothesis (e.g.
"Bon") that may lead to the generation of the rare word (e.g. "Bonham"). If the
full word ("Bonham") isn't ultimately recognized, the system revokes those
earlier bonuses. This revocation is limited to beam search and is
computationally expensive, particularly for models with large decoders. To
overcome these limitations, we propose adapting ASR models to look ahead and
predict multiple steps at once. This avoids the revocation step entirely by
better estimating whether a partial hypothesis will lead to the generation of
the full rare word. By fine-tuning Whisper with only 10 hours of synthetic
data, our method reduces the word error rate on the NSC Part 2 test set from
30.86% to 12.19%.

</details>


### [17] [Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function](https://arxiv.org/abs/2509.09197)
*Chin Yuen Kwok,Jia Qi Yip,Eng Siong Chng*

Main category: cs.CL

TL;DR: 通过关键词感知损失函数改进TCPGen上下文偏置方法，在少量合成数据上训练ASR模型，将词语误率从29.71%降至11.81%


<details>
  <summary>Details</summary>
Motivation: 解决稀有词识别中的过拟合问题，合成数据包含声音虚假特征导致上下文偏置模块训练时出现过拟合

Method: 提出关键词感知损失函数，包含偶偶交叉瑣损失和二元分类损失，分别用于偏置词预测和偏置词位置检测

Result: 在10小时合成数据上对Whisper模型进行适配，在NSC Part 2测试集上将词语误率从29.71%显著降至11.81%

Conclusion: 关键词感知损失函数能够有效提升上下文偏置模块的训练效果，通过补充性的偏置词预测和位置检测，显著改善稀有词识别性能

Abstract: Rare word recognition can be improved by adapting ASR models to synthetic
data that includes these words. Further improvements can be achieved through
contextual biasing, which trains and adds a biasing module into the model
architecture to prioritize rare words. While training the module on synthetic
rare word data is more effective than using non-rare-word data, it can lead to
overfitting due to artifacts in the synthetic audio. To address this, we
enhance the TCPGen-based contextual biasing approach and propose a
keyword-aware loss function that additionally focuses on biased words when
training biasing modules. This loss includes a masked cross-entropy term for
biased word prediction and a binary classification term for detecting biased
word positions. These two terms complementarily support the decoding of biased
words during inference. By adapting Whisper to 10 hours of synthetic data, our
method reduced the word error rate on the NSC Part 2 test set from 29.71% to
11.81%.

</details>


### [18] [GmSLM : Generative Marmoset Spoken Language Modeling](https://arxiv.org/abs/2509.09198)
*Talia Sternberg,Michael London,David Omer,Yossi Adi*

Main category: cs.CL

TL;DR: 提出了GmSLM模型，用于狨猴声音通信建模，通过无监督方法生成逼真的狨猴叫声，在声学特征和下游任务上表现优异


<details>
  <summary>Details</summary>
Motivation: 狨猴具有复杂的语音通信能力，类似于人类语言的某些特征，研究其声音通信可以链接大脑活动，但由于主要使用叫声通信，传统LLM方法不适用

Method: 开发了优化的语音语言模型管道GmSLM，使用无监督野外数据和弱标记对话数据进行零样本评估，生成狨猴叫声

Result: GmSLM生成的叫声在声学上与真实样本高度匹配，在下游任务表现良好，能有效区分真实与人工对话

Conclusion: GmSLM为研究声音通信的神经基础提供了实用框架，有助于神经科学、生物声学和进化生物学领域的未来研究

Abstract: Marmoset monkeys exhibit complex vocal communication, challenging the view
that nonhuman primates vocal communication is entirely innate, and show similar
features of human speech, such as vocal labeling of others and turn-taking.
Studying their vocal communication offers a unique opportunity to link it with
brain activity-especially given the difficulty of accessing the human brain in
speech and language research. Since Marmosets communicate primarily through
vocalizations, applying standard LLM approaches is not straightforward. We
introduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized
spoken language model pipeline for Marmoset vocal communication. We designed a
novel zero-shot evaluation metrics using unsupervised in-the-wild data,
alongside weakly labeled conversational data, to assess GmSLM and demonstrate
its advantage over a basic human-speech-based baseline. GmSLM generated
vocalizations closely matched real resynthesized samples acoustically and
performed well on downstream tasks. Despite being fully unsupervised, GmSLM
effectively distinguish real from artificial conversations and may support
further investigations of the neural basis of vocal communication and provides
a practical framework linking vocalization and brain activity. We believe GmSLM
stands to benefit future work in neuroscience, bioacoustics, and evolutionary
biology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.

</details>


### [19] [CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling](https://arxiv.org/abs/2509.09199)
*Wenhao Li,Bangcheng Sun,Weihao Ye,Tianyi Zhang,Daohai Yu,Fei Chao,Rongrong Ji*

Main category: cs.CL

TL;DR: CCF是一个新颖的上下文压缩框架，通过分层潜在表示学习来高效处理长上下文，在保持语义的同时减少冗余，显著提升了计算和内存效率。


<details>
  <summary>Details</summary>
Motivation: 扩展语言模型到更长上下文对于捕捉扩展语篇中的丰富依赖关系至关重要，但简单的上下文扩展会带来显著的计算和内存负担，导致训练和推理效率低下。

Method: 提出CCF框架，整合分段语义聚合和键值记忆编码，形成紧凑表示；引入训练效率优化策略，结合增量分段解码和稀疏储层采样来减少内存开销。

Result: 在多个长上下文语言建模基准测试中，CCF在高压缩比下实现了有竞争力的困惑度，相比现有方法显著提高了吞吐量和内存效率。

Conclusion: 结构化压缩对于可扩展和有效的长上下文语言建模具有巨大潜力，CCF框架为此提供了有效的解决方案。

Abstract: Scaling language models to longer contexts is essential for capturing rich
dependencies across extended discourse. However, na\"ive context extension
imposes significant computational and memory burdens, often resulting in
inefficiencies during both training and inference. In this work, we propose
CCF, a novel context compression framework designed to enable efficient
long-context modeling by learning hierarchical latent representations that
preserve global semantics while aggressively reducing input redundancy. CCF
integrates segment-wise semantic aggregation with key-value memory encoding,
forming compact representations that support accurate reconstruction and
long-range understanding. To further enhance scalability, we introduce a
training-efficient optimization strategy that couples incremental segment
decoding with sparse reservoir sampling, substantially reducing memory overhead
without degrading performance. Empirical results on multiple long-context
language modeling benchmarks demonstrate that CCF achieves competitive
perplexity under high compression ratios, and significantly improves throughput
and memory efficiency compared to existing approaches. These findings highlight
the potential of structured compression for scalable and effective long-context
language modeling.

</details>


### [20] [Reading Between the Lines: Classifying Resume Seniority with Large Language Models](https://arxiv.org/abs/2509.09229)
*Matan Cohen,Shira Shani,Eden Menahem,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: 本研究探讨了使用大型语言模型（包括微调的BERT架构）来自动化简历中资历分类的有效性，通过混合数据集评估模型在检测资历夸大和隐含专业能力方面的表现。


<details>
  <summary>Details</summary>
Motivation: 准确评估简历中的候选人资历是一个关键但具有挑战性的任务，由于普遍存在经验夸大和模糊自我呈现的问题，需要自动化解决方案来提高评估准确性。

Method: 引入包含真实简历和合成生成的困难样本的混合数据集，使用大型语言模型（包括微调BERT架构）来检测与资历夸大相关的微妙语言线索和隐含专业能力。

Result: 研究发现大型语言模型在检测资历夸大和隐含专业能力方面表现出色，为增强AI驱动的候选人评估系统提供了有前景的方向。

Conclusion: 该方法有助于减轻自我推销语言引入的偏见，提高候选人评估的准确性，相关数据集已向研究社区开放。

Abstract: Accurately assessing candidate seniority from resumes is a critical yet
challenging task, complicated by the prevalence of overstated experience and
ambiguous self-presentation. In this study, we investigate the effectiveness of
large language models (LLMs), including fine-tuned BERT architectures, for
automating seniority classification in resumes. To rigorously evaluate model
performance, we introduce a hybrid dataset comprising both real-world resumes
and synthetically generated hard examples designed to simulate exaggerated
qualifications and understated seniority. Using the dataset, we evaluate the
performance of Large Language Models in detecting subtle linguistic cues
associated with seniority inflation and implicit expertise. Our findings
highlight promising directions for enhancing AI-driven candidate evaluation
systems and mitigating bias introduced by self-promotional language. The
dataset is available for the research community at https://bit.ly/4mcTovt

</details>


### [21] [Agentic LLMs for Question Answering over Tabular Data](https://arxiv.org/abs/2509.09234)
*Rishit Tyagi,Mohit Gupta,Rahul Bouri*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的NL-to-SQL方法，在SemEval 2025 Task 8的DataBench基准测试中取得了显著优于基线的性能表现


<details>
  <summary>Details</summary>
Motivation: 表格问答面临表格结构、大小和数据类型的多样性挑战，需要开发能够准确回答结构化查询的模型

Method: 采用多阶段流水线方法，包括示例选择、SQL查询生成、答案提取、验证和迭代优化，利用GPT-4o、GPT-4o-mini和DeepSeek v2:16b等大语言模型动态生成SQL查询

Result: 在DataBench QA上达到70.5%准确率，在DataBench Lite QA上达到71.6%准确率，显著超越26%和27%的基线分数

Conclusion: 大语言模型驱动的表格问答方法有效，论文详细分析了方法的优势和局限性

Abstract: Question Answering over Tabular Data (Table QA) presents unique challenges
due to the diverse structure, size, and data types of real-world tables. The
SemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,
domain-diverse datasets to evaluate the ability of models to accurately answer
structured queries. We propose a Natural Language to SQL (NL-to-SQL) approach
leveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and
DeepSeek v2:16b to generate SQL queries dynamically. Our system follows a
multi-stage pipeline involving example selection, SQL query generation, answer
extraction, verification, and iterative refinement. Experiments demonstrate the
effectiveness of our approach, achieving 70.5\% accuracy on DataBench QA and
71.6\% on DataBench Lite QA, significantly surpassing baseline scores of 26\%
and 27\% respectively. This paper details our methodology, experimental
results, and alternative approaches, providing insights into the strengths and
limitations of LLM-driven Table QA.

</details>


### [22] [From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models](https://arxiv.org/abs/2509.09303)
*Grazia Sveva Ascione,Nicolò Tamagnone*

Main category: cs.CL

TL;DR: 使用弱监督方法通过专利引用和LLM提取概念来构建专利-SDG分类数据集，解决了标注数据稀缺问题，在内部和外部验证中均优于基线方法


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模标注数据集限制了监督学习在专利-SDG分类中的应用，现有方法缺乏可扩展性和泛化性

Method: 将专利-SDG分类构建为弱监督问题，使用专利到SDG标记科学文献的引用作为噪声信号，开发复合标注函数利用LLM从专利和SDG论文中提取结构化概念，计算跨域相似度得分

Result: 构建了银标准软多标签数据集，在内部验证中优于包括transformer模型和零样本LLM在内的多个基线，在外部验证中显示出比传统技术分类更好的主题、认知和组织一致性

Conclusion: 弱监督和语义对齐可以大规模增强SDG分类，该方法为解决全球挑战的创新跟踪提供了有效工具

Abstract: Classifying patents by their relevance to the UN Sustainable Development
Goals (SDGs) is crucial for tracking how innovation addresses global
challenges. However, the absence of a large, labeled dataset limits the use of
supervised learning. Existing methods, such as keyword searches, transfer
learning, and citation-based heuristics, lack scalability and generalizability.
This paper frames patent-to-SDG classification as a weak supervision problem,
using citations from patents to SDG-tagged scientific publications (NPL
citations) as a noisy initial signal. To address its sparsity and noise, we
develop a composite labeling function (LF) that uses large language models
(LLMs) to extract structured concepts, namely functions, solutions, and
applications, from patents and SDG papers based on a patent ontology.
Cross-domain similarity scores are computed and combined using a rank-based
retrieval approach. The LF is calibrated via a custom positive-only loss that
aligns with known NPL-SDG links without penalizing discovery of new SDG
associations. The result is a silver-standard, soft multi-label dataset mapping
patents to SDGs, enabling the training of effective multi-label regression
models. We validate our approach through two complementary strategies: (1)
internal validation against held-out NPL-based labels, where our method
outperforms several baselines including transformer-based models, and zero-shot
LLM; and (2) external validation using network modularity in patent citation,
co-inventor, and co-applicant graphs, where our labels reveal greater thematic,
cognitive, and organizational coherence than traditional technological
classifications. These results show that weak supervision and semantic
alignment can enhance SDG classification at scale.

</details>


### [23] [MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems](https://arxiv.org/abs/2509.09360)
*Channdeth Sok,David Luz,Yacine Haddam*

Main category: cs.CL

TL;DR: MetaRAG是一个用于检索增强生成(RAG)系统的幻觉检测框架，通过分解答案、生成变异事实、验证一致性来实时检测幻觉，无需真实参考或模型内部访问。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法主要针对独立LLM，无法解决RAG系统中响应必须与检索证据一致的特殊挑战，企业应用中需要可靠的幻觉检测方案。

Method: 四阶段框架：1)分解答案为原子事实 2)使用同义词和反义词生成受控变异 3)验证变异与检索上下文的一致性 4)聚合不一致性惩罚为响应级幻觉分数

Result: 在企业专有数据集上的实验证明了MetaRAG在检测幻觉方面的有效性，支持RAG对话代理的可信部署。

Conclusion: MetaRAG为RAG系统提供了实时、无监督的黑盒幻觉检测方案，特别适用于身份敏感查询的可信部署，支持span级定位和阈值配置。

Abstract: Large Language Models (LLMs) are increasingly deployed in enterprise
applications, yet their reliability remains limited by hallucinations, i.e.,
confident but factually incorrect information. Existing detection approaches,
such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not
address the unique challenges of Retrieval-Augmented Generation (RAG) systems,
where responses must be consistent with retrieved evidence. We therefore
present MetaRAG, a metamorphic testing framework for hallucination detection in
Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,
unsupervised, black-box setting, requiring neither ground-truth references nor
access to model internals, making it suitable for proprietary and high-stakes
domains. The framework proceeds in four stages: (1) decompose answers into
atomic factoids, (2) generate controlled mutations of each factoid using
synonym and antonym substitutions, (3) verify each variant against the
retrieved context (synonyms are expected to be entailed and antonyms
contradicted), and (4) aggregate penalties for inconsistencies into a
response-level hallucination score. Crucially for identity-aware AI, MetaRAG
localizes unsupported claims at the factoid span where they occur (e.g.,
pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),
allowing users to see flagged spans and enabling system designers to configure
thresholds and guardrails for identity-sensitive queries. Experiments on a
proprietary enterprise dataset illustrate the effectiveness of MetaRAG for
detecting hallucinations and enabling trustworthy deployment of RAG-based
conversational agents. We also outline a topic-based deployment design that
translates MetaRAG's span-level scores into identity-aware safeguards; this
design is discussed but not evaluated in our experiments.

</details>


### [24] [Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research](https://arxiv.org/abs/2509.09381)
*Molly R Petersen,Claire E Stevenson,Lonneke van der Plas*

Main category: cs.CL

TL;DR: 本文总结了认知科学中类比推理的关键理论，并将其与自然语言处理研究联系起来，指出这些理论虽然与NLP概念相关但缺乏认知视角，并展示了这些理论对NLP主要挑战的适用性。


<details>
  <summary>Details</summary>
Motivation: 类比推理是人类认知的重要方面，但当前NLP研究虽然涉及相关概念，却缺乏从认知科学角度的深入理解。作者希望将认知科学的类比推理理论与NLP研究相结合，为优化文本中的关系理解提供指导。

Method: 通过总结认知科学文献中关于类比推理过程的关键理论，并将其与自然语言处理中的相关概念进行关联分析，探讨这些认知理论如何应用于NLP的主要挑战。

Result: 研究发现认知科学的类比推理理论可以很好地与NLP概念相联系，这些理论不仅适用于类比求解问题，还对NLP研究的其他主要挑战具有相关性，能够帮助研究者更好地优化文本中的关系理解。

Conclusion: 将认知科学的类比推理理论引入NLP研究具有重要意义，可以引导研究者减少对实体层面相似性的依赖，转而更加关注关系层面的理解，从而提升NLP系统的认知能力和性能。

Abstract: Analogical reasoning is an essential aspect of human cognition. In this
paper, we summarize key theory about the processes underlying analogical
reasoning from the cognitive science literature and relate it to current
research in natural language processing. While these processes can be easily
linked to concepts in NLP, they are generally not viewed through a cognitive
lens. Furthermore, we show how these notions are relevant for several major
challenges in NLP research, not directly related to analogy solving. This may
guide researchers to better optimize relational understanding in text, as
opposed to relying heavily on entity-level similarity.

</details>


### [25] [Hierarchical Bracketing Encodings Work for Dependency Graphs](https://arxiv.org/abs/2509.09388)
*Ana Ezquerro,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: 这篇论文提出了一种层次括号编码方法，用于依赖图解析，在线性时间内处理回环、重入和空节点，同时减少标签空间保持结构信息。


<details>
  <summary>Details</summary>
Motivation: 重新考虑层次括号编码在依赖图解析中的实际应用价值，解决现有图线性化方法标签空间过大的问题。

Method: 将图结构编码为序列，通过n个标签动作实现线性时间解析，支持回环、重入和空节点表示。

Result: 在多语言多形式基准测试中显示出竞争力，在准确匹配率上正常超过其他方法。

Conclusion: 该层次括号编码方法在保持结构信息的同时显著减少标签空间，为依赖图解析提供了一种高效实用的解决方案。

Abstract: We revisit hierarchical bracketing encodings from a practical perspective in
the context of dependency graph parsing. The approach encodes graphs as
sequences, enabling linear-time parsing with $n$ tagging actions, and still
representing reentrancies, cycles, and empty nodes. Compared to existing graph
linearizations, this representation substantially reduces the label space while
preserving structural information. We evaluate it on a multilingual and
multi-formalism benchmark, showing competitive results and consistent
improvements over other methods in exact match accuracy.

</details>


### [26] [GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models](https://arxiv.org/abs/2509.09438)
*Zhaohan Zhang,Ziquan Liu,Ioannis Patras*

Main category: cs.CL

TL;DR: GrACE是一种新的LLM置信度评估方法，通过隐藏状态与特殊标记嵌入的相似性来实时生成置信度，无需额外采样或辅助模型，在准确性和校准方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM置信度评估方法要么计算成本高，要么校准效果差，难以在实际高风险应用中可靠部署。

Method: 提出GrACE方法，通过模型最后一个隐藏状态与词汇表中特殊标记嵌入的相似性来实时表达置信度，并通过与准确性相关的校准目标进行微调。

Result: 在三个LLM和两个基准数据集上的实验表明，GrACE在开放式生成任务中具有最佳的判别能力和校准效果，优于六种竞争方法。

Conclusion: GrACE为LLM部署提供了可扩展、可靠且实时的置信度估计实用解决方案，不仅能提高最终决策准确性，还能显著减少测试时缩放所需的样本数量。

Abstract: Assessing the reliability of Large Language Models (LLMs) by confidence
elicitation is a prominent approach to AI safety in high-stakes applications,
such as healthcare and finance. Existing methods either require expensive
computational overhead or suffer from poor calibration, making them impractical
and unreliable for real-world deployment. In this work, we propose GrACE, a
Generative Approach to Confidence Elicitation that enables scalable and
reliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in
which the model expresses confidence by the similarity between the last hidden
state and the embedding of a special token appended to the vocabulary, in
real-time. We fine-tune the model for calibrating the confidence with
calibration targets associated with accuracy. Experiments with three LLMs and
two benchmark datasets show that the confidence produced by GrACE achieves the
best discriminative capacity and calibration on open-ended generation tasks,
outperforming six competing methods without resorting to additional sampling or
an auxiliary model. Moreover, we propose two strategies for improving test-time
scaling based on confidence induced by GrACE. Experimental results show that
using GrACE not only improves the accuracy of the final decision but also
significantly reduces the number of required samples in the test-time scaling
scheme, indicating the potential of GrACE as a practical solution for deploying
LLMs with scalable, reliable, and real-time confidence estimation.

</details>


### [27] [Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation](https://arxiv.org/abs/2509.09473)
*Lucie Poláková,Martin Popel,Věra Kloudová,Michal Novák,Mariia Anisimova,Jiří Balhar*

Main category: cs.CL

TL;DR: EdUKate项目开发多语言教育材料，通过机器翻译将捷克语互动练习翻译成乌克兰语、英语和德语，重点关注教育领域的捷克-乌克兰机器翻译系统。


<details>
  <summary>Details</summary>
Motivation: 为捷克中小学开发多语言学习材料，满足非捷克语学生的教育需求，特别是在乌克兰难民学生增加的背景下。

Method: 结合数字教育、语言学和机器翻译技术，开发专门针对教育领域的捷克-乌克兰机器翻译系统，处理XML和PDF格式内容及科技术语。

Result: 成功翻译了多达9000个多模态互动练习，开发了专门的机器翻译系统，并通过教师需求调查指导系统开发。

Conclusion: 项目为多语言教育提供了实用解决方案，所有应用免费向学生、教育工作者和研究人员开放，促进了教育包容性。

Abstract: The EdUKate project combines digital education, linguistics, translation
studies, and machine translation to develop multilingual learning materials for
Czech primary and secondary schools. Launched through collaboration between a
major Czech academic institution and the country's largest educational
publisher, the project is aimed at translating up to 9,000 multimodal
interactive exercises from Czech into Ukrainian, English, and German for an
educational web portal. It emphasizes the development and evaluation of a
direct Czech-Ukrainian machine translation system tailored to the educational
domain, with special attention to processing formatted content such as XML and
PDF and handling technical and scientific terminology. We present findings from
an initial survey of Czech teachers regarding the needs of non-Czech-speaking
students and describe the system's evaluation and implementation on the web
portal. All resulting applications are freely available to students, educators,
and researchers.

</details>


### [28] [Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs](https://arxiv.org/abs/2509.09522)
*Vadim Zadykian,Bruno Andrade,Haithem Afli*

Main category: cs.CL

TL;DR: 提出一种结合知识图谱和文本嵌入的自监督混合架构，用于语义文本相关性的细粒度分析，在职位匹配任务中显著提升高相关性区域的性能


<details>
  <summary>Details</summary>
Motivation: 解决简历推荐系统中职位匹配的挑战，传统方法在词汇重叠有限或误导的情况下效果不佳，需要捕捉超越表面词汇相似度的语义关系

Method: 自监督混合架构，结合密集句子嵌入和领域特定知识图谱，通过图神经网络整合知识图谱，并采用分层评估方法将语义相关性分数分为低、中、高三个区域

Result: 经过知识图谱增强的微调SBERT模型在高语义相关性区域表现最佳，RMSE相比强基线降低25%，揭示了全局指标掩盖的模型优缺点

Conclusion: 知识图谱与文本嵌入的结合不仅提升性能，更重要的是分层评估方法能够提供更细粒度的模型行为分析，对HR系统中公平性、可解释性和上下文匹配至关重要的应用具有重要价值

Abstract: Semantic Textual Relatedness (STR) captures nuanced relationships between
texts that extend beyond superficial lexical similarity. In this study, we
investigate STR in the context of job title matching - a key challenge in
resume recommendation systems, where overlapping terms are often limited or
misleading. We introduce a self-supervised hybrid architecture that combines
dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to
improve both semantic alignment and explainability. Unlike previous work that
evaluated models on aggregate performance, our approach emphasizes data
stratification by partitioning the STR score continuum into distinct regions:
low, medium, and high semantic relatedness. This stratified evaluation enables
a fine-grained analysis of model performance across semantically meaningful
subspaces. We evaluate several embedding models, both with and without KG
integration via graph neural networks. The results show that fine-tuned SBERT
models augmented with KGs produce consistent improvements in the high-STR
region, where the RMSE is reduced by 25% over strong baselines. Our findings
highlight not only the benefits of combining KGs with text embeddings, but also
the importance of regional performance analysis in understanding model
behavior. This granular approach reveals strengths and weaknesses hidden by
global metrics, and supports more targeted model selection for use in Human
Resources (HR) systems and applications where fairness, explainability, and
contextual matching are essential.

</details>


### [29] [DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning](https://arxiv.org/abs/2509.09524)
*Daniil Ignatev,Nan Li,Hugh Mee Wong,Anh Dang,Shane Kaszefski Yaschuk*

Main category: cs.CL

TL;DR: DeMeVa团队在LeWiDi 2025任务中探索了两种方法：基于大语言模型的上下文学习(ICL)和基于RoBERTa的标签分布学习(LDL)，发现ICL能有效预测个体标注者注释，LDL在软标签预测方面表现优异


<details>
  <summary>Details</summary>
Motivation: 探索在存在分歧的标注数据中，如何有效预测个体标注者的注释(perspectivist annotations)和软标签分布，以处理自然语言处理中的标注分歧问题

Method: 1) 使用大语言模型进行上下文学习，比较不同的示例采样策略；2) 使用RoBERTa进行标签分布学习，评估多种微调方法

Result: ICL能够有效预测个体标注者的注释，将这些预测聚合成软标签可获得有竞争力的性能；LDL方法在软标签预测方面表现优异，值得进一步探索

Conclusion: 两种方法在处理标注分歧任务中都显示出良好潜力，特别是LDL方法在软标签预测方面具有广阔的应用前景，值得perspectivist研究社区进一步深入研究

Abstract: This system paper presents the DeMeVa team's approaches to the third edition
of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et
al., 2025). We explore two directions: in-context learning (ICL) with large
language models, where we compare example sampling strategies; and label
distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we
evaluate several fine-tuning methods. Our contributions are twofold: (1) we
show that ICL can effectively predict annotator-specific annotations
(perspectivist annotations), and that aggregating these predictions into soft
labels yields competitive performance; and (2) we argue that LDL methods are
promising for soft label predictions and merit further exploration by the
perspectivist community.

</details>


### [30] [Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)](https://arxiv.org/abs/2509.09544)
*Paolo Pedinotti,Peter Baumann,Nathan Jessurun,Leslie Barrett,Enrico Santus*

Main category: cs.CL

TL;DR: MetaGraph是一个从科学文献中提取知识图谱的方法论，用于分析金融NLP研究趋势，揭示了LLM在金融NLP领域的三个发展阶段


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)快速重塑了金融NLP领域，但传统调查方法跟不上这种变革速度，需要新的方法来结构化分析研究趋势

Method: 定义了金融NLP研究本体论，应用基于LLM的提取管道处理681篇论文(2022-2025)，构建可查询的知识图谱进行大规模数据分析

Result: 揭示了三个关键阶段：早期LLM采用和任务/数据集创新；对LLM局限性的批判性反思；以及外围技术向模块化系统的日益整合

Conclusion: MetaGraph提供了对金融NLP演变的清晰理解，突出了新兴趋势和方���论转变，同时展示了在其他领域映射科学进展的可重用方法

Abstract: Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling
new tasks and driving a proliferation of datasets and diversification of data
sources. Yet, this transformation has outpaced traditional surveys. In this
paper, we present MetaGraph, a generalizable methodology for extracting
knowledge graphs from scientific literature and analyzing them to obtain a
structured, queryable view of research trends. We define an ontology for
financial NLP research and apply an LLM-based extraction pipeline to 681 papers
(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals
three key phases: early LLM adoption and task/dataset innovation; critical
reflection on LLM limitations; and growing integration of peripheral techniques
into modular systems. This structured view offers both practitioners and
researchers a clear understanding of how financial NLP has evolved -
highlighting emerging trends, shifting priorities, and methodological
shifts-while also demonstrating a reusable approach for mapping scientific
progress in other domains.

</details>


### [31] [Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking](https://arxiv.org/abs/2509.09583)
*Brittany Harbison,Samuel Taubman,Travis Taylor,Ashok. K. Goel*

Main category: cs.CL

TL;DR: 使用GPT零样本能力从论坛介绍帖推断大五人格特质，集成到SAMI匹配系统中提供人格感知的社交推荐


<details>
  <summary>Details</summary>
Motivation: 在线课程环境阻碍社交群体自然形成，SAMI系统因缺乏完整心理理论而受限，无法直觉人格特质影响推荐相关性

Method: 提出人格检测模型，利用GPT零样本能力从论坛介绍帖推断大五人格特质，集成到基于实体的匹配系统中

Result: 模型在人格检测任务中表现有效，初步集成表明人格特质可以补充现有匹配因素

Conclusion: 人格特质能够增强社交推荐系统，但需要进一步评估其对学生参与度和匹配质量的全面影响

Abstract: Social connection is a vital part of learning, yet online course environments
present barriers to the organic formation of social groups. SAMI offers one
solution by facilitating student connections, but its effectiveness is
constrained by an incomplete Theory of Mind, limiting its ability to create an
effective mental model of a student. One facet of this is its inability to
intuit personality, which may influence the relevance of its recommendations.
To explore this, we propose a personality detection model utilizing GPTs
zero-shot capability to infer Big-Five personality traits from forum
introduction posts, often encouraged in online courses. We benchmark its
performance against established models, demonstrating its efficacy in this
task. Furthermore, we integrate this model into SAMIs entity-based matchmaking
system, enabling personality-informed social recommendations. Initial
integration suggests personality traits can complement existing matching
factors, though additional evaluation is required to determine their full
impact on student engagement and match quality.

</details>


### [32] [Fluent but Unfeeling: The Emotional Blind Spots of Language Models](https://arxiv.org/abs/2509.09593)
*Bangzhao Shu,Isha Joshi,Melissa Karnaze,Anh C. Pham,Ishita Kakkar,Sindhu Kothe,Arpine Hovasapian,Mai ElSherief*

Main category: cs.CL

TL;DR: EXPRESS基准数据集评估LLM在细粒度情感识别方面与人类情感的匹配程度，发现LLM在预测与人类自我披露情感一致的情感方面仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要将情感分类到预定义的有限类别中，忽视了更细微的情感表达，需要评估LLM是否能在细粒度水平上与人类情感对齐。

Method: 引入EXPRESS基准数据集（包含251个细粒度情感标签），通过综合评估框架分析预测情感术语并将其分解为八种基本情感，系统测试主流LLM在不同提示设置下的表现。

Result: LLM准确预测与人类自我披露情感一致的情感仍然具有挑战性；某些LLM能生成符合情感理论的情感术语，但在捕捉上下文线索方面不如人类自我披露有效。

Conclusion: 研究揭示了LLM在细粒度情感对齐方面的局限性，为未来增强其上下文理解能力的研究提供了见解。

Abstract: The versatility of Large Language Models (LLMs) in natural language
understanding has made them increasingly popular in mental health research.
While many studies explore LLMs' capabilities in emotion recognition, a
critical gap remains in evaluating whether LLMs align with human emotions at a
fine-grained level. Existing research typically focuses on classifying emotions
into predefined, limited categories, overlooking more nuanced expressions. To
address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit
communities featuring 251 fine-grained, self-disclosed emotion labels. Our
comprehensive evaluation framework examines predicted emotion terms and
decomposes them into eight basic emotions using established emotion theories,
enabling a fine-grained comparison. Systematic testing of prevalent LLMs under
various prompt settings reveals that accurately predicting emotions that align
with human self-disclosed emotions remains challenging. Qualitative analysis
further shows that while certain LLMs generate emotion terms consistent with
established emotion theories and definitions, they sometimes fail to capture
contextual cues as effectively as human self-disclosures. These findings
highlight the limitations of LLMs in fine-grained emotion alignment and offer
insights for future research aimed at enhancing their contextual understanding.

</details>


### [33] [LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination](https://arxiv.org/abs/2509.09602)
*Yiqun T. Chen,Tyler H. McCormick,Li Liu,Abhirup Datta*

Main category: cs.CL

TL;DR: LA-VA目标是通过结合大语言模型和传统算法来提高质量评估的准确性，以满足资源有限地区的公共卫生需求。


<details>
  <summary>Details</summary>
Motivation: 质量评估在资源有限的地区很重要，但缺乏医疗证明。LA-VA目标通过结合大语言模型和传统算法来提高准确性。

Method: 使用PHMRC数据集，评估多种方法：GPT-5预测、LCVA基线、文本嵌入和元学习器集成。

Result: GPT-5表现最佳，成年人、儿童和新生儿的测试准确度分别为48.6%、50.5%和53.5%，超过传统统计机器学习基线5-10%。

Conclusion: 简单的商业大语言模型辅助方法可显著提高质量评估准确性，对低资源设施的全球健康监测有重要意义。

Abstract: Verbal autopsy (VA) is a critical tool for estimating causes of death in
resource-limited settings where medical certification is unavailable. This
study presents LA-VA, a proof-of-concept pipeline that combines Large Language
Models (LLMs) with traditional algorithmic approaches and embedding-based
classification for improved cause-of-death prediction. Using the Population
Health Metrics Research Consortium (PHMRC) dataset across three age categories
(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:
GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.
Our results demonstrate that GPT-5 achieves the highest individual performance
with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%
(Neonate), outperforming traditional statistical machine learning baselines by
5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches
could substantially improve verbal autopsy accuracy, with important
implications for global health surveillance in low-resource settings.

</details>


### [34] [Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems](https://arxiv.org/abs/2509.09629)
*Minghang Zhu,Zhengliang Shi,Zhiwei Xu,Shiguang Wu,Lingjie Wang,Pengjie Ren,Zhaochun Ren,Zhumin Chen*

Main category: cs.CL

TL;DR: MOAT是一个多智能体联合对齐调优框架，通过迭代对齐提升智能体协作能力，在六个基准测试中平均提升3.1%-4.4%


<details>
  <summary>Details</summary>
Motivation: 现有方法通常独立微调多智能体系统中的各个智能体，导致智能体间能力差距大、协作效果差

Method: 提出MOAT框架，交替进行规划智能体对齐（优化子目标生成）和接地智能体改进（使用多样化子目标-动作对微调）

Result: 在六个基准测试中超越最先进基线，在held-in任务上平均提升3.1%，在held-out任务上平均提升4.4%

Conclusion: MOAT通过迭代对齐方法有效解决了多智能体协作问题，理论分析证明其训练过程非递减且渐进收敛

Abstract: The advancement of large language models (LLMs) has enabled the construction
of multi-agent systems to solve complex tasks by dividing responsibilities
among specialized agents, such as a planning agent for subgoal generation and a
grounding agent for executing tool-use actions. Most existing methods typically
fine-tune these agents independently, leading to capability gaps among them
with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint
Alignment Tuning framework that improves agents collaboration through iterative
alignment. MOAT alternates between two key stages: (1) Planning Agent
Alignment, which optimizes the planning agent to generate subgoal sequences
that better guide the grounding agent; and (2) Grounding Agent Improving, which
fine-tunes the grounding agent using diverse subgoal-action pairs generated by
the agent itself to enhance its generalization capablity. Theoretical analysis
proves that MOAT ensures a non-decreasing and progressively convergent training
process. Experiments across six benchmarks demonstrate that MOAT outperforms
state-of-the-art baselines, achieving average improvements of 3.1% on held-in
tasks and 4.4% on held-out tasks.

</details>


### [35] [All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens](https://arxiv.org/abs/2509.09650)
*Siddarth Mamidanna,Daking Rai,Ziyu Yao,Yilun Zhou*

Main category: cs.CL

TL;DR: 本文通过Context-Aware Mean Ablation (CAMA)和Attention-Based Peeking (ABP)技术，在大语言模型的数学计算任务中发现了一个All-for-One子图(AF1)，该子图在深层仅通过最后一个token进行计算，且对模型性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在计算任务上表现出色，但其内部工作机制仍不明确。本文旨在探究模型在数学计算任务中实际的信息处理方式，特别是token间的计算和信息传递机制。

Method: 采用三步骤方法：抑制初始层的token特定计算、限制中间层token位置间的信息传递路径、强制剩余层在最后一个token进行计算。提出了CAMA和ABP两种技术来识别关键的All-for-One子图。

Result: 实验发现AF1子图在各种数学计算任务中具有高准确性，计算主要发生在深层且仅通过最后一个token进行，该子图对模型性能既充分又必要，且在不同模型和输入风格间具有可迁移性。

Conclusion: 研究揭示了LLMs在数学计算中的特定计算模式，AF1子图的发现为理解模型内部工作机制提供了重要见解，CAMA和ABP技术相比其他方法具有独特优势。

Abstract: Large language models (LLMs) demonstrate proficiency across numerous
computational tasks, yet their inner workings remain unclear. In theory, the
combination of causal self-attention and multilayer perceptron layers allows
every token to access and compute information based on all preceding tokens. In
practice, to what extent are such operations present? In this paper, on mental
math tasks (i.e., direct math calculation via next-token prediction without
explicit reasoning), we investigate this question in three steps: inhibiting
input-specific token computations in the initial layers, restricting the routes
of information transfer across token positions in the next few layers, and
forcing all computation to happen at the last token in the remaining layers.
With two proposed techniques, Context-Aware Mean Ablation (CAMA) and
Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with
high accuracy on a wide variety of mental math tasks, where meaningful
computation occurs very late (in terms of layer depth) and only at the last
token, which receives information of other tokens in few specific middle
layers. Experiments on a variety of models and arithmetic expressions show that
this subgraph is sufficient and necessary for high model performance, transfers
across different models, and works on a variety of input styles. Ablations on
different CAMA and ABP alternatives reveal their unique advantages over other
methods, which may be of independent interest.

</details>


### [36] [Steering MoE LLMs via Expert (De)Activation](https://arxiv.org/abs/2509.09660)
*Mohsen Fayyaz,Ali Modarressi,Hanieh Deilamsalehy,Franck Dernoncourt,Ryan Rossi,Trung Bui,Hinrich Schütze,Nanyun Peng*

Main category: cs.CL

TL;DR: SteerMoE框架通过检测和控制MoE模型中与特定行为相关的专家，无需重新训练即可在推理时控制LLM的忠实性和安全性等行为表现


<details>
  <summary>Details</summary>
Motivation: 现有的MoE模型在专家路由机制中存在行为关联的专家，这些专家可能隐藏着对齐伪造的风险，需要一种无需重新训练就能控制模型行为的方法

Method: 通过检测在对比行为输入对中表现出不同激活模式的专家，识别行为关联专家，然后在推理时选择性地激活或停用这些专家来控制模型行为

Result: 在11个基准测试和6个LLM上的实验显示，该方法可将安全性提升高达20%，忠实性提升27%；在对抗攻击模式下，单独使用可降低安全性41%，与现有越狱方法结合可完全绕过所有安全防护

Conclusion: SteerMoE揭示了MoE模型中专家层面存在的对齐伪造新维度，提供了一种无需修改权重即可控制模型行为的有效框架

Abstract: Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token
through a subset of specialized Feed-Forward Networks (FFN), known as experts.
We present SteerMoE, a framework for steering MoE models by detecting and
controlling behavior-linked experts. Our detection method identifies experts
with distinct activation patterns across paired inputs exhibiting contrasting
behaviors. By selectively (de)activating such experts during inference, we
control behaviors like faithfulness and safety without retraining or modifying
weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to
+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by
-41% alone, and -100% when combined with existing jailbreak methods, bypassing
all safety guardrails and exposing a new dimension of alignment faking hidden
within experts.

</details>


### [37] [CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2509.09675)
*Runpeng Dai,Linfeng Song,Haolin Liu,Zhenwen Liang,Dian Yu,Haitao Mi,Zhaopeng Tu,Rui Liu,Tong Zheng,Hongtu Zhu,Dong Yu*

Main category: cs.CL

TL;DR: 提出了好奇心驱动探索(CDE)框架，通过演员的困惑度和评论家的价值估计方差作为内在奖励，解决RLVR方法中的探索不足和熵崩溃问题，在AIME基准上比标准RLVR方法提升约3分


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法在增强大语言模型推理能力时存在探索不足的问题，导致过早收敛和熵崩溃，需要更好的探索机制

Method: 使用演员对生成响应的困惑度和评论家多头架构的价值估计方差作为内在好奇心信号，形成探索奖励，在RLVR框架中引导模型探索

Result: 在AIME基准测试中比标准RLVR(GRPO/PPO)方法提升约3分，理论分析显示该方法能惩罚过度自信错误并促进正确响应的多样性

Conclusion: CDE框架通过内在好奇心信号有效解决了RLVR的探索问题，揭示了RLVR中的校准崩溃机制，为理解LLM失败模式提供了新视角

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm
for enhancing the reasoning ability of Large Language Models (LLMs). Yet
current RLVR methods often explore poorly, leading to premature convergence and
entropy collapse. To address this challenge, we introduce Curiosity-Driven
Exploration (CDE), a framework that leverages the model's own intrinsic sense
of curiosity to guide exploration. We formalize curiosity with signals from
both the actor and the critic: for the actor, we use perplexity over its
generated response, and for the critic, we use the variance of value estimates
from a multi-head architecture. Both signals serve as an exploration bonus
within the RLVR framework to guide the model. Our theoretical analysis shows
that the actor-wise bonus inherently penalizes overconfident errors and
promotes diversity among correct responses; moreover, we connect the
critic-wise bonus to the well-established count-based exploration bonus in RL.
Empirically, our method achieves an approximate +3 point improvement over
standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a
calibration collapse mechanism within RLVR, shedding light on common LLM
failure modes.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [38] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: 本文开发了区间二型贝叶斯定理，使用保守方法避免输入不一致性，并提出新算法将专家提供的区间编码为二型模糊隶属函数。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯推断假设精确输入值，但现实应用中专家通常提供区间范围估计，需要扩展贝叶斯定理处理不确定性输入。

Method: 开发区间二型贝叶斯定理版本，使用保守方法避免输入不一致性；提出新算法将专家提供的区间编码为二型模糊隶属函数。

Result: 成功扩展了贝叶斯定理到区间二型版本，能够处理专家提供的区间输入概率，避免了传统方法的不一致性风险。

Conclusion: 提出的区间二型贝叶斯定理和编码算法为处理现实世界中的不确定性输入提供了有效工具，扩展了贝叶斯推断的应用范围。

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [39] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 使用NLP和多模态LLM将游戏设计文档自动转换为Unity游戏模板，通过细调LLaMA-3模型实现了高质量的C#代码生成


<details>
  <summary>Details</summary>
Motivation: 解决AI辅助游戏开发中从设计到实现的迁移问题，缩短游戏开发周期

Method: 细调LLaMA-3模型专门用于Unity代码生成，结合自定制Unity集成包，实现从GDD解析到代码生成的结构化处理流程

Result: 在编译成功率、GDD遵循度、最佳实践和代码模块性方面较基准模型有显著提升，平均评分4.8/5.0，支持多游戏类型

Conclusion: 该框架有效填补了AI辅助游戏开发的关键空白，使LLM成为从游戏设计到实现迁移过程的有力工具

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [40] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 使用多个专门化的LLM代理分解MiniZinc模型建模任务，每个代理处理特定全局约束类型，最后组装成完整模型，实验表明性能超过一次性提示和思维链提示方法


<details>
  <summary>Details</summary>
Motivation: 自然语言描述向MiniZinc模型的转换需要逻辑推理和约束编程专业知识，这个过程很具挑战性

Method: 提出一个多代理框架，使用专门化的LLM代理按全局约束类型分解建模任务，每个代理负责检测和生成特定类型的约束代码，最后由组装代理集成完整模型

Result: 初步实验显示该方法在多个LLM上都表现出更好的性能，超过了一次性提示和思维链提示等基准方法

Conclusion: 通过将复杂问题分解为更小的、明确定义的子任务，每个LLM可以处理更简单的推理挑战，并提出了未来工作的全面路线图

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [41] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 提出了一种基于置信度感知的截断交叉熵损失函数(TCE)，通过降低对高置信度预测的权重来有效延缓生成式AI模型在合成数据递归训练中的崩溃现象


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在训练中的比例不断增加，模型在合成数据上的递归训练会导致模型崩溃现象，现有缓解策略有限。研究发现模型对自身生成数据的过度自信是崩溃的关键驱动因素

Method: 提出截断交叉熵损失函数(TCE)，在训练过程中降低高置信度预测的权重，建立了与模型崩溃缓解相关的模型无关框架

Result: TCE显著延缓了递归训练中的模型崩溃，将模型崩溃前的保真度区间延长了2.3倍以上，且方法在不同模态上都具有通用性

Conclusion: 损失函数设计为在合成数据时代保持生成模型质量提供了简单而强大的工具，置信度感知方法能有效应对模型崩溃问题

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [42] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: 本文研究了可解释AI中的不确定性解释和全局解释，通过测试一种同时涵盖不确定性、鲁棒性和全局XAI概念的算法，来验证其校准信任的能力和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 虽然可解释AI(XAI)已被广泛研究，但不确定性解释和全局解释这两个领域相对较少被关注。研究者希望探索能够同时处理多个XAI概念的算法，并检验其是否能提供更直观的可视化理解和更高的用户满意度。

Method: 选择了一种能够同时处理不确定性、鲁棒性和全局XAI概念的算法进行测试，重点评估该算法在提供直观可视化理解方面的能力，尽管算法本身可能较为复杂。

Result: 研究发现这种旨在提供更直观视觉理解的算法，尽管理解起来可能较为复杂，但能够提供更高的用户满意度和人类可解释性。

Conclusion: 该研究强调了在XAI中同时考虑不确定性、鲁棒性和全局解释的重要性，并证明即使算法复杂，通过提供直观的可视化解释也能有效提高用户信任和满意度。

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [43] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: 本文提出了一种基于上下文条件提示的方法来优化大语言模型在冷启动推荐任务中的表现，通过最优样本注入和指令结构调整显著提升了few-shot场景下的推荐精度。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中冷启动用户因缺乏历史行为信息而导致推荐效果不佳的问题，探索如何通过提示工程优化大语言模型在低数据设置下的推荐性能。

Method: 提出上下文条件提示公式P(u, Ds)→R̂，其中u是冷启动用户画像，Ds是精选支持集，R̂是预测的物品排序列表。使用基于transformer的自回归大语言模型（BioGPT、LLaMA-2、GPT-4），采用token级对齐和嵌入空间正则化技术。

Result: 通过系统实验证明，最优样本注入和指令结构调整可以显著提高模型在低数据设置下的precision@k和NDCG分数，提示组合不仅具有语法意义还具有功能意义。

Conclusion: 基于提示的适配方法可以作为解决基于大语言模型的推荐系统中冷启动问题的一种有效途径，及时的提示组合能够直接控制注意力规模和推理过程中的解码器行为。

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [44] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: 人类、LLM和贝叶斯模型在动态协商中表现出不同行为模式，虽然总体效益相近但过程存在根本差异


<details>
  <summary>Details</summary>
Motivation: 评估自主协调代理在动态多代理环境中的表现和协商过程，比较不同类型代理的优势

Method: 在动态协商设置中比较人类(N=216)、LLM(GPT-4o, Gemini 1.5 Pro)和贝叶斯代理，捕捉结果和行为动态

Result: 贝叶斯代理通过突出优化获得最高剩余，但拒绝率高；LLM偏向保守进退交易，拒绝率低；人类采用更多战略性、冒险和公平导向行为

Conclusion: 绩效平等指标可能隐藏了过程和对齐方面的根本差异，这对实际部署至关重要

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [45] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: 本文提出了一种用于反洗钱(AML)的机器学习管道，通过16步设计和统计分析，在银行客户数据集中识别高风险客户，最终获得0.961的AUROC分数并在竞赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 反洗钱是金融机构的优先事项，机器学习在此领域具有巨大潜力。本文旨在开发一个系统化的ML管道来识别高风险银行客户。

Method: 采用16步设计和统计分析流程，构建SQLite数据库，开发基于SQL的特征工程算法，连接预训练模型并提供可解释AI模块来分析特征重要性。

Result: 管道实现了0.961的平均AUROC（标准差0.005），在多伦多大学2023-2024大数据与人工智能竞赛中获得第二名。

Conclusion: 提出的机器学习管道在反洗钱风险识别方面表现出色，具有高准确性和稳定性，为金融机构提供了有效的AML解决方案。

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [46] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于神经科学原理的计算框架，用于提升机器人等人工智能系统的空间推理能力，包含六个核心模块：生物受灵多模态感知、多感官整合、自我中心-绝对坐标转换、人工认知地图、空间记忆和空间推理。


<details>
  <summary>Details</summary>
Motivation: 当前自主AI系统在空间推理能力方面存在显著缺口，主要限于符号和序列处理，而人类空间智能基于多感官知觉、空间记忆和认知地图，能够在非结构化环境中进行灵活的上下文感知决策。缩小这一差距对于提升人工智能系统与物理三维世界的交互能力至关重要。

Method: 从计算神经科学角度审视现有空间神经模型，提出基于神经科学原理的计算框架，将核心生物功能映射到六个计算模块：生物受灵多模态感知、多感官整合、自我中心-绝对坐标转换、人工认知地图、空间记忆和空间推理。对最新方法进行框架引导的分析，评估其与每个模块的相关性并确定关键缺口。

Result: 该框架为虚拟和物理环境中的人工智能空间推理能力提供了一个全面的视角图景。通过对最新方法的分析，识别了阻碍基于神经科学的空间推理模块发展的关键缺口，并探讨了从虚拟到体现系统（如机器人）的潜在应用领域。

Conclusion: 该研究为人工智能空间推理领域提供了基于神经科学的视角和结构化路径，呈现了在动态或非结构化环境中实现通用空间推理能力的充满望的研究路线图，将有助于推动该领域的发展。

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [47] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: 提出ProgD方法，通过动态异构图建模和多尺度解码策略，解决多智能体运动预测中交互关系动态演化的问题，在多个基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了多智能体交互关系的动态演化特性，无法准确捕捉未来场景中不断变化的社会交互

Method: 使用动态异构图进行渐进式场景建模，设计分解式架构处理时空依赖关系，结合多尺度解码逐步消除未来运动不确定性

Result: 在INTERACTION多智能体预测基准中排名第一，在Argoverse 2多世界预测基准中达到最先进性能

Conclusion: ProgD方法通过显式建模动态交互关系，显著提升了多智能体运动预测的准确性和一致性

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [48] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: 基于区块链的多自治理架构，通过行为追踪、声誉评估和恶意行为预测三个核心模块，解决大语言模型自主组件的可靠性和可负责性挑战


<details>
  <summary>Details</summary>
Motivation: 大语言模型自主组件虽然在各领域带来重大机遇，但其不可预测的行为和异构能力造成了重大的管理和负责制挑战

Method: 提出三层架构（组件层、区块链数据层、监管应用层），包含行为追踪中裁、动态声誉评估和恶意行为预测三个核心模块

Result: 为大规模组件生态系统建立了可信、弹性和可扩展的监管机制系统基础

Conclusion: 该框架为多组件系统中的区块链监管提供了系统性基础，并提出了未来研究方向

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [49] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 通过从真实Jupyter笔记本提取高质量数据分析任务，构建NbQA数据集，并使用MCTS搜索框架Jupiter提升LLM在多步数据分析任务中的工具使用能力


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在数据科学工作流中多步推理和工具使用的挑战，提升复杂数据分析任务的效果

Method: 从真实Jupyter笔记本提取标准化任务-解决方案对，构建NbQA数据集，使用MCTS搜索框架Jupiter生成多样化解决轨迹进行价值模型学习

Result: Qwen2.5-7B和14B-Instruct模型在NbQA上分别解决77.82%和86.38%的任务，跟GPT-4o和高级代理框架相当或更好，在多步推理任务中显示更强的工具使用能力

Conclusion: 通过真实数据分析任务的标准化和MCTS搜索框架，显著提升了LLM在数据科学工作流中的多步推理和工具使用能力

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [50] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: 知识图构建方法对问答系统性能影响的对比研究，测试了spaCy、Stanford CoreNLP-OpenIE和GraphRAG三种方法的效果、可行性和适应性


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在处理复杂文本的主题性和整体理解方面有限，需要知识图来提升问答系统的深度分析能力

Method: 对比研究三种知识图构建方法：spaCy、Stanford CoreNLP-OpenIE和GraphRAG，分析它们的能力、发展状态和对LLM问答性能的影响

Result: 实验结果显示OpenIE提供了最全面的三元组覆盖，而GraphRAG在推理能力方面表现最优

Conclusion: 讨论了各方法的优缺点，并提供了改进知识图基于问答系统的未来发展方向

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [51] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: 将MCTS轨迹用于GRPO策略优化，提出基于树状结构的优势估计方法，解决优势饱和和奖励信号崩溃问题


<details>
  <summary>Details</summary>
Motivation: 探索如何将传统用于训练价值/奖励模型的MCTS轨迹重新用于改进基于偏好的强化学习中的策略优化

Method: 提出分阶段GRPO训练范式，使用部分揭示的MCTS rollout生成补全，引入新颖的树状结构优势估计方法

Result: 结构化优势估计可以稳定更新并更好反映组合推理质量，但仍面临优势饱和和奖励信号崩溃的挑战

Conclusion: 提出了启发式和统计解决方案来缓解这些问题，并讨论了在分阶段或树状奖励结构下学习的开放挑战

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [52] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: LightAgent是一个轻量级但功能强大的多智能体框架，解决了现有框架在灵活性和简洁性之间的权衡问题，集成了内存、工具和思维树等核心功能


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，多智能体系统在各种应用场景中取得了显著进展，但在设计多功能、鲁棒且高效的智能体部署平台方面仍存在重大挑战

Method: 提出了LightAgent框架，集成了Memory (mem0)、Tools和Tree of Thought (ToT)等核心功能，同时保持极轻量级结构，作为完全开源解决方案与主流聊天平台无缝集成

Result: 开发了一个轻量级但功能强大的智能体框架，使开发者能够轻松构建自学习智能体

Conclusion: LightAgent有效解决了现有框架在灵活性和简洁性之间的权衡问题，为多智能体系统提供了一个实用的部署平台解决方案

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [53] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: 该论文研究锦标赛中获胜者的认证解释问题，通过识别最小支持子锦标赛来解释为什么某个候选人在不同锦标赛规则下必然获胜。


<details>
  <summary>Details</summary>
Motivation: 锦标赛广泛应用于表示候选者之间的两两优势关系，但缺乏对获胜原因的正式解释。研究旨在为各种锦标赛规则提供认证的解释，回答"为什么获胜者会赢"这一可解释AI的核心问题。

Method: 识别最小支持子锦标赛（候选人在其中是必要获胜者的最小子锦标赛），针对多种锦标赛规则（顶级循环、无覆盖集、Copeland规则、Borda规则、极大极小规则、加权无覆盖集）进行分析。

Result: 确定了每种规则的最小支持子锦标赛大小，提出了多项式时间算法来计算除加权无覆盖集外的所有规则的最小支持，其中加权无覆盖集的问题是NP完全的。

Conclusion: 最小支持子锦标赛能够产生紧凑、认证且直观的解释，为锦标赛获胜提供了形式化的可解释性框架。

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [54] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: 研究空间协调三个维度（探索多样性、移动专业化、自适应空间接近度）对团队在受限通信环境下协作搜救任务表现的影响


<details>
  <summary>Details</summary>
Motivation: 许多团队（消防、军事、执法、应急响应）需要在没有视觉提示或广泛显式通信的情况下协调物理空间中的移动，但现有研究主要关注共址同步团队或知识工作协调

Method: 分析34个四人团队（136名参与者）在协作在线搜救任务中的数据，使用空间接近度、分布模式和移动对齐等指标来衡量团队协作的关系方面

Result: 空间专业化正向预测绩效，自适应空间接近度呈现边际倒U型关系（适度适应最优），这些指标的时态动态能够区分高低绩效团队

Conclusion: 研究为基于角色的团队工作中隐式空间协调提供了见解，强调了平衡自适应策略的重要性，对训练和AI辅助团队支持系统具有启示意义

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [55] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: TAM Bench是一个用于评估基于LLM的机器学习代理的多样化、现实化基准测试，通过自动化收集ML挑战任务，提供多维度评估框架和难度分级机制。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在任务覆盖、领域多样性、难度建模和评估严谨性方面存在不足，无法充分评估LLM代理在真实环境中的端到端ML工作流能力。

Method: 1) 基于浏览器自动化和LLM的任务采集系统，从Kaggle等平台自动收集结构化ML挑战；2) 基于排行榜的难度建模机制，利用参与人数和分数离散度估计任务复杂度；3) 包含性能、格式合规性、约束遵守和任务泛化的多维度评估框架。

Result: 基于150个精选AutoML任务构建了三个不同规模的基准子集（Lite、Medium、Full），其中Lite版本包含18个任务，在模态和难度级别上实现平衡覆盖。

Conclusion: TAM Bench提供了一个实用的测试平台，支持日常基准测试和比较研究，能够更全面地评估LLM代理在端到端机器学习工作流中的能力。

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [56] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: 通过集成视觉-语言模型和层状奖励函数的新题DRL架构，实现了资源效率高的语义探索能力


<details>
  <summary>Details</summary>
Motivation: 解决传统RL方法在语义探索中平衡效率和认知能力的挑战，应对复杂环境中资源限制下的智能探索需求

Method: 集成VLM通过层状奖励函数，将VLM查询模型化为专门动作，结合课程学习策略在不同复杂度级别指导学习

Result: 实验结果显示代理在物体发现率和语义富集区域导航方面取得显著提升，同时掌握了战略性主动查询外部信息的能力

Conclusion: 该研究提供了一种将常识性语义推理嵌入自主代理的实用可扩展方法，为完全智能自主探索提供了新方向

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [57] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: TORSO方法通过引导LLM利用内在推理能力生成响应，无需手动构建few-shot示例，在多个基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有few-shot提示方法过度依赖提供的示例，限制了模型内在推理能力的利用，且构建任务特定的few-shot提示成本高且在不同任务间存在不一致性

Method: 提出Template-Oriented Reasoning (TORSO)方法，引导大语言模型利用内部推理能力生成适当响应，无需手动构建few-shot示例

Result: 实验结果表明TORSO在多样化LLM基准测试中实现了强劲性能，并生成合理的推理过程

Conclusion: TORSO是一种有效的方法，能够在不依赖手动few-shot示例的情况下，激发LLM的内在推理能力，在多个任务上取得优异表现

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [58] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: 论文分析了大语言模型在法律领域中的幻觉问题，评估了RAG策略的效果和局限性，建议采用以验证性和可追溯性为核心的咨询式AI范式，强调人类监督的关键作用。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在法律应用中产生假信息（幻觉）的挑战，探索其根源、表现形式和有效的缓解策略。

Method: 分析RAG（Retrieval-Augmented Generation）缓解策略的效果性和局限性，提出整体性优化方案，考察法律领域的伦理和监管启示。

Result: 发现RAG策略存在局限性，需要更全面的优化；人类监督在法律AI应用中具有无可替代的作用。

Conclusion: 解决方案不在于求于增量改进生成模型，而是应采用以验证性和可追溯性为核心的"咨询式"AI范式，作为增强专业判断的工具，而非替代人类专业能力。

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [59] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: SEDM是一个自演化分布式记忆框架，通过可验证写入、动态记忆调度和跨域知识扩散来解决多智能体系统中记忆管理的噪声积累、无限扩张和泛化限制问题。


<details>
  <summary>Details</summary>
Motivation: 长期多智能体系统产生大量轨迹和历史交互，现有记忆管理方法存在噪声积累、记忆无限扩张和跨域泛化能力有限的问题，需要更高效可验证的记忆管理框架。

Method: SEDM框架包含三个核心组件：基于可重现回放的可验证写入准入、根据经验效用动态排序和整合条目的自调度记忆控制器、以及抽象可重用见解支持跨异构任务迁移的跨域知识扩散。

Result: 在基准数据集上的评估显示，SEDM相比强记忆基线提高了推理准确性并降低了token开销，同时能够将从事实验证中提炼的知识增强多跳推理能力。

Conclusion: SEDM作为一个可扩展和可持续的记忆机制，为开放式多智能体协作提供了有效的解决方案，将记忆从被动存储库转变为主动自优化的组件。

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [60] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: 量子模型在组合泛化任务中相比经典组合模型表现更好，特别是在使用多热编码的图像表示时取得了良好的概念验证结果。


<details>
  <summary>Details</summary>
Motivation: 组合泛化是人类认知的关键能力，但当前AI工具如视觉语言模型缺乏这种能力。量子模型训练效率更高，可能在此类任务中表现更好。

Method: 将组合张量模型的表示解释在希尔伯特空间中，训练变分量子电路学习这些表示。使用两种图像编码技术：多热编码和基于CLIP模型的角/幅度编码。

Result: 使用噪声多热编码获得了良好的概念验证结果。在CLIP图像向量上的表现较为复杂，但仍优于经典组合模型。

Conclusion: 量子模型在组合泛化任务中显示出潜力，特别是在特定编码方式下表现优异，为量子AI在自然语言处理中的应用提供了新思路。

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [61] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: Auras是一个算法-系统协同设计的推理框架，通过解耦感知和生成模块并提供管道并行性，显著提升具身AI系统的推理频率和吞吐量，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 传统顺序计算模式在确保准确性的同时，难以满足具身AI系统在动态环境中对高频输入输出的实时处理需求，限制了系统的"思考"频率。

Method: Auras框架解耦感知和生成模块，为它们提供受控的管道并行性以实现高且稳定的吞吐量。通过建立共享的公共上下文来解决并行性增加时出现的数据陈旧问题，保证具身代理的准确性。

Result: 实验结果显示，Auras平均将吞吐量提高了2.54倍，同时达到原始准确性的102.7%，有效克服了顺序计算的限制。

Conclusion: Auras框架通过算法-系统协同设计，成功实现了具身AI系统的高吞吐量推理，在保持准确性的同时显著提升了处理频率，为实时应用提供了有效解决方案。

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [62] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: 论文研究发现，大语言模型的单步准确率边际提升能在长任务中产生指数级改进。模型在长任务中的失败主要源于执行错误而非推理能力不足，且存在自我条件效应——模型在包含先前错误的上下文中更容易犯错。思维模型能避免自我条件效应，在单次推理中执行更长的任务。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型规模扩展是否带来收益递减，以及为什么模型在简单任务变长时会失败，尽管具备复杂推理能力。关注模型执行长序列任务的能力，而非单步推理准确性。

Method: 通过显式提供知识和计划来隔离执行能力，测试不同规模模型在多步任务中的表现。分析模型在长上下文中的错误模式，特别是自我条件效应。对比传统LLM与思维模型在执行长任务时的差异。

Result: 更大模型即使在单步准确率相同的情况下也能正确执行更多步骤。模型单步准确率随步骤增加而下降，这种下降不仅源于长上下文限制，还因为自我条件效应。思维模型不会自我条件，能在单次推理中执行更长的任务。

Conclusion: 模型规模扩展和顺序测试时计算对长视野任务有巨大益处。通过关注执行能力，可以解释LLM为何能解决复杂推理问题却在简单任务变长时失败。思维模型是解决长任务执行问题的有前景方向。

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [63] [Uncertainty Estimation using Variance-Gated Distributions](https://arxiv.org/abs/2509.08846)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 本文提出一种基于类概率分布信器比的不确定性评估和分解框架，通过汇聚模型预测的方差来量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统的贝叶斯或近似模型通过加性分解预测不确定性的方法存在疑问，需要更直观的不确定性评估方法来支持高风险应用的决策。

Method: 提出基于类概率分布信器比(SNR)的框架，引入方差门控测量，通过汇聚模型预测的方差来测量不确定性并计算信心因子。

Result: 该方法能够更直观地评估和分解不确定性，并用于讨论委员机器多样性冲击的存在问题。

Conclusion: 新框架为神经网络每个样本的不确定性量化提供了更直观的方法，有助于高风险应用中的可靠决策。

Abstract: Evaluation of per-sample uncertainty quantification from neural networks is
essential for decision-making involving high-risk applications. A common
approach is to use the predictive distribution from Bayesian or approximation
models and decompose the corresponding predictive uncertainty into epistemic
(model-related) and aleatoric (data-related) components. However, additive
decomposition has recently been questioned. In this work, we propose an
intuitive framework for uncertainty estimation and decomposition based on the
signal-to-noise ratio of class probability distributions across different model
predictions. We introduce a variance-gated measure that scales predictions by a
confidence factor derived from ensembles. We use this measure to discuss the
existence of a collapse in the diversity of committee machines.

</details>


### [64] [Instance-Optimal Matrix Multiplicative Weight Update and Its Quantum Applications](https://arxiv.org/abs/2509.08911)
*Weiyuan Gong,Tongyang Li,Xinzhao Wang,Zhiyu Zhang*

Main category: cs.LG

TL;DR: 本文提出了矩阵学习专家建议问题的新算法，通过改进的势函数和新的矩阵不等式，实现了比经典MMWU算法更优的实例相关遗憾界，且计算复杂度相同。


<details>
  <summary>Details</summary>
Motivation: 经典MMWU算法在矩阵版本的学习专家建议问题中只能达到最小最大最优遗憾界，但无法实现实例最优性能。本文旨在开发一个既能保持相同计算复杂度又能获得更好实例相关遗憾界的改进算法。

Method: 开发了基于势函数的通用框架，MMWU是其特例；提出了新的"单边"Jensen迹不等式；利用虚误差函数构造最优势函数；分析了内存下界并应用于量子学习理论。

Result: 新算法达到了O(√(T·S(X||d⁻¹I_d)))的实例最优遗憾界，比经典O(√(T log d))更好；在量子态学习、去极化噪声、随机量子态和Gibbs态学习等应用中表现优于现有技术。

Conclusion: 本文成功开发了矩阵LEA问题的改进算法，在保持计算效率的同时显著提升了性能，为量子学习理论提供了有力的新工具，能够处理非线性量子性质预测等复杂任务。

Abstract: The Matrix Multiplicative Weight Update (MMWU) is a seminal online learning
algorithm with numerous applications. Applied to the matrix version of the
Learning from Expert Advice (LEA) problem on the $d$-dimensional spectraplex,
it is well known that MMWU achieves the minimax-optimal regret bound of
$O(\sqrt{T\log d})$, where $T$ is the time horizon. In this paper, we present
an improved algorithm achieving the instance-optimal regret bound of
$O(\sqrt{T\cdot S(X||d^{-1}I_d)})$, where $X$ is the comparator in the regret,
$I_d$ is the identity matrix, and $S(\cdot||\cdot)$ denotes the quantum
relative entropy. Furthermore, our algorithm has the same computational
complexity as MMWU, indicating that the improvement in the regret bound is
``free''.
  Technically, we first develop a general potential-based framework for matrix
LEA, with MMWU being its special case induced by the standard exponential
potential. Then, the crux of our analysis is a new ``one-sided'' Jensen's trace
inequality built on a Laplace transform technique, which allows the application
of general potential functions beyond exponential to matrix LEA. Our algorithm
is finally induced by an optimal potential function from the vector LEA
problem, based on the imaginary error function.
  Complementing the above, we provide a memory lower bound for matrix LEA, and
explore the applications of our algorithm in quantum learning theory. We show
that it outperforms the state of the art for learning quantum states corrupted
by depolarization noise, random quantum states, and Gibbs states. In addition,
applying our algorithm to linearized convex losses enables predicting nonlinear
quantum properties, such as purity, quantum virtual cooling, and R\'{e}nyi-$2$
correlation.

</details>


### [65] [Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates](https://arxiv.org/abs/2509.08933)
*Sreejeet Maity,Aritra Mitra*

Main category: cs.LG

TL;DR: 该论文提出了对抗性腐败环境下鲁棒的Q-learning算法，在异步采样模型中实现了有限时间收敛保证，匹配了非对抗情况下的收敛速率，并建立了信息理论下界证明其最优性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法在奖励信号受到对抗性腐败（如极端噪声、传感器故障或恶意攻击）时性能严重下降，需要开发能够有效处理腐败奖励的鲁棒算法。

Method: 提出了新的鲁棒Q-learning变体算法，使用精炼的Azuma-Hoeffding不等式处理几乎鞅，不需要先验了解真实奖励分布的统计信息。

Result: 在异步采样模型下，即使存在对抗性腐败，算法的有限时间收敛速率与非对抗情况相匹配，仅增加一个与腐败样本比例成正比的附加项。

Conclusion: 该研究填补了鲁棒强化学习的重要空白，提供了异步Q-learning的第一个有限时间鲁棒性保证，所提出的技术工具可能具有独立的研究价值。

Abstract: We consider the problem of learning the optimal policy in a discounted,
infinite-horizon reinforcement learning (RL) setting where the reward signal is
subject to adversarial corruption. Such corruption, which may arise from
extreme noise, sensor faults, or malicious attacks, can severely degrade the
performance of classical algorithms such as Q-learning. To address this
challenge, we propose a new provably robust variant of the Q-learning algorithm
that operates effectively even when a fraction of the observed rewards are
arbitrarily perturbed by an adversary. Under the asynchronous sampling model
with time-correlated data, we establish that despite adversarial corruption,
the finite-time convergence rate of our algorithm matches that of existing
results for the non-adversarial case, up to an additive term proportional to
the fraction of corrupted samples. Moreover, we derive an information-theoretic
lower bound revealing that the additive corruption term in our upper bounds is
unavoidable.
  Next, we propose a variant of our algorithm that requires no prior knowledge
of the statistics of the true reward distributions. The analysis of this
setting is particularly challenging and is enabled by carefully exploiting a
refined Azuma-Hoeffding inequality for almost-martingales, a technical tool
that might be of independent interest. Collectively, our contributions provide
the first finite-time robustness guarantees for asynchronous Q-learning,
bridging a significant gap in robust RL.

</details>


### [66] [Group Distributionally Robust Machine Learning under Group Level Distributional Uncertainty](https://arxiv.org/abs/2509.08942)
*Xenia Konti,Yi Shen,Zifan Wang,Karl Henrik Johansson,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: 提出基于Wasserstein分布鲁棒优化的新框架，解决多源异构数据中群体分布不确定性下的最差群体性能优化问题


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在多源异构数据中容易学习虚假相关性，在非典型或欠代表群体上性能下降。现有最差群体优化方法假设群体分布可准确估计，但在噪声、非平稳和演化环境中这一假设常被违反

Method: 使用Wasserstein分布鲁棒优化(DRO)框架，考虑每个群体内的分布不确定性，同时保持优化最差群体性能的目标。开发了梯度下降-上升算法求解DRO问题

Result: 提供了算法收敛性证明，并在真实世界数据上验证了方法的有效性

Conclusion: 提出的框架能够有效处理群体分布不确定性，在噪声和非平稳环境中提升最差群体性能

Abstract: The performance of machine learning (ML) models critically depends on the
quality and representativeness of the training data. In applications with
multiple heterogeneous data generating sources, standard ML methods often learn
spurious correlations that perform well on average but degrade performance for
atypical or underrepresented groups. Prior work addresses this issue by
optimizing the worst-group performance. However, these approaches typically
assume that the underlying data distributions for each group can be accurately
estimated using the training data, a condition that is frequently violated in
noisy, non-stationary, and evolving environments. In this work, we propose a
novel framework that relies on Wasserstein-based distributionally robust
optimization (DRO) to account for the distributional uncertainty within each
group, while simultaneously preserving the objective of improving the
worst-group performance. We develop a gradient descent-ascent algorithm to
solve the proposed DRO problem and provide convergence results. Finally, we
validate the effectiveness of our method on real-world data.

</details>


### [67] [FoundationalECGNet: A Lightweight Foundational Model for ECG-based Multitask Cardiac Analysis](https://arxiv.org/abs/2509.08961)
*Md. Sajeebul Islam Sk.,Md Jobayer,Md Mehedi Hasan Shawon,Md. Golam Raibul Alam*

Main category: cs.LG

TL;DR: FoundationalECGNet是一个用于自动ECG分类的基础框架，通过多技术融合实现心电信号的时空特征提取，在正常/异常分类和多类心脏病检测中达到99% F1分数，提供可扩展的临床决策支持。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，现有ECG分析方法受噪声、类别不平衡和数据集异质性限制，需要开发更准确和可扩展的诊断系统。

Method: 整合Morlet和Daubechies小波变换的双阶段去噪、卷积块注意力模块(CBAM)、图注意力网络(GAT)和时间序列变换器(TST)，共同捕获多通道ECG信号的时空依赖性。

Result: 在多个数据集上，正常vs异常分类达到99% F1分数，多类疾病检测表现优异：传导障碍和肥厚99% F1分数，心律失常98.9% F1分数。

Conclusion: FoundationalECGNet提供了一个可扩展、可解释和通用性强的自动ECG分析解决方案，有望提高医疗环境中的诊断精度和患者预后。

Abstract: Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide,
underscoring the importance of accurate and scalable diagnostic systems.
Electrocardiogram (ECG) analysis is central to detecting cardiac abnormalities,
yet challenges such as noise, class imbalance, and dataset heterogeneity limit
current methods. To address these issues, we propose FoundationalECGNet, a
foundational framework for automated ECG classification. The model integrates a
dual-stage denoising by Morlet and Daubechies wavelets transformation,
Convolutional Block Attention Module (CBAM), Graph Attention Networks (GAT),
and Time Series Transformers (TST) to jointly capture spatial and temporal
dependencies in multi-channel ECG signals. FoundationalECGNet first
distinguishes between Normal and Abnormal ECG signals, and then classifies the
Abnormal signals into one of five cardiac conditions: Arrhythmias, Conduction
Disorders, Myocardial Infarction, QT Abnormalities, or Hypertrophy. Across
multiple datasets, the model achieves a 99% F1-score for Normal vs. Abnormal
classification and shows state-of-the-art performance in multi-class disease
detection, including a 99% F1-score for Conduction Disorders and Hypertrophy,
as well as a 98.9% F1-score for Arrhythmias. Additionally, the model provides
risk level estimations to facilitate clinical decision-making. In conclusion,
FoundationalECGNet represents a scalable, interpretable, and generalizable
solution for automated ECG analysis, with the potential to improve diagnostic
precision and patient outcomes in healthcare settings. We'll share the code
after acceptance.

</details>


### [68] [Value bounds and Convergence Analysis for Averages of LRP attributions](https://arxiv.org/abs/2509.08963)
*Alexander Binder,Nastaran Takmil-Homayouni,Urun Dogan*

Main category: cs.LG

TL;DR: 本文通过将LRP型归因方法表示为修正梯度矩阵的乘积，建立了与雅可比矩阵乘法的类比，推导了奇异值上界和归因图值的分量界，并获得了控制经验均值收敛到期望的乘法常数。


<details>
  <summary>Details</summary>
Motivation: 分析LRP型归因方法的数值特性，特别是研究归因值的分布规律和收敛性质，为多数据增强场景和Smoothgrad型方法提供理论依据。

Method: 将LRP归因方法表示为修正梯度矩阵的乘积，与链式法则的雅可比矩阵乘法建立类比，推导奇异值上界和分量界，获得控制收敛的乘法常数。

Result: 发现LRP-beta的常数与权重范数无关，这与基于梯度的方法和LRP-epsilon形成显著区别，对多数据增强和Smoothgrad型方法有重要影响。

Conclusion: LRP-beta方法在数值稳定性方面具有独特优势，其收敛常数不依赖于权重范数，这为实际应用中的归因方法选择提供了重要理论指导。

Abstract: We analyze numerical properties of Layer-wise relevance propagation
(LRP)-type attribution methods by representing them as a product of modified
gradient matrices. This representation creates an analogy to matrix
multiplications of Jacobi-matrices which arise from the chain rule of
differentiation. In order to shed light on the distribution of attribution
values, we derive upper bounds for singular values. Furthermore we derive
component-wise bounds for attribution map values. As a main result, we apply
these component-wise bounds to obtain multiplicative constants. These constants
govern the convergence of empirical means of attributions to expectations of
attribution maps. This finding has important implications for scenarios where
multiple non-geometric data augmentations are applied to individual test
samples, as well as for Smoothgrad-type attribution methods. In particular, our
analysis reveals that the constants for LRP-beta remain independent of weight
norms, a significant distinction from both gradient-based methods and
LRP-epsilon.

</details>


### [69] [Green Federated Learning via Carbon-Aware Client and Time Slot Scheduling](https://arxiv.org/abs/2509.08980)
*Daniel Richards Arputharaj,Charlotte Rodriguez,Angelo Rodio,Giovanni Neglia*

Main category: cs.LG

TL;DR: 该论文提出了一种联邦学习中碳感知的客户端选择和训练调度方法，通过利用碳强度的时间变化和宽松时间窗口来减少碳排放，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大规模机器学习训练产生大量碳排放，联邦学习通过分布式计算可以利用区域和时间上的碳强度变化来减少排放。

Method: 采用碳感知调度策略，利用宽松时间窗口允许适度延长训练时间，让客户端在低碳时段进行本地训练，并结合α-公平碳分配和全局微调阶段。

Result: 在真实碳强度数据上的实验表明，该方法优于不考虑宽松时间的基线，在广泛碳预算范围内获得更高模型精度，在严格碳约束下表现尤为突出。

Conclusion: 碳感知调度能有效减少联邦学习的碳排放，通过合理利用时间窗口和公平分配机制，可以在保持模型性能的同时实现显著的碳减排效果。

Abstract: Training large-scale machine learning models incurs substantial carbon
emissions. Federated Learning (FL), by distributing computation across
geographically dispersed clients, offers a natural framework to leverage
regional and temporal variations in Carbon Intensity (CI). This paper
investigates how to reduce emissions in FL through carbon-aware client
selection and training scheduling. We first quantify the emission savings of a
carbon-aware scheduling policy that leverages slack time -- permitting a modest
extension of the training duration so that clients can defer local training
rounds to lower-carbon periods. We then examine the performance trade-offs of
such scheduling which stem from statistical heterogeneity among clients,
selection bias in participation, and temporal correlation in model updates. To
leverage these trade-offs, we construct a carbon-aware scheduler that
integrates slack time, $\alpha$-fair carbon allocation, and a global
fine-tuning phase. Experiments on real-world CI data show that our scheduler
outperforms slack-agnostic baselines, achieving higher model accuracy across a
wide range of carbon budgets, with especially strong gains under tight carbon
constraints.

</details>


### [70] [ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning](https://arxiv.org/abs/2509.09534)
*Sena Ergisi,Luis Maßny,Rawad Bitar*

Main category: cs.LG

TL;DR: 提出了ProDiGy算法，一种基于双重评分系统的拜占庭鲁棒联邦学习方法，在非IID数据分布下表现出色


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据异构环境下容易受到对抗攻击，现有防御机制在非IID数据分布下效果不佳

Method: 使用基于梯度接近度和差异度的联合双重评分系统来评估客户端梯度，促进诚实客户端间的自然相似性，同时检测可疑的一致性作为攻击指标

Result: 在广泛的数值实验中，ProDiGy在各种场景下优于现有防御方法，特别是在非IID数据分布下仍能保持强大的防御能力和模型准确性

Conclusion: 双重视角方法的有效性得到验证，能够有效检测攻击并保持模型性能

Abstract: Federated Learning (FL) emerged as a widely studied paradigm for distributed
learning. Despite its many advantages, FL remains vulnerable to adversarial
attacks, especially under data heterogeneity. We propose a new Byzantine-robust
FL algorithm called ProDiGy. The key novelty lies in evaluating the client
gradients using a joint dual scoring system based on the gradients' proximity
and dissimilarity. We demonstrate through extensive numerical experiments that
ProDiGy outperforms existing defenses in various scenarios. In particular, when
the clients' data do not follow an IID distribution, while other defense
mechanisms fail, ProDiGy maintains strong defense capabilities and model
accuracy. These findings highlight the effectiveness of a dual perspective
approach that promotes natural similarity among honest clients while detecting
suspicious uniformity as a potential indicator of an attack.

</details>


### [71] [Active Learning and Explainable AI for Multi-Objective Optimization of Spin Coated Polymers](https://arxiv.org/abs/2509.08988)
*Brendan Young,Brendan Alvey,Andreas Werbrouck,Will Murphy,James Keller,Mattias J. Young,Matthew Maschmann*

Main category: cs.LG

TL;DR: 提出一个集成主动帕累托前沿学习算法(PyePAL)、可视化技术和可解释AI的框架，用于优化聚合物薄膜旋涂工艺参数，实现硬度和弹性的多目标优化。


<details>
  <summary>Details</summary>
Motivation: 旋涂聚合物薄膜实现特定力学性能本质上是一个多目标优化问题，需要同时优化硬度、弹性等多个性能指标，传统方法难以有效处理这种复杂的多目标优化挑战。

Method: 使用PyePAL算法结合高斯过程模型预测目标值，通过UMAP进行二维可视化，并采用模糊语言总结将参数与性能关系转化为语言描述，增强结果的可解释性。

Result: 实验结果表明该方法能有效识别有前景的聚合物设计，同时视觉和语言解释促进了专家驱动的分析和知识发现。

Conclusion: 该框架成功解决了聚合物薄膜旋涂的多目标优化问题，通过结合机器学习、可视化和可解释AI技术，为材料设计提供了有效的优化和解释工具。

Abstract: Spin coating polymer thin films to achieve specific mechanical properties is
inherently a multi-objective optimization problem. We present a framework that
integrates an active Pareto front learning algorithm (PyePAL) with
visualization and explainable AI techniques to optimize processing parameters.
PyePAL uses Gaussian process models to predict objective values (hardness and
elasticity) from the design variables (spin speed, dilution, and polymer
mixture), guiding the adaptive selection of samples toward promising regions of
the design space. To enable interpretable insights into the high-dimensional
design space, we utilize UMAP (Uniform Manifold Approximation and Projection)
for two-dimensional visualization of the Pareto front exploration.
Additionally, we incorporate fuzzy linguistic summaries, which translate the
learned relationships between process parameters and performance objectives
into linguistic statements, thus enhancing the explainability and understanding
of the optimization results. Experimental results demonstrate that our method
efficiently identifies promising polymer designs, while the visual and
linguistic explanations facilitate expert-driven analysis and knowledge
discovery.

</details>


### [72] [Fast attention mechanisms: a tale of parallelism](https://arxiv.org/abs/2509.09001)
*Jingwen Liu,Hantao Yu,Clayton Sanford,Alexandr Andoni,Daniel Hsu*

Main category: cs.LG

TL;DR: ANNA是一种近似最近邻注意力机制，具有次二次时间复杂度，能保持标准注意力的表达能力，同时解决关键推理任务。


<details>
  <summary>Details</summary>
Motivation: Transformer虽然能模拟大规模并行计算算法，但存在二次时间复杂度问题，严重限制了其可扩展性。

Method: 提出近似最近邻注意力机制(ANNA)，具有次二次时间复杂度，证明其能保持标准注意力的表达能力。

Result: ANNA-transformers能够解决Match2和k-hop等关键推理任务，具有接近最优的深度，并能模拟恒定深度的低秩transformers。

Conclusion: ANNA提供了一种统一的方式来推理各种高效注意力近似方法，在保持表达能力的同时显著降低了计算复杂度。

Abstract: Transformers have the representational capacity to simulate Massively
Parallel Computation (MPC) algorithms, but they suffer from quadratic time
complexity, which severely limits their scalability. We introduce an efficient
attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with
sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the
expressive power previously established for standard attention in terms of
matching the capabilities of MPC algorithms, and (2) can solve key reasoning
tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC
framework, we further prove that constant-depth ANNA-transformers can simulate
constant-depth low-rank transformers, thereby providing a unified way to reason
about a broad class of efficient attention approximations.

</details>


### [73] [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
*Marianna Nezhurina,Taishi Nakamura,Timur Carstensen,Niccolò Ajroldi,Ville Komulainen,David Salinas,Jenia Jitsev*

Main category: cs.LG

TL;DR: open-sci-ref是一个密集Transformer模型家族，在多个参数规模(0.13B-1.7B)和token规模(最高1T)上训练，在8个开放参考数据集上建立研究基线，为评估不同训练方法提供参考基准。


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供标准化的参考基准点，用于评估不同训练方法的合理性和质量，通过计算轴对齐实现训练过程的公平比较。

Method: 在8个近期开放参考数据集上训练密集Transformer模型，涵盖多个参数和token规模，提供中间检查点、训练日志和代码。

Result: NemoTron-CC HQ数据集训练效果最佳，其次是DCLM-baseline和FineWeb-Edu。建立了可复现的参考基线，支持训练动态研究。

Conclusion: open-sci-ref提供了标准化的研究基线，简化了复现过程，促进了训练方法的比较和未来研究的发展。

Abstract: We introduce open-sci-ref, a family of dense transformer models trained as
research baselines across multiple model (0.13B to 1.7B parameters) and token
scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on
various standardized benchmarks, our training runs set establishes reference
points that enable researchers to assess the sanity and quality of alternative
training approaches across scales and datasets. Intermediate checkpoints allow
comparison and studying of the training dynamics. The established reference
baselines allow training procedures to be compared through their scaling
trends, aligning them on a common compute axis. Comparison of open reference
datasets reveals that training on NemoTron-CC HQ consistently outperforms other
reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to
intermediate training checkpoints, the release includes logs, code, and
downstream evaluations to simplify reproduction, standardize comparison, and
facilitate future research.

</details>


### [74] [Deep Context-Conditioned Anomaly Detection for Tabular Data](https://arxiv.org/abs/2509.09030)
*Spencer King,Zhilu Zhang,Ruofan Yu,Baris Coskun,Wei Ding,Qian Cui*

Main category: cs.LG

TL;DR: 这篇论文提出了一种上下文条件化异常检测框架，通过自动识别上下文特征和深度自募编码器模型，在多个表格数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 在网络安全和金融等领域，面对大规模表格数据时，无监督异常检测面临巨大挑战。实际数据中存在异质上下文，全局稀罕事件在特定上下文下可能是正常的，但单一全局分布模型忽视了这些上下文细差。

Method: 提出上下文条件化异常检测框架，自动识别上下文特征，使用简单深度自募编码器建模条件数据分布。

Result: 在多个表格数据集上进行的广泛实验表明，该方法在异常检测性能上超过了现有最先进方法。

Conclusion: 该研究强调了考虑上下文信息对于准确区分异常与正常实例的重要性，提出的框架能够有效处理异质上下文的表格数据异常检测问题。

Abstract: Anomaly detection is critical in domains such as cybersecurity and finance,
especially when working with large-scale tabular data. Yet, unsupervised
anomaly detection -- where no labeled anomalies are available -- remains a
significant challenge. Although various deep learning methods have been
proposed to model a dataset's joint distribution, real-world tabular data often
contain heterogeneous contexts (e.g., different users), making globally rare
events normal under certain contexts. Consequently, relying on a single global
distribution can overlook these contextual nuances, degrading detection
performance. In this paper, we present a context-conditional anomaly detection
framework tailored for tabular datasets. Our approach automatically identifies
context features and models the conditional data distribution using a simple
deep autoencoder. Extensive experiments on multiple tabular benchmark datasets
demonstrate that our method outperforms state-of-the-art approaches,
underscoring the importance of context in accurately distinguishing anomalous
from normal instances.

</details>


### [75] [MoWE : A Mixture of Weather Experts](https://arxiv.org/abs/2509.09052)
*Dibyajyoti Chakraborty,Romit Maulik,Peter Harrington,Dallas Foster,Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 通过混合专家(MoWE)方法，使用Vision Transformer间控网络动态结合现有气象模型输出，以较低计算成本获得更准确的预报结果


<details>
  <summary>Details</summary>
Motivation: 数据驱动气象模型进展凝固，需要新方法突破现有限制，而非重新建立新预报器

Method: 使用Vision Transformer基础的间控网络，根据预报时长动态学习多个专家模型在每个格点的权重，最优化组合现有模型输出

Result: 在2天预报时效上，此方法比最佳AI气象模型进步10%（RMSE），显著超过单独专家模型和简单平均法

Conclusion: 提出了一种计算效率高、可扩展的策略，通过充分利用现有高质量预报模型来推动数据驱动气象预报技术的发展

Abstract: Data-driven weather models have recently achieved state-of-the-art
performance, yet progress has plateaued in recent years. This paper introduces
a Mixture of Experts (MoWE) approach as a novel paradigm to overcome these
limitations, not by creating a new forecaster, but by optimally combining the
outputs of existing models. The MoWE model is trained with significantly lower
computational resources than the individual experts. Our model employs a Vision
Transformer-based gating network that dynamically learns to weight the
contributions of multiple "expert" models at each grid point, conditioned on
forecast lead time. This approach creates a synthesized deterministic forecast
that is more accurate than any individual component in terms of Root Mean
Squared Error (RMSE). Our results demonstrate the effectiveness of this method,
achieving up to a 10% lower RMSE than the best-performing AI weather model on a
2-day forecast horizon, significantly outperforming individual experts as well
as a simple average across experts. This work presents a computationally
efficient and scalable strategy to push the state of the art in data-driven
weather prediction by making the most out of leading high-quality forecast
models.

</details>


### [76] [A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management](https://arxiv.org/abs/2509.09053)
*Julian Oelhaf,Georg Kordowich,Mehran Pashaei,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 这篇综述论文分析了机器学习在电力系统保护和扰动管理中的应用，发现虽然ML模型在模拟数据上表现良好，但缺乏真实世界验证，文献存在方法不一致和标准化不足的问题，提出了分类法和标准化指南。


<details>
  <summary>Details</summary>
Motivation: 可再生能源和分布式能源的集成改变了现代电力系统，对传统保护方案构成挑战，需要评估机器学习在电力系统保护中的应用现状和潜力。

Method: 采用PRISMA范围综述框架，基于100多篇文献进行综合分析，建立了面向机器学习的保护任务分类法，解决了术语不一致问题，并提出了标准化报告实践指南。

Result: ML模型在模拟数据集上通常表现出高精度，但在真实条件下的性能验证不足；现有文献在方法严谨性、数据集质量和评估指标方面存在不一致性；缺乏标准化阻碍了结果的可比性和发现的可推广性。

Conclusion: 需要优先考虑公共基准数据集、现实验证方法和先进ML架构，以推动基于机器学习的保护从理论承诺走向在日益动态和分散的电力系统中的实际部署。

Abstract: The integration of renewable and distributed energy resources reshapes modern
power systems, challenging conventional protection schemes. This scoping review
synthesizes recent literature on machine learning (ML) applications in power
system protection and disturbance management, following the PRISMA for Scoping
Reviews framework. Based on over 100 publications, three key objectives are
addressed: (i) assessing the scope of ML research in protection tasks; (ii)
evaluating ML performance across diverse operational scenarios; and (iii)
identifying methods suitable for evolving grid conditions. ML models often
demonstrate high accuracy on simulated datasets; however, their performance
under real-world conditions remains insufficiently validated. The existing
literature is fragmented, with inconsistencies in methodological rigor, dataset
quality, and evaluation metrics. This lack of standardization hampers the
comparability of results and limits the generalizability of findings. To
address these challenges, this review introduces a ML-oriented taxonomy for
protection tasks, resolves key terminological inconsistencies, and advocates
for standardized reporting practices. It further provides guidelines for
comprehensive dataset documentation, methodological transparency, and
consistent evaluation protocols, aiming to improve reproducibility and enhance
the practical relevance of research outcomes. Critical gaps remain, including
the scarcity of real-world validation, insufficient robustness testing, and
limited consideration of deployment feasibility. Future research should
prioritize public benchmark datasets, realistic validation methods, and
advanced ML architectures. These steps are essential to move ML-based
protection from theoretical promise to practical deployment in increasingly
dynamic and decentralized power systems.

</details>


### [77] [STRIDE: Scalable and Interpretable XAI via Subset-Free Functional Decomposition](https://arxiv.org/abs/2509.09070)
*Chaeyun Ko*

Main category: cs.LG

TL;DR: STRIDE是一个可扩展的XAI框架，通过RKHS中的正交函数分解来避免特征子集枚举，提供功能组件而非单一标量归因，在表格数据上实现快速且高保真的解释。


<details>
  <summary>Details</summary>
Motivation: 解决现有可解释AI框架的两个主要限制：特征子集枚举的指数级计算成本和将效应总结为单一标量值的表达能力不足。

Method: 基于再生核希尔伯特空间的正交函数分解，通过递归核中心化程序的分析投影方案计算功能组件f_S(x_S)，避免显式子集枚举。

Result: 在10个数据集上实现了0.6-9.7倍的速度提升（中位数约3倍），保持高保真度（R²在0.81-0.999之间）和良好的排序一致性。

Conclusion: STRIDE通过提供结构化功能视角补充了标量归因方法，支持新颖的诊断功能如'组件手术'来量化特定交互作用的影响。

Abstract: Most explainable AI (XAI) frameworks face two practical limitations: the
exponential cost of reasoning over feature subsets and the reduced
expressiveness of summarizing effects as single scalar values. We present
STRIDE, a scalable framework that aims to mitigate both issues by framing
explanation as a subset-enumeration-free, orthogonal functional decomposition
in a Reproducing Kernel Hilbert Space (RKHS). Rather than focusing only on
scalar attributions, STRIDE computes functional components f_S(x_S) via an
analytical projection scheme based on a recursive kernel-centering procedure,
avoiding explicit subset enumeration. In the tabular setups we study, the
approach is model-agnostic, provides both local and global views, and is
supported by theoretical results on orthogonality and L^2 convergence under
stated assumptions. On public tabular benchmarks in our environment, we
observed speedups ranging from 0.6 times (slower than TreeSHAP on a small
dataset) to 9.7 times (California), with a median approximate 3.0 times across
10 datasets, while maintaining high fidelity (R^2 between 0.81 and 0.999) and
substantial rank agreement on most datasets. Overall, STRIDE complements scalar
attribution methods by offering a structured functional perspective, enabling
novel diagnostics like 'component surgery' to quantitatively measure the impact
of specific interactions within our experimental scope.

</details>


### [78] ["A 6 or a 9?": Ensemble Learning Through the Multiplicity of Performant Models and Explanations](https://arxiv.org/abs/2509.09073)
*Gianlucca Zuin,Adriano Veloso*

Main category: cs.LG

TL;DR: 提出Rashomon Ensemble方法，通过从多个性能相近的模型中策略性选择，基于性能和解释进行分组，构建多样化集成模型以提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器学习中模型选择泛化能力好的模型仍然具有挑战性。Rashomon效应指多个模型在给定学习问题上表现相似的情况，这在现实场景中常见，需要找到方法来利用这种多样性提高泛化性能。

Method: Rashomon Ensemble方法：1）从多个高性能解决方案中策略性选择模型；2）基于模型性能和解释进行分组；3）构建集成模型以最大化多样性同时保持预测准确性；4）确保每个模型覆盖解空间的不同区域。

Result: 在开放和专有协作真实数据集上验证，在Rashomon比率较大的场景中实现了高达0.20+ AUROC的改进。在各种实际应用中展示了具体的商业效益，证明了方法的鲁棒性、实用性和有效性。

Conclusion: Rashomon Ensemble方法通过利用Rashomon效应中的模型多样性，构建的集成模型对分布偏移和未见数据变化具有更好的鲁棒性，在实际应用中表现出显著的性能提升和实用价值。

Abstract: Creating models from past observations and ensuring their effectiveness on
new data is the essence of machine learning. However, selecting models that
generalize well remains a challenging task. Related to this topic, the Rashomon
Effect refers to cases where multiple models perform similarly well for a given
learning problem. This often occurs in real-world scenarios, like the
manufacturing process or medical diagnosis, where diverse patterns in data lead
to multiple high-performing solutions. We propose the Rashomon Ensemble, a
method that strategically selects models from these diverse high-performing
solutions to improve generalization. By grouping models based on both their
performance and explanations, we construct ensembles that maximize diversity
while maintaining predictive accuracy. This selection ensures that each model
covers a distinct region of the solution space, making the ensemble more robust
to distribution shifts and variations in unseen data. We validate our approach
on both open and proprietary collaborative real-world datasets, demonstrating
up to 0.20+ AUROC improvements in scenarios where the Rashomon ratio is large.
Additionally, we demonstrate tangible benefits for businesses in various
real-world applications, highlighting the robustness, practicality, and
effectiveness of our approach.

</details>


### [79] [An entropy formula for the Deep Linear Network](https://arxiv.org/abs/2509.09088)
*Govind Menon,Tianmin Yu*

Main category: cs.LG

TL;DR: 本文研究深度线性网络的黎曼几何，为学习过程提供热力学描述基础。主要工具包括使用群作用分析过参数化，以及从参数空间到可观测空间的黎曼淹没。通过群轨道对平衡流形的叶状结构来定义和计算玻尔兹曼熵。


<details>
  <summary>Details</summary>
Motivation: 为深度学习过程建立热力学描述的理论基础，通过黎曼几何方法分析过参数化网络的几何结构，为理解学习动态提供新的数学框架。

Method: 使用群作用分析过参数化，采用黎曼淹没技术从参数空间映射到可观测空间，利用雅可比矩阵理论显式构造平衡流形切空间的正交基。

Result: 建立了深度线性网络的黎曼几何框架，证明了可观测空间上的黎曼几何可以通过平衡流形的黎曼淹没获得，实现了玻尔兹曼熵的定义和计算。

Conclusion: 该工作为深度学习提供了基于黎曼几何和热力学的理论分析工具，通过群作用和流形结构揭示了过参数化网络的几何特性，为后续研究奠定了基础。

Abstract: We study the Riemannian geometry of the Deep Linear Network (DLN) as a
foundation for a thermodynamic description of the learning process. The main
tools are the use of group actions to analyze overparametrization and the use
of Riemannian submersion from the space of parameters to the space of
observables. The foliation of the balanced manifold in the parameter space by
group orbits is used to define and compute a Boltzmann entropy. We also show
that the Riemannian geometry on the space of observables defined in [2] is
obtained by Riemannian submersion of the balanced manifold. The main technical
step is an explicit construction of an orthonormal basis for the tangent space
of the balanced manifold using the theory of Jacobi matrices.

</details>


### [80] [Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models](https://arxiv.org/abs/2509.09119)
*Hao Zhang,Bo Huang,Zhenjia Li,Xi Xiao,Hui Yi Leong,Zumeng Zhang,Xinwei Long,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: 基于敏感度的动态秩分配方法，提高LoRA精细调整效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 解决LoRA方法中统一秩分配的限制，以及现有秩分配技术计算效率低、复杂、不稳定的问题

Method: 提出Sensitivity-LoRA方法，基于权重矩阵的全局和局部敏感度动态分配秩，利用损失函数的二阶导数（Hessian矩阵）来抓取权重敏感性

Result: 实验结果表明Sensitivity-LoRA在多样化任务和测试集上都显示出稳健的效果、效率和稳定性

Conclusion: Sensitivity-LoRA为资源受限环境下的LLMs精细调整提供了一种高效、简洁且稳定的解决方案

Abstract: Large Language Models (LLMs) have transformed both everyday life and
scientific research. However, adapting LLMs from general-purpose models to
specialized tasks remains challenging, particularly in resource-constrained
environments. Low-Rank Adaptation (LoRA), a prominent method within
Parameter-Efficient Fine-Tuning (PEFT), has emerged as a promising approach to
LLMs by approximating model weight updates using low-rank decomposition.
However, LoRA is limited by its uniform rank ( r ) allocation to each
incremental matrix, and existing rank allocation techniques aimed at addressing
this issue remain computationally inefficient, complex, and unstable, hindering
practical applications. To address these limitations, we propose
Sensitivity-LoRA, an efficient fine-tuning method that dynamically allocates
ranks to weight matrices based on both their global and local sensitivities. It
leverages the second-order derivatives (Hessian Matrix) of the loss function to
effectively capture weight sensitivity, enabling optimal rank allocation with
minimal computational overhead. Our experimental results have demonstrated
robust effectiveness, efficiency and stability of Sensitivity-LoRA across
diverse tasks and benchmarks.

</details>


### [81] [Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction](https://arxiv.org/abs/2509.09128)
*Emam Hossain,Md Osman Gani*

Main category: cs.LG

TL;DR: 通过整合多元格兰杰因果关系和PCMCI+算法，提出了一种因果感知深度学习框架，在北极海冰预测中实现了更高准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 解决传统机器学习模型依赖相关性学习导致的问题：无法区分真实因果关系与偏偏相关、稳健性不足、可解释性差、沿射能力有限

Method: 整合多元格兰杰因果关系(MVGC)和PCMCI+算法进行因果特征选择，构建混合神经网络架构，基于43年北极海冰范围数据和海洋-大气变量

Result: 结果显示统计因果输入能够提高预测准确性和可解释性，适用于不同预测时间段，识别关键因果预测因子，减少不必要特征，提高计算效率

Conclusion: 该因果感知深度学习框架不仅在北极海冰预测中表现优异，还可扩展到其他动态高维预测领域，为因果感知预测模型提供了可扩展的理论基础和实践性能

Abstract: Conventional machine learning and deep learning models typically rely on
correlation-based learning, which often fails to distinguish genuine causal
relationships from spurious associations, limiting their robustness,
interpretability, and ability to generalize. To overcome these limitations, we
introduce a causality-aware deep learning framework that integrates
Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection
within a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic
Sea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily
and monthly resolutions, the proposed method identifies causally influential
predictors, prioritizes direct causes of SIE dynamics, reduces unnecessary
features, and enhances computational efficiency. Experimental results show that
incorporating causal inputs leads to improved prediction accuracy and
interpretability across varying lead times. While demonstrated on Arctic SIE
forecasting, the framework is broadly applicable to other dynamic,
high-dimensional domains, offering a scalable approach that advances both the
theoretical foundations and practical performance of causality-informed
predictive modeling.

</details>


### [82] [Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.09135)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Ahmed H. Qureshi,Husheng Li*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息神经网络(PINN)的连续时间多自主体强化学习框架(CT-MARL)，通过价值梯度迭代(VGI)模块提高了水势解的准确性，充分利用连续时间动力学系统的特性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在处理高频或不规则时间间隔的复杂动力学系统时遇到困难，而连续时间RL方法又主要限于单自主体领域，因为多自主体设置中存在维数灾难和价值函数近似困难。

Method: 使用PINN来近似HJB基础的价值函数，并通过价值梯度迭代(VGI)模块迭代精炼沿轨迹的价值梯度，使价值学习与梯度学习保持一致性。

Result: 在连续时间版本的多自主体粒子环境(MPE)和多自主体MuJoCo测试中，该方法一贯表现超过现有连续时间RL基线，能够扩展到复杂的多自主体动力学系统。

Conclusion: 该研究成功将连续时间RL扩展到多自主体领域，通过PINN和VGI模块有效解决了维数灾难和价值函数近似问题，为处理复杂动力学系统提供了新的解决方案。

Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical
systems that demand interactions at high frequencies or irregular time
intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by
replacing discrete-time Bellman recursion with differential value functions
defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation.
While CTRL has shown promise, its applications have been largely limited to the
single-agent domain. This limitation stems from two key challenges: (i)
conventional solution methods for HJB equations suffer from the curse of
dimensionality (CoD), making them intractable in high-dimensional systems; and
(ii) even with HJB-based learning approaches, accurately approximating
centralized value functions in multi-agent settings remains difficult, which in
turn destabilizes policy training. In this paper, we propose a CT-MARL
framework that uses physics-informed neural networks (PINNs) to approximate
HJB-based value functions at scale. To ensure the value is consistent with its
differential structure, we align value learning with value-gradient learning by
introducing a Value Gradient Iteration (VGI) module that iteratively refines
value gradients along trajectories. This improves gradient fidelity, in turn
yielding more accurate values and stronger policy learning. We evaluate our
method using continuous-time variants of standard benchmarks, including
multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results
demonstrate that our approach consistently outperforms existing continuous-time
RL baselines and scales to complex multi-agent dynamics.

</details>


### [83] [Peering Partner Recommendation for ISPs using Machine Learning](https://arxiv.org/abs/2509.09146)
*Md Ibrahim Ibne Alam,Ankur Senapati,Anindo Mahmood,Murat Yuksel,Koushik Kar*

Main category: cs.LG

TL;DR: 通过机器学习模型预测ISP间的对等关系，实现自动化选择对等伙伴，提高互联网连接效率


<details>
  <summary>Details</summary>
Motivation: 传统对等关系建立过程复杂耗时，ISPs需要更高效的方法来选择对等伙伴以优化全球连接性

Method: 使用PeeringDB、CAIDA等公开数据库收集ISP数据，比较树基模型、神经网络和Transformer三类机器学习模型的性能

Result: XGBoost树基模型表现最佳，预测准确率达98%，并且对时间、空间和数据缺失具有良好的强锐性

Conclusion: 该方法可以帮助ISPs完全自动化对等伙伴选择过程，推动互联网生态系统向更高效和优化的方向发展

Abstract: Internet service providers (ISPs) need to connect with other ISPs to provide
global connectivity services to their users. To ensure global connectivity,
ISPs can either use transit service(s) or establish direct peering
relationships between themselves via Internet exchange points (IXPs). Peering
offers more room for ISP-specific optimizations and is preferred, but it often
involves a lengthy and complex process. Automating peering partner selection
can enhance efficiency in the global Internet ecosystem. We explore the use of
publicly available data on ISPs to develop a machine learning (ML) model that
can predict whether an ISP pair should peer or not. At first, we explore public
databases, e.g., PeeringDB, CAIDA, etc., to gather data on ISPs. Then, we
evaluate the performance of three broad types of ML models for predicting
peering relationships: tree-based, neural network-based, and transformer-based.
Among these, we observe that tree-based models achieve the highest accuracy and
efficiency in our experiments. The XGBoost model trained with publicly
available data showed promising performance, with a 98% accuracy rate in
predicting peering partners. In addition, the model demonstrated great
resilience to variations in time, space, and missing data. We envision that
ISPs can adopt our method to fully automate the peering partner selection
process, thus transitioning to a more efficient and optimized Internet
ecosystem.

</details>


### [84] [HISPASpoof: A New Dataset For Spanish Speech Forensics](https://arxiv.org/abs/2509.09155)
*Maria Risques,Kratika Bhagtani,Amit Kumar Singh Yadav,Edward J. Delp*

Main category: cs.LG

TL;DR: 首个大规模西班牙语语音检测数据集HISPASpoof，包含多种口音和合成语音，解决西班牙语语音验真空白问题


<details>
  <summary>Details</summary>
Motivation: 虽然雷达的语音合成技术引发了法律风险，但目前的语音检测器主要集中在英语和中文，而全球过6亿人使用的西班牙语缺乏相应的验真技术

Method: 构建HISPASpoof数据集，包含公开语料库中6种口音的真实语音，以6种零样本TTS系统生成的合成语音，并评估五种代表性检测方法

Result: 发现基于英语训练的检测器在西班牙语上表现差，而使用HISPASpoof训练显著提高了检测性能，同时在合成语音源头转归任务中也取得了良好效果

Conclusion: HISPASpoof数据集为西班牙语语音验真领域提供了重要的基准测试平台，有助于推动更可靠和包容性语音法识技术的发展

Abstract: Zero-shot Voice Cloning (VC) and Text-to-Speech (TTS) methods have advanced
rapidly, enabling the generation of highly realistic synthetic speech and
raising serious concerns about their misuse. While numerous detectors have been
developed for English and Chinese, Spanish-spoken by over 600 million people
worldwide-remains underrepresented in speech forensics. To address this gap, we
introduce HISPASpoof, the first large-scale Spanish dataset designed for
synthetic speech detection and attribution. It includes real speech from public
corpora across six accents and synthetic speech generated with six zero-shot
TTS systems. We evaluate five representative methods, showing that detectors
trained on English fail to generalize to Spanish, while training on HISPASpoof
substantially improves detection. We also evaluate synthetic speech attribution
performance on HISPASpoof, i.e., identifying the generation method of synthetic
speech. HISPASpoof thus provides a critical benchmark for advancing reliable
and inclusive speech forensics in Spanish.

</details>


### [85] [Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication](https://arxiv.org/abs/2509.09168)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 这篇论文提出了一种无需训练的适应性token合并框架，通过多目标优化和贝叶斯优化来减少视觉transformer的计算开销和传输资源消耗，同时保持语义通信系统的准确性。


<details>
  <summary>Details</summary>
Motivation: 大规模transformer模型在语义通信中展现了强大能力，但其计算需求成为在资源受限的6G网络中实际部署的主要障碍。

Method: 提出一种无需训练的适应性token合并框架，将每层合并比例选择形式化为多目标优化问题，使用高斯过程贝叶斯优化来构建Pareto最优配置前沿。

Result: 实验结果显示该方法在广泛的信器比下都能显著减少浮点运算量，同时保持竞争性的准确度，超迈其他基准方法。

Conclusion: 这些发现为在未来边缘智能系统中部署基于transformer的语义通信提供了一种可扩展且高效的方法。

Abstract: Large-scale transformer models have emerged as a powerful tool for semantic
communication systems, enabling edge devices to extract rich representations
for robust inference across noisy wireless channels. However, their substantial
computational demands remain a major barrier to practical deployment in
resource-constrained 6G networks. In this paper, we present a training-free
framework for adaptive token merging in pretrained vision transformers to
jointly reduce inference time and transmission resource usage. We formulate the
selection of per-layer merging proportions as a multi-objective optimization
problem to balance accuracy and computational cost. We employ Gaussian
process-based Bayesian optimization to construct a Pareto frontier of optimal
configurations, enabling flexible runtime adaptation to dynamic application
requirements and channel conditions. Extensive experiments demonstrate that our
method consistently outperforms other baselines and achieves significant
reductions in floating-point operations while maintaining competitive accuracy
across a wide range of signal-to-noise ratio (SNR) conditions. Additional
results highlight the effectiveness of adaptive policies that adjust merging
aggressiveness in response to channel quality, providing a practical mechanism
to trade off latency and semantic fidelity on demand. These findings establish
a scalable and efficient approach for deploying transformer-based semantic
communication in future edge intelligence systems.

</details>


### [86] [Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning](https://arxiv.org/abs/2509.09176)
*Jun-Hao Chen,Yu-Chien Huang,Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Main category: cs.LG

TL;DR: 量子启发的QLSTM和QA3C混合模型在USD/TWD交易中取得11.87%收益，最大回撤仅0.92%，优于多个货币ETF


<details>
  <summary>Details</summary>
Motivation: 结合量子启发神经网络和深度强化学习，为金融交易提供新方法，特别是在外汇交易中寻求更好的风险调整后收益

Method: 集成量子长短期记忆网络(QLSTM)进行短期趋势预测，结合量子异步优势演员评论家(QA3C)算法，使用多核训练，状态设计包含QLSTM特征和技术指标

Result: 在2000-2025年数据上训练测试，纯多头策略获得11.87%的5年收益，最大回撤仅0.92%，表现优于多个货币ETF

Conclusion: 混合模型在外汇交易中具有竞争力，QLSTM特别适合小利润紧风险交易，未来可进一步优化量子模拟和策略复杂度

Abstract: The convergence of quantum-inspired neural networks and deep reinforcement
learning offers a promising avenue for financial trading. We implemented a
trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM)
for short-term trend prediction with Quantum Asynchronous Advantage
Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained
on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the
long-only agent achieves 11.87\% return over around 5 years with 0.92\% max
drawdown, outperforming several currency ETFs. We detail state design (QLSTM
features and indicators), reward function for trend-following/risk control, and
multi-core training. Results show hybrid models yield competitive FX trading
performance. Implications include QLSTM's effectiveness for small-profit trades
with tight risk and future enhancements. Key hyperparameters: QLSTM sequence
length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and
simplified strategy. \footnote{The views expressed in this article are those of
the authors and do not represent the views of Wells Fargo. This article is for
informational purposes only. Nothing contained in this article should be
construed as investment advice. Wells Fargo makes no express or implied
warranties and expressly disclaims all legal, tax, and accounting implications
related to this article.

</details>


### [87] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: FSPO是一种序列级强化学习方法，通过在重要性采样权重空间中实施长度公平裁剪来解决PPO/GRPO方法在序列长度处理上的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有序列级RL方法在移植PPO/GRPO式裁剪时存在固定裁剪范围系统性地重加权短响应与长响应的问题，导致有效目标失真。

Method: 提出FSPO方法，采用高斯启发式解决方案：用KL校正漂移项和√L缩放的带对序列对数IS比率进行裁剪。

Result: FSPO在多个评估数据集上平坦化了长度分箱中的裁剪率，稳定了训练，并优于所有基线方法。

Conclusion: FSPO通过理论形式化的长度重加权误差和方向余弦保证，有效解决了序列级RL中的长度公平性问题。

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [88] [Breaking the Statistical Similarity Trap in Extreme Convection Detection](https://arxiv.org/abs/2509.09195)
*Md Tanveer Hossain Munim*

Main category: cs.LG

TL;DR: 深度学习天气模型当前评估指标存在"统计相似性陷阱"，奖励模糊预测而激洋缺失稀有高影响事件。DART框架通过双解码器结构和专门优化，显著提升极端对流检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有天气预测模型评估指标存在统计相似性陷阱，无法有效检测稀有但高影响的极端天气事件，影响预警准确性。

Method: 提出DART框架，采用双解码器结构（背景/极端分解）、物理动机过采样和任务专用损失函数，将粗细度大气预测转换为高分辨率卫星亮温场。

Result: DART在极端对流检测上达到CSI=0.273（基线模型为0.00），偏差从6.72降至2.52，移除水气运输可提升270%检测性能，训练时间小于10分钟。

Conclusion: DART系统性解决了深度学习天气模型的统计相似性陷阱问题，为极端天气预警提供了可靠的AI解决方案，具有强应用潜力。

Abstract: Current evaluation metrics for deep learning weather models create a
"Statistical Similarity Trap", rewarding blurry predictions while missing rare,
high-impact events. We provide quantitative evidence of this trap, showing
sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous
convection detection. We introduce DART (Dual Architecture for Regression
Tasks), a framework addressing the challenge of transforming coarse atmospheric
forecasts into high-resolution satellite brightness temperature fields
optimized for extreme convection detection (below 220 K). DART employs
dual-decoder architecture with explicit background/extreme decomposition,
physically motivated oversampling, and task-specific loss functions. We present
four key findings: (1) empirical validation of the Statistical Similarity Trap
across multiple sophisticated baselines; (2) the "IVT Paradox", removing
Integrated Water Vapor Transport, widely regarded as essential for atmospheric
river analysis, improves extreme convection detection by 270%; (3)
architectural necessity demonstrated through operational flexibility (DART
achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent
CSI), and (4) real-world validation with the August 2023 Chittagong flooding
disaster as a case study. To our knowledge, this is the first work to
systematically address this hybrid conversion-segmentation-downscaling task,
with no direct prior benchmarks identified in existing literature. Our
validation against diverse statistical and deep learning baselines sufficiently
demonstrates DART's specialized design. The framework enables precise
operational calibration through beta-tuning, trains in under 10 minutes on
standard hardware, and integrates seamlessly with existing meteorological
workflows, demonstrating a pathway toward trustworthy AI for extreme weather
preparedness.

</details>


### [89] [Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning](https://arxiv.org/abs/2509.09208)
*Somnath Hazra,Pallab Dasgupta,Soumyajit Dey*

Main category: cs.LG

TL;DR: 提出IP3O算法，通过自适应激励机制和渐进惩罚策略解决约束强化学习中的训练不稳定问题，在约束边界附近实现更好的性能平衡


<details>
  <summary>Details</summary>
Motivation: 连续控制环境中，智能体在最大化回报和满足约束条件之间存在平衡难题，传统策略优化方法在约束边界附近表现不稳定，导致训练性能不佳

Method: 引入自适应激励机制，在接近约束边界前保持约束限制；提出IP3O算法，实施渐进增加的惩罚来稳定训练动态

Result: 在基准环境上的实证评估显示，IP3O相比最先进的安全RL算法表现出更好的效能

Conclusion: 该方法不仅在实践中有效，还通过理论分析提供了最优性最坏情况误差的界限保证，为约束强化学习提供了稳定可靠的解决方案

Abstract: Constrained Reinforcement Learning (RL) aims to maximize the return while
adhering to predefined constraint limits, which represent domain-specific
safety requirements. In continuous control settings, where learning agents
govern system actions, balancing the trade-off between reward maximization and
constraint satisfaction remains a significant challenge. Policy optimization
methods often exhibit instability near constraint boundaries, resulting in
suboptimal training performance. To address this issue, we introduce a novel
approach that integrates an adaptive incentive mechanism in addition to the
reward structure to stay within the constraint bound before approaching the
constraint boundary. Building on this insight, we propose Incrementally
Penalized Proximal Policy Optimization (IP3O), a practical algorithm that
enforces a progressively increasing penalty to stabilize training dynamics.
Through empirical evaluation on benchmark environments, we demonstrate the
efficacy of IP3O compared to the performance of state-of-the-art Safe RL
algorithms. Furthermore, we provide theoretical guarantees by deriving a bound
on the worst-case error of the optimality achieved by our algorithm.

</details>


### [90] [Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach](https://arxiv.org/abs/2509.09214)
*Alka Gadakh,Vidya Kumbhar,Sonal Khosla,Kumar Karunendra*

Main category: cs.LG

TL;DR: 该研究通过机器学习方法确定了农业旅游发展的关键指标，其中LASSO结合逻辑回归模型在两种数据分割方案下分别达到了98%和99%的最高准确率。


<details>
  <summary>Details</summary>
Motivation: 农业旅游作为促进农村发展的经济模式，需要系统研究其发展策略和关键指标以支持农民收入多样化和传统文化保护。

Method: 研究分两阶段：通过文献综述识别关键指标，然后使用LASSO等机器学习特征选择方法结合逻辑回归、决策树、随机森林和XGBoost等分类器进行分析。

Result: 在70-30%数据分割下，LASSO+逻辑回归模型达到98%准确率，968f机森林为95%；在80-20%数据分割下，逻辑回归达到99%准确率，决策树和XGBoost为97%。

Conclusion: 研究确认了农业旅游发展的关键指标，并证明机器学习方法在识别和优化这些指标方面具有高效性，特别是LASSO结合逻辑回归模型表现最优。

Abstract: Agro-tourism serves as a strategic economic model designed to facilitate
rural development by diversifying income streams for local communities like
farmers while promoting the conservation of indigenous cultural heritage and
traditional agricultural practices. As a very booming subdomain of tourism,
there is a need to study the strategies for the growth of Agro-tourism in
detail. The current study has identified the important indicators for the
growth and enhancement of agro-tourism. The study is conducted in two phases:
identification of the important indicators through a comprehensive literature
review and in the second phase state-of-the-art techniques were used to
identify the important indicators for the growth of agro-tourism. The
indicators are also called features synonymously, the machine learning models
for feature selection were applied and it was observed that the Least Absolute
Shrinkage and Selection Operator (LASSO) method combined with, the machine
Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT),
Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were
used to suggest the growth of the agro-tourism. The results show that with the
LASSO method, LR model gives the highest classification accuracy of 98% in
70-30% train-test data followed by RF with 95% accuracy. Similarly, in the
80-20% train-test data LR maintains the highest accuracy at 99%, while DT and
XGBoost follow with 97% accuracy.

</details>


### [91] [Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement](https://arxiv.org/abs/2509.09219)
*Jakob Nyberg,Pontus Johnson*

Main category: cs.LG

TL;DR: Vejde框架结合数据抽象、图神经网络和强化学习，为具有丰富结构化状态的决策问题生成归纳策略函数，在未见过的测试实例上表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决具有丰富结构化状态（如对象类和关系）的决策问题，需要能够处理不同规模和结构问题的归纳策略函数。

Method: 将MDP状态表示为实体事实数据库，转换为二分图，通过神经消息传递映射到潜在状态，使用监督学习和强化学习训练策略。

Result: Vejde策略在未见测试实例上平均泛化良好，得分接近针对特定实例训练的MLP代理，表现优于在线规划算法Prost。

Conclusion: Vejde框架能够有效处理结构化决策问题，实现良好的跨实例泛化，为复杂决策问题提供了有效的归纳策略学习方法。

Abstract: We present and evaluate Vejde; a framework which combines data abstraction,
graph neural networks and reinforcement learning to produce inductive policy
functions for decision problems with richly structured states, such as object
classes and relations. MDP states are represented as data bases of facts about
entities, and Vejde converts each state to a bipartite graph, which is mapped
to latent states through neural message passing. The factored representation of
both states and actions allows Vejde agents to handle problems of varying size
and structure. We tested Vejde agents on eight problem domains defined in RDDL,
with ten problem instances each, where policies were trained using both
supervised and reinforcement learning. To test policy generalization, we
separate problem instances in two sets, one for training and the other solely
for testing. Test results on unseen instances for the Vejde agents were
compared to MLP agents trained on each problem instance, as well as the online
planning algorithm Prost. Our results show that Vejde policies in average
generalize to the test instances without a significant loss in score.
Additionally, the inductive agents received scores on unseen test instances
that on average were close to the instance-specific MLP agents.

</details>


### [92] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: LDSim是一种通过知识蒸馏从LLM获取领域知识和推理能力来提升问答模拟器性能的方法，在保持高效推理的同时实现了更好的模拟效果


<details>
  <summary>Details</summary>
Motivation: 解决现有问答模拟器中LLM-free方法性能不佳而LLM-based方法推理速度慢、资源消耗高的问题，寻求在性能和效率之间的平衡

Method: 提出LLM蒸馏模拟器(LDSim)，通过知识蒸馏技术从大型语言模型中提取领域知识和推理能力来辅助预测，提升模拟性能

Result: 大量实验表明LDSim在模拟任务和知识追踪任务上都取得了强劲的结果

Conclusion: LDSim通过知识蒸馏有效平衡了问答模拟器的性能和效率，为教育推荐系统提供了高质量的模拟数据生成方案

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [93] [Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis](https://arxiv.org/abs/2509.09251)
*Hanyang Wang,Yuxuan Yang,Hongjun Wang,Lihui Wang*

Main category: cs.LG

TL;DR: 提出MMT-FD方法解决旋转机械故障诊断中标记样本稀缺和模型泛化能力不足的问题，通过多注意力元变换器和对比学习实现少样本无监督诊断


<details>
  <summary>Details</summary>
Motivation: 工业应用中获取大量标记故障数据困难且成本高，不同机械设备需要单独训练诊断模型，缺乏通用性

Method: 集成时频域编码器和元学习泛化模型，使用时频域随机增强预测状态表示，通过少量对比学习迭代优化，最后用少量标记数据微调

Result: 在轴承故障数据集和转子试验台数据上验证，仅用1%标记样本数据就达到99%故障诊断准确率，展现强大泛化能力

Conclusion: MMT-FD框架能有效提取无标记数据中的故障特征，具有高效率和强泛化能力，适用于多种机械设备的少样本故障诊断

Abstract: The intelligent fault diagnosis of rotating mechanical equipment usually
requires a large amount of labeled sample data. However, in practical
industrial applications, acquiring enough data is both challenging and
expensive in terms of time and cost. Moreover, different types of rotating
mechanical equipment with different unique mechanical properties, require
separate training of diagnostic models for each case. To address the challenges
of limited fault samples and the lack of generalizability in prediction models
for practical engineering applications, we propose a Multi-Attention Meta
Transformer method for few-shot unsupervised rotating machinery fault diagnosis
(MMT-FD). This framework extracts potential fault representations from
unlabeled data and demonstrates strong generalization capabilities, making it
suitable for diagnosing faults across various types of mechanical equipment.
The MMT-FD framework integrates a time-frequency domain encoder and a
meta-learning generalization model. The time-frequency domain encoder predicts
status representations generated through random augmentations in the
time-frequency domain. These enhanced data are then fed into a meta-learning
network for classification and generalization training, followed by fine-tuning
using a limited amount of labeled data. The model is iteratively optimized
using a small number of contrastive learning iterations, resulting in high
efficiency. To validate the framework, we conducted experiments on a bearing
fault dataset and rotor test bench data. The results demonstrate that the
MMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeled
sample data, exhibiting robust generalization capabilities.

</details>


### [94] [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)
*Jiawei Wang,Jiacai Liu,Yuqian Fu,Yingru Li,Xintao Wang,Yuan Lin,Yu Yue,Lin Zhang,Yang Wang,Ke Wang*

Main category: cs.LG

TL;DR: 本文提出Entropy-Modulated Policy Gradients (EMPG)框架，通过基于不确定性和任务结果重新校准学习信号，解决LLM智能体在长时程任务中因稀疏奖励导致的信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 长时程任务中，基于大语言模型的智能体面临稀疏奖励难以分配到中间步骤的问题。现有方法主要关注创建密集奖励信号，但忽略了LLM学习动态中策略梯度幅度与熵的内在耦合问题。

Method: 提出EMPG框架，基于步骤不确定性和最终任务结果重新校准学习信号：放大自信正确动作的更新，惩罚自信错误，衰减不确定步骤的更新以稳定探索。还引入了未来清晰度奖励项来鼓励寻找更可预测的解决方案路径。

Result: 在WebShop、ALFWorld和Deep Search三个具有挑战性的智能体任务上进行综合实验，EMPG实现了显著的性能提升，显著优于强策略梯度基线方法。

Conclusion: EMPG通过解耦策略梯度幅度与熵的耦合关系，有效解决了长时程任务中的信用分配问题，为基于LLM的智能体学习提供了更稳定和高效的训练框架。

Abstract: In long-horizon tasks, recent agents based on Large Language Models (LLMs)
face a significant challenge that sparse, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of LLMs: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/

</details>


### [95] [Data Driven Discovery of Emergent Dynamics in Reaction Diffusion Systems from Sparse and Noisy Observations](https://arxiv.org/abs/2509.09278)
*Saumitra Dwivedi,Ricardo da Silva Torres,Ibrahim A. Hameed,Gunnar Tufte,Anniken Susanne T. Karlsen*

Main category: cs.LG

TL;DR: 这篇论文提出了一种数据驱动的DRSALife框架，能够从观测数据中学习软人工生命模型，用于反应-正分布系统的出现动力学识别，而无需物理先验知识。


<details>
  <summary>Details</summary>
Motivation: 解决在没有物理先验知识的情况下，从观测数据中识别反应-正分布系统出现动力学的挑战。

Method: 使用DRSALife概念框架学习软人工生命模型（如基于代理的模型和细胞自动机）的规则集，并在噪声和稀疏数据下进行实验。

Result: 学习到的模型能够准确预测出现动力学（74%准确率），对高斯噪声和时间稀疏性表现强壮，还能识别底层偏微分方程的结构和参数。

Conclusion: DRSALife框架为无先验知识情况下的系统识别提供了有效方法，在噪声和稀疏数据环境中依然能够学习到准确的出现动力学模型。

Abstract: Data-driven discovery of emergent dynamics is gaining popularity,
particularly in the context of reaction-diffusion systems. These systems are
widely studied across various fields, including neuroscience, ecology,
epidemiology, and several other subject areas that deal with emergent dynamics.
A current challenge in the discovery process relates to system identification
when there is no prior knowledge of the underlying physics. We attempt to
address this challenge by learning Soft Artificial Life (Soft ALife) models,
such as Agent-based and Cellular Automata (CA) models, from observed data for
reaction-diffusion systems. In this paper, we present findings on the
applicability of a conceptual framework, the Data-driven Rulesets for Soft
Artificial Life (DRSALife) model, to learn Soft ALife rulesets that accurately
represent emergent dynamics in a reaction-diffusion system from observed data.
This model has demonstrated promising results for Elementary CA Rule 30, Game
of Life, and Vicsek Flocking problems in recent work. To our knowledge, this is
one of the few studies that explore machine-based Soft ALife ruleset learning
and system identification for reaction-diffusion dynamics without any prior
knowledge of the underlying physics. Moreover, we provide comprehensive
findings from experiments investigating the potential effects of using noisy
and sparse observed datasets on learning emergent dynamics. Additionally, we
successfully identify the structure and parameters of the underlying partial
differential equations (PDEs) representing these dynamics. Experimental results
demonstrate that the learned models are able to predict the emergent dynamics
with good accuracy (74%) and exhibit quite robust performance when subjected to
Gaussian noise and temporal sparsity.

</details>


### [96] [MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts](https://arxiv.org/abs/2509.09337)
*Junda Ye,Zhongbao Zhang,Li Sun,Siqiang Luo*

Main category: cs.LG

TL;DR: MoSE框架通过匿名游走提取子图，动态路由到专家网络，提升图神经网络的表达能力，在多种图任务中表现优异且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN依赖局部消息传递，难以捕捉复杂高阶子图模式，现有方法主要针对图级任务且配置固定，限制了灵活性和应用范围。

Method: 提出MoSE框架：使用匿名游走提取信息子图，基于结构语义动态路由到专门专家网络，理论证明在SWL测试中表达能力更强。

Result: 大量实验表明MoSE优于现有基线方法，可视化显示模型学习到了可解释的结构模式。

Conclusion: MoSE提供了灵活且表达力强的子图表示学习框架，在多种图任务中表现优异，同时具有良好的可解释性。

Abstract: While graph neural networks (GNNs) have achieved great success in learning
from graph-structured data, their reliance on local, pairwise message passing
restricts their ability to capture complex, high-order subgraph patterns.
leading to insufficient structural expressiveness. Recent efforts have
attempted to enhance structural expressiveness by integrating random walk
kernels into GNNs. However, these methods are inherently designed for
graph-level tasks, which limits their applicability to other downstream tasks
such as node classification. Moreover, their fixed kernel configurations hinder
the model's flexibility in capturing diverse subgraph structures. To address
these limitations, this paper proposes a novel Mixture of Subgraph Experts
(MoSE) framework for flexible and expressive subgraph-based representation
learning across diverse graph tasks. Specifically, MoSE extracts informative
subgraphs via anonymous walks and dynamically routes them to specialized
experts based on structural semantics, enabling the model to capture diverse
subgraph patterns with improved flexibility and interpretability. We further
provide a theoretical analysis of MoSE's expressivity within the Subgraph
Weisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL.
Extensive experiments, together with visualizations of learned subgraph
experts, demonstrate that MoSE not only outperforms competitive baselines but
also provides interpretable insights into structural patterns learned by the
model.

</details>


### [97] [Robust Non-Linear Correlations via Polynomial Regression](https://arxiv.org/abs/2509.09380)
*Luca Giuliani,Michele Lombardi*

Main category: cs.LG

TL;DR: 提出了一种基于多项式核的HGR相关系数计算方法，相比现有方法具有更好的鲁棒性和确定性，适用于实际机器学习应用中的约束优化。


<details>
  <summary>Details</summary>
Motivation: HGR相关系数能够捕捉非线性相关性，在算法公平性、科学分析和因果发现中有重要应用。但现有可微分计算方法存在偏差-方差权衡问题，影响方法鲁棒性，限制了实际应用。

Method: 使用用户可配置的多项式核来计算HGR相关系数，提供了比先前方法更强的鲁棒性，并实现了更快且几乎同等有效的限制条件。

Result: 新方法在鲁棒性和确定性方面具有显著优势，实验验证了其在约束机器学习框架中的适用性，计算得到的次梯度可作为损失正则化器。

Conclusion: 提出的多项式核方法为HGR相关系数的计算提供了更可靠的选择，特别适合实际应用场景中的约束机器学习任务。

Abstract: The Hirschfeld-Gebelein-R\'enyi (HGR) correlation coefficient is an extension
of Pearson's correlation that is not limited to linear correlations, with
potential applications in algorithmic fairness, scientific analysis, and causal
discovery. Recently, novel algorithms to estimate HGR in a differentiable
manner have been proposed to facilitate its use as a loss regularizer in
constrained machine learning applications. However, the inherent
uncomputability of HGR requires a bias-variance trade-off, which can possibly
compromise the robustness of the proposed methods, hence raising technical
concerns if applied in real-world scenarios. We introduce a novel computational
approach for HGR that relies on user-configurable polynomial kernels, offering
greater robustness compared to previous methods and featuring a faster yet
almost equally effective restriction. Our approach provides significant
advantages in terms of robustness and determinism, making it a more reliable
option for real-world applications. Moreover, we present a brief experimental
analysis to validate the applicability of our approach within a constrained
machine learning framework, showing that its computation yields an insightful
subgradient that can serve as a loss regularizer.

</details>


### [98] [MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization](https://arxiv.org/abs/2509.09387)
*Mohammed Tiouti,Mohamed Bal-Ghaoui*

Main category: cs.LG

TL;DR: MetaLLMiX是一个零样本超参数优化框架，结合元学习、可解释AI和高效LLM推理，无需额外试验即可推荐最优超参数和预训练模型，大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的AutoML方法依赖试错和昂贵API，缺乏可解释性和通用性，需要更高效的自动化深度学习模型选择方案。

Method: 利用历史实验结果的SHAP解释，结合元学习和LLM推理，采用LLM-as-judge评估控制输出格式、准确性和完整性。

Result: 在8个医学影像数据集上，MetaLLMiX达到或超越传统HPO方法性能，计算成本大幅降低，响应时间减少99.6-99.9%，在5/8任务上获得最优结果，训练速度提升2.4-15.7倍。

Conclusion: MetaLLMiX提供了一种高效、低成本的超参数优化解决方案，在保持精度的同时显著提升计算效率，为自动化深度学习模型选择提供了新途径。

Abstract: Effective model and hyperparameter selection remains a major challenge in
deep learning, often requiring extensive expertise and computation. While
AutoML and large language models (LLMs) promise automation, current LLM-based
approaches rely on trial and error and expensive APIs, which provide limited
interpretability and generalizability. We propose MetaLLMiX, a zero-shot
hyperparameter optimization framework combining meta-learning, explainable AI,
and efficient LLM reasoning. By leveraging historical experiment outcomes with
SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained
models without additional trials. We further employ an LLM-as-judge evaluation
to control output format, accuracy, and completeness. Experiments on eight
medical imaging datasets using nine open-source lightweight LLMs show that
MetaLLMiX achieves competitive or superior performance to traditional HPO
methods while drastically reducing computational cost. Our local deployment
outperforms prior API-based approaches, achieving optimal results on 5 of 8
tasks, response time reductions of 99.6-99.9%, and the fastest training times
on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of
best-performing baselines.

</details>


### [99] [LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations](https://arxiv.org/abs/2509.09396)
*Harry Mayne,Ryan Othniel Kearns,Yushi Yang,Andrew M. Bean,Eoin Delaney,Chris Russell,Adam Mahdi*

Main category: cs.LG

TL;DR: 研究发现LLM生成的反事实解释存在有效性-最小性权衡问题，要么修改过多缺乏洞察力，要么修改过少无法改变预测结果，表明自生成反事实解释作为可解释性工具效果有限甚至可能误导。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型如何通过自生成反事实解释来向人类解释其决策过程，以促进有效的人机协作。

Method: 评估LLM生成的自生成反事实解释(SCEs)的有效性和最小性，即在保持预测结果改变的前提下对输入进行最小修改。

Result: LLM通常能生成有效的反事实解释，但修改幅度远非最小；当要求生成最小修改时，又往往修改过小导致预测结果不变。这种有效性-最小性权衡在多个模型、数据集和评估设置中一致存在。

Conclusion: 自生成反事实解释作为可解释性工具效果有限，在高风险场景部署LLM时需要考虑不可靠的自解释对下游决策的影响。

Abstract: To collaborate effectively with humans, language models must be able to
explain their decisions in natural language. We study a specific type of
self-explanation: self-generated counterfactual explanations (SCEs), where a
model explains its prediction by modifying the input such that it would have
predicted a different outcome. We evaluate whether LLMs can produce SCEs that
are valid, achieving the intended outcome, and minimal, modifying the input no
more than necessary. When asked to generate counterfactuals, we find that LLMs
typically produce SCEs that are valid, but far from minimal, offering little
insight into their decision-making behaviour. Worryingly, when asked to
generate minimal counterfactuals, LLMs typically make excessively small edits
that fail to change predictions. The observed validity-minimality trade-off is
consistent across several LLMs, datasets, and evaluation settings. Our findings
suggest that SCEs are, at best, an ineffective explainability tool and, at
worst, can provide misleading insights into model behaviour. Proposals to
deploy LLMs in high-stakes settings must consider the impact of unreliable
self-explanations on downstream decision-making. Our code is available at
https://github.com/HarryMayne/SCEs.

</details>


### [100] [Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping](https://arxiv.org/abs/2509.09408)
*Jonas Schmidinger,Viacheslav Barkov,Sebastian Vogel,Martin Atzmueller,Gerard B M Heuvelink*

Main category: cs.LG

TL;DR: 提出了一种结合机器学习和地质统计学的混合框架KpR，通过空间滞后特征增强ML模型，在土壤属性预测中显著提升精度和不确定性估计


<details>
  <summary>Details</summary>
Motivation: 机器学习和地质统计学是两种不同的土壤属性预测框架，前者依赖环境特征关系，后者利用空间结构。需要结合两者优势来提升数字土壤制图性能

Method: 提出Kriging Prior Regression (KpR)方法，通过普通克里金法生成'空间滞后'特征来丰富机器学习模型的空间上下文，使用TabPFN模型在六个田间数据集上进行评估

Result: KpR与TabPFN组合相比其他空间技术和非空间机器学习算法（如随机森林）具有更可靠的uncertainty估计和更准确的预测，平均R2提升约30%

Conclusion: KpR与TabPFN是一个强大且通用的数字土壤制图建模框架，特别适用于样本量小的精准农业场景，能够补偿近端土壤传感数据有限时的弱关系问题

Abstract: Machine learning and geostatistics are two fundamentally different frameworks
for predicting and spatially mapping soil properties. Geostatistics leverages
the spatial structure of soil properties, while machine learning captures the
relationship between available environmental features and soil properties. We
propose a hybrid framework that enriches ML with spatial context through
engineering of 'spatial lag' features from ordinary kriging. We call this
approach 'kriging prior regression' (KpR), as it follows the inverse logic of
regression kriging. To evaluate this approach, we assessed both the point and
probabilistic prediction performance of KpR, using the TabPFN model across six
fieldscale datasets from LimeSoDa. These datasets included soil organic carbon,
clay content, and pH, along with features derived from remote sensing and
in-situ proximal soil sensing. KpR with TabPFN demonstrated reliable
uncertainty estimates and more accurate predictions in comparison to several
other spatial techniques (e.g., regression/residual kriging with TabPFN), as
well as to established non-spatial machine learning algorithms (e.g., random
forest). Most notably, it significantly improved the average R2 by around 30%
compared to machine learning algorithms without spatial context. This
improvement was due to the strong prediction performance of the TabPFN
algorithm itself and the complementary spatial information provided by KpR
features. TabPFN is particularly effective for prediction tasks with small
sample sizes, common in precision agriculture, whereas KpR can compensate for
weak relationships between sensing features and soil properties when proximal
soil sensing data are limited. Hence, we conclude that KpR with TabPFN is a
very robust and versatile modelling framework for digital soil mapping in
precision agriculture.

</details>


### [101] [Fused Lasso Improves Accuracy of Co-occurrence Network Inference in Grouped Samples](https://arxiv.org/abs/2509.09413)
*Daniel Agyapong,Briana H. Beatty,Peter G. Kennedy,Toby D. Hocking*

Main category: cs.LG

TL;DR: 本文提出了fuser算法，用于微生物群落关联网络推断，解决传统算法在不同生态环境下的逆逆性问题，能够在同质环境中保持类似性能，同时在跨环境场景中显著降低测试误差。


<details>
  <summary>Details</summary>
Motivation: 传统的共现网络推断算法通常只分析单一环境尾的样本，捕获静态晚照，而不能涵盖微生物群落在不同生态条件下的动态调整过程。之前的研究常将不同环境的样本混合分析，没有充分考虑微生物关联在不同生态条件下的适应性变化。

Method: 研究分析了多个地点和时间点的公开微生物组丰度数据，使用Same-All交叉验证(SAC)框架评估算法性能。SAC包含两种场景：同一环境内训练测试(Same)和多环境数据混合训练测试(All)。提出fuser算法，在训练过程中保留子样本特异信号的同时共享环境间相关信息，生成环境特异性预测网络。

Result: fuser算法在同质环境(Same)中与glmnet等现有算法拥有相似的预测性能，而在跨环境(All)场景中显著降低了测试误差，超过了基准算法。

Conclusion: fuser算法能够有效处理微生物群落关联网络推断中的环境特异性问题，在保持同质环境性能的同时，显著提升了跨环境预测能力，为微生物群落动态过程研究提供了更好的工具。

Abstract: Co-occurrence network inference algorithms have significantly advanced our
understanding of microbiome communities. However, these algorithms typically
analyze microbial associations within samples collected from a single
environmental niche, often capturing only static snapshots rather than dynamic
microbial processes. Previous studies have commonly grouped samples from
different environmental niches together without fully considering how microbial
communities adapt their associations when faced with varying ecological
conditions. Our study addresses this limitation by explicitly investigating
both spatial and temporal dynamics of microbial communities. We analyzed
publicly available microbiome abundance data across multiple locations and time
points, to evaluate algorithm performance in predicting microbial associations
using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates
algorithms in two distinct scenarios: training and testing within the same
environmental niche (Same), and training and testing on combined data from
multiple environmental niches (All). To overcome the limitations of
conventional algorithms, we propose fuser, an algorithm that, while not
entirely new in machine learning, is novel for microbiome community network
inference. It retains subsample-specific signals while simultaneously sharing
relevant information across environments during training. Unlike standard
approaches that infer a single generalized network from combined data, fuser
generates distinct, environment-specific predictive networks. Our results
demonstrate that fuser achieves comparable predictive performance to existing
algorithms such as glmnet when evaluated within homogeneous environments
(Same), and notably reduces test error compared to baseline algorithms in
cross-environment (All) scenarios.

</details>


### [102] [Composable Score-based Graph Diffusion Model for Multi-Conditional Molecular Generation](https://arxiv.org/abs/2509.09451)
*Anjie Qiao,Zhen Wang,Chuan Chen,DeFu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: CSGD是一个基于可组合分数匹配的图扩散模型，通过具体分数实现离散图生成，在分子设计中提供细粒度的多条件控制


<details>
  <summary>Details</summary>
Motivation: 解决现有图扩散模型在多条件设置下效果有限的问题，传统方法依赖联合条件或连续松弛，会损害生成保真度

Method: 提出CSGD模型，将分数匹配扩展到离散图；引入Composable Guidance (CoG)实现采样时对任意条件子集的细粒度控制；使用Probability Calibration (PC)调整转移概率以减少训练-测试不匹配

Result: 在四个分子数据集上达到最先进性能，可控性比先前方法平均提升15.3%，同时保持高有效性和分布保真度

Conclusion: 基于分数的建模在离散图生成中具有实际优势，能够实现灵活的多属性分子设计

Abstract: Controllable molecular graph generation is essential for material and drug
discovery, where generated molecules must satisfy diverse property constraints.
While recent advances in graph diffusion models have improved generation
quality, their effectiveness in multi-conditional settings remains limited due
to reliance on joint conditioning or continuous relaxations that compromise
fidelity. To address these limitations, we propose Composable Score-based Graph
Diffusion model (CSGD), the first model that extends score matching to discrete
graphs via concrete scores, enabling flexible and principled manipulation of
conditional guidance. Building on this foundation, we introduce two score-based
techniques: Composable Guidance (CoG), which allows fine-grained control over
arbitrary subsets of conditions during sampling, and Probability Calibration
(PC), which adjusts estimated transition probabilities to mitigate train-test
mismatches. Empirical results on four molecular datasets show that CSGD
achieves state-of-the-art performance, with a 15.3% average improvement in
controllability over prior methods, while maintaining high validity and
distributional fidelity. Our findings highlight the practical advantages of
score-based modeling for discrete graph generation and its capacity for
flexible, multi-property molecular design.

</details>


### [103] [AquaCast: Urban Water Dynamics Forecasting with Precipitation-Informed Multi-Input Transformer](https://arxiv.org/abs/2509.09458)
*Golnoosh Abdollahinejad,Saleh Baghersalimi,Denisa-Andreea Constantinescu,Sergey Shevchik,David Atienza*

Main category: cs.LG

TL;DR: AquaCast模型通过融合内源变量和外源因素，实现了城市水动力学的多输入多输出深度学习预测，在真实和合成数据集上均优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统城市水动力学预测方法无法有效捕捉变量间和时间依赖关系的问题，特别是如何整合外源因素（如降水历史）而不需要预测这些外源变量。

Method: 开发多输入多输出深度学习模型AquaCast，使用嵌入层融合外源输入，专注于内源变量预测，同时捕捉所有输入的变量间和时间依赖性。

Result: 在LausanneCity数据集上达到最先进性能，加入外源变量后性能进一步提升。在三个大规模合成数据集上的测试证实了模型的泛化能力和可扩展性。

Conclusion: AquaCast模型能够稳健准确地预测城市水动力学，在真实和合成环境中均优于现有方法，证明了其处理复杂时间依赖关系的能力。

Abstract: This work addresses the challenge of forecasting urban water dynamics by
developing a multi-input, multi-output deep learning model that incorporates
both endogenous variables (e.g., water height or discharge) and exogenous
factors (e.g., precipitation history and forecast reports). Unlike conventional
forecasting, the proposed model, AquaCast, captures both inter-variable and
temporal dependencies across all inputs, while focusing forecast solely on
endogenous variables. Exogenous inputs are fused via an embedding layer,
eliminating the need to forecast them and enabling the model to attend to their
short-term influences more effectively. We evaluate our approach on the
LausanneCity dataset, which includes measurements from four urban drainage
sensors, and demonstrate state-of-the-art performance when using only
endogenous variables. Performance also improves with the inclusion of exogenous
variables and forecast reports. To assess generalization and scalability, we
additionally test the model on three large-scale synthesized datasets,
generated from MeteoSwiss records, the Lorenz Attractors model, and the Random
Fields model, each representing a different level of temporal complexity across
100 nodes. The results confirm that our model consistently outperforms existing
baselines and maintains a robust and accurate forecast across both real and
synthetic datasets.

</details>


### [104] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: 一种全自动化的AI系统，利用专门设计的Agent-E代理从会议议程中识别特定地理区域的论文，并通过RPA完成预定动作，在586篇论文中实现了100%的可召回率和99.4%的准确率。


<details>
  <summary>Details</summary>
Motivation: 应对学术文献快速增长带来的挑战，减少研究人员、资助机构和学术社会在学术发现过程中的手工劳动。

Method: 开发了一种从数据发现到直接执行的全自动化系统，使用专门的AI代理Agent-E识别特定地理区域的论文，然后通过机器人过程自动化(RPA)执行预定动作。

Result: 在5个不同会议的586篇论文上验证，系统成功识别了所有目标论文，可召回率达100%，准确率达99.4%。

Conclusion: 这一实验展示了任务导向的AI代理不仅能够过滤信息，还能积极参与并加速学术社群的工作流程。

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [105] [CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2509.09474)
*Julia Gastinger,Christian Meilicke,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: 基于时间规则的可解释时态知识图预测方法，在9个数据集上达到或超过现有方法的性能，且具有完全可解释性


<details>
  <summary>Details</summary>
Motivation: 受到基于重复事实的强基线方法的启发，需要一种既具有高性能又完全可解释的时态知识图预测方法

Method: 学习四种简单类型的时间规则，使用考虑到新近性和频率的置信度函数

Result: 在9个数据集上评估，该方法的性能匹配或超过八个最新模型和两个基线模型

Conclusion: 该方法为时态知识图预测提供了一种高性能且完全可解释的解决方案

Abstract: We address the task of temporal knowledge graph (TKG) forecasting by
introducing a fully explainable method based on temporal rules. Motivated by
recent work proposing a strong baseline using recurrent facts, our approach
learns four simple types of rules with a confidence function that considers
both recency and frequency. Evaluated on nine datasets, our method matches or
surpasses the performance of eight state-of-the-art models and two baselines,
while providing fully interpretable predictions.

</details>


### [106] [Balancing Utility and Privacy: Dynamically Private SGD with Random Projection](https://arxiv.org/abs/2509.09485)
*Zhanhong Jiang,Md Zahid Hasan,Nastaran Saadati,Aditya Balu,Chao Liu,Soumik Sarkar*

Main category: cs.LG

TL;DR: 提出D2P2-SGD优化器，结合动态差分隐私和随机投影技术，在保护隐私的同时提高模型性能


<details>
  <summary>Details</summary>
Motivation: 现有DPSGD的静态噪声机制影响模型性能，且随着模型参数指数增长，随机优化器的效率面临挑战，需要解决隐私保护与模型效用之间的平衡问题

Method: 结合动态差分隐私（自动梯度裁剪）和随机投影技术，通过SGD实现隐私与效用的动态权衡调整

Result: 在不同目标函数上表现出可证明的次线性收敛率，匹配最佳可用速率，实验显示在保持隐私的同时显著提高准确性

Conclusion: 动态差分隐私以隐私为代价带来更好的效用，随机投影实现更高效的模型学习，D2P2-SGD在隐私保护和模型性能方面取得良好平衡

Abstract: Stochastic optimization is a pivotal enabler in modern machine learning,
producing effective models for various tasks. However, several existing works
have shown that model parameters and gradient information are susceptible to
privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy
concerns, its static noise mechanism impacts the error bounds for model
performance. Additionally, with the exponential increase in model parameters,
efficient learning of these models using stochastic optimizers has become more
challenging. To address these concerns, we introduce the Dynamically
Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we
combine two important ideas: (i) dynamic differential privacy (DDP) with
automatic gradient clipping and (ii) random projection with SGD, allowing
dynamic adjustment of the tradeoff between utility and privacy of the model. It
exhibits provably sub-linear convergence rates across different objective
functions, matching the best available rate. The theoretical analysis further
suggests that DDP leads to better utility at the cost of privacy, while random
projection enables more efficient model learning. Extensive experiments across
diverse datasets show that D2P2-SGD remarkably enhances accuracy while
maintaining privacy. Our code is available here.

</details>


### [107] [ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms](https://arxiv.org/abs/2509.09679)
*Bingxin Xu,Zhen Dong,Oussama Elachqar,Yuzhang Shang*

Main category: cs.LG

TL;DR: ButterflyQuant是一种新的2位量化方法，使用可学习的蝴蝶变换替代固定Hadamard变换，通过连续Givens旋转角度参数化，实现层自适应旋转来抑制激活值异常值，显著提升极端量化性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要大量内存，极端2位量化因激活值异常值导致性能严重下降。现有旋转方法使用固定变换无法适应不同Transformer层的特定异常值模式，需要层自适应方法。

Method: 提出ButterflyQuant，用可学习的蝴蝶变换替代Hadamard矩阵，通过连续Givens旋转角度参数化保证正交性，同时引入均匀性正则化促进量化友好分布。仅需128个校准样本和单GPU几分钟训练。

Result: 在LLaMA-2-7B模型上，2位量化达到15.4困惑度，相比QuaRot的22.1有显著提升。

Conclusion: ButterflyQuant通过可学习的层自适应旋转变换，有效解决极端量化中的异常值问题，以最小计算成本实现显著性能改进。

Abstract: Large language models require massive memory footprints, severely limiting
deployment on consumer hardware. Quantization reduces memory through lower
numerical precision, but extreme 2-bit quantization suffers from catastrophic
performance loss due to outliers in activations. Rotation-based methods such as
QuIP and QuaRot apply orthogonal transforms to eliminate outliers before
quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} =
(\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these
methods use fixed transforms--Hadamard matrices achieving optimal worst-case
coherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weight
distributions. We identify that different transformer layers exhibit distinct
outlier patterns, motivating layer-adaptive rotations rather than
one-size-fits-all approaches. We propose ButterflyQuant, which replaces
Hadamard rotations with learnable butterfly transforms parameterized by
continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$
entries that are non-differentiable and prohibit gradient-based learning,
butterfly transforms' continuous parameterization enables smooth optimization
while guaranteeing orthogonality by construction. This orthogonal constraint
ensures theoretical guarantees in outlier suppression while achieving $O(n \log
n)$ computational complexity with only $\frac{n \log n}{2}$ learnable
parameters. We further introduce a uniformity regularization on
post-transformation activations to promote smoother distributions amenable to
quantization. Learning requires only 128 calibration samples and converges in
minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit
quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.

</details>


### [108] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: PIPES是一个解决算法选择问题中计算成本高和OpenML数据局限性的实验集合，包含9,408个管道在300个数据集上的完整实验结果


<details>
  <summary>Details</summary>
Motivation: 解决算法选择问题中评估算法性能的高计算成本，以及OpenML等在线存储库中实验记录的局限性（缺乏管道多样性、预处理步骤代表性不足、样本不平衡）

Method: 创建PIPES实验集合，设计代表所有技术组合的多样化管道，在300个数据集上执行9,408个管道的实验，记录详细的管道块信息、训练测试时间、预测结果、性能指标和错误信息

Result: 建立了包含详细实验结果的综合集合，支持研究人员在多样化和代表性管道及数据集上进行元学习分析

Conclusion: PIPES克服了OpenML的局限性，提供了更完整和多样化的实验数据，支持元学习社区的进一步发展，并具有扩展潜力

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [109] [Cough Classification using Few-Shot Learning](https://arxiv.org/abs/2509.09515)
*Yoga Disha Sendhil Kumar,Manas V Shetty,Sudip Vhaduri*

Main category: cs.LG

TL;DR: 本文研究使用少样本学习进行呼吸音分类，特别是咳嗽声检测COVID-19、流感和健康状态。通过原型网络和声谱图表示，在有限标注数据下取得了与传统深度学习方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决医疗诊断中标注数据稀缺的问题，探索少样本学习在呼吸音分类中的应用潜力，特别是在COVID-19等呼吸道疾病检测方面。

Method: 使用原型网络（Prototypical Networks）结合咳嗽声的声谱图表示，进行多类别和二元分类比较，每个类别仅使用15个支持样本。

Result: 多类别分类达到74.87%准确率，二元分类在所有类别对上都超过70%准确率。统计测试显示多类别和二元模型性能无显著差异（t检验p=0.149，Wilcoxon p=0.125）。流感最容易区分，健康状态最具挑战性。

Conclusion: 少样本学习在医疗诊断中具有可行性，特别是在标注数据有限的情况下，多类别分类模型可以达到与二元分类相当的性能。

Abstract: This paper investigates the effectiveness of few-shot learning for
respiratory sound classification, focusing on coughbased detection of COVID-19,
Flu, and healthy conditions. We leverage Prototypical Networks with spectrogram
representations of cough sounds to address the challenge of limited labeled
data. Our study evaluates whether few-shot learning can enable models to
achieve performance comparable to traditional deep learning approaches while
using significantly fewer training samples. Additionally, we compare
multi-class and binary classification models to assess whether multi-class
models can perform comparably to their binary counterparts. Experimental
findings show that few-shot learning models can achieve competitive accuracy.
Our model attains 74.87% accuracy in multi-class classification with only 15
support examples per class, while binary classification achieves over 70%
accuracy across all class pairs. Class-wise analysis reveals Flu as the most
distinguishable class, and Healthy as the most challenging. Statistical tests
(paired t-test p = 0.149, Wilcoxon p = 0.125) indicate no significant
performance difference between binary and multiclass models, supporting the
viability of multi-class classification in this setting. These results
highlight the feasibility of applying few-shot learning in medical diagnostics,
particularly when large labeled datasets are unavailable.

</details>


### [110] [Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication](https://arxiv.org/abs/2509.09597)
*Maysam Behmanesh,Erkan Turan,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: 提出了一种新颖的图对齐框架，通过双通道编码器和几何感知功能映射模块，同时增强节点区分度并确保跨图潜在空间的几何一致性，在无监督图对齐任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有无监督图对齐方法存在两个关键问题：GNN嵌入中的过度平滑导致节点区分度降低，以及结构噪声、特征异质性和训练不稳定性导致的潜在空间错位，最终导致节点对应关系不可靠。

Method: 采用双通道编码器结合低通和高通谱滤波器生成既结构感知又高度区分的嵌入；引入几何感知功能映射模块学习图嵌入之间的双射等距变换，确保跨表示的一致几何关系。

Result: 在图基准测试中 consistently 优于现有无监督对齐基线，对结构不一致性和挑战性对齐场景表现出 superior 鲁棒性；在视觉-语言基准测试中能有效泛化，实现视觉和语言表示的无监督对齐。

Conclusion: 该框架通过同时增强节点区分度和强制几何一致性，有效解决了图对齐中的关键挑战，并在多个领域展现出优异的性能和泛化能力。

Abstract: Graph alignment-the problem of identifying corresponding nodes across
multiple graphs-is fundamental to numerous applications. Most existing
unsupervised methods embed node features into latent representations to enable
cross-graph comparison without ground-truth correspondences. However, these
methods suffer from two critical limitations: the degradation of node
distinctiveness due to oversmoothing in GNN-based embeddings, and the
misalignment of latent spaces across graphs caused by structural noise, feature
heterogeneity, and training instability, ultimately leading to unreliable node
correspondences. We propose a novel graph alignment framework that
simultaneously enhances node distinctiveness and enforces geometric consistency
across latent spaces. Our approach introduces a dual-pass encoder that combines
low-pass and high-pass spectral filters to generate embeddings that are both
structure-aware and highly discriminative. To address latent space
misalignment, we incorporate a geometry-aware functional map module that learns
bijective and isometric transformations between graph embeddings, ensuring
consistent geometric relationships across different representations. Extensive
experiments on graph benchmarks demonstrate that our method consistently
outperforms existing unsupervised alignment baselines, exhibiting superior
robustness to structural inconsistencies and challenging alignment scenarios.
Additionally, comprehensive evaluation on vision-language benchmarks using
diverse pretrained models shows that our framework effectively generalizes
beyond graph domains, enabling unsupervised alignment of vision and language
representations.

</details>


### [111] [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](https://arxiv.org/abs/2509.09599)
*Ira J. S. Shokar,Rich R. Kerswell,Peter H. Haynes*

Main category: cs.LG

TL;DR: 提出了一种针对随机和混沌时空系统的深度学习模拟器，能够根据PDE参数值进行条件化模拟，通过预训练和微调实现参数空间泛化，并提供计算加速和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传统数值积分方法计算成本高，难以高效探索参数空间。需要开发能够处理不同参数值、域大小和分辨率的计算高效模拟器，同时提供不确定性量化。

Method: 采用预训练+微调策略，在单一参数域预训练后，使用多样化小数据集微调实现参数泛化。引入局部注意力机制处理可变域大小和分辨率，支持概率变体进行不确定性量化。

Result: 在混沌Kuramoto-Sivashinsky方程和随机强迫beta-plane湍流上验证，模型能够捕捉插值参数值的现象，相比传统数值积分提供显著计算加速。

Conclusion: 该方法成功实现了参数条件化的时空系统模拟，提供计算高效的参数空间探索能力，概率变体支持罕见事件的统计研究，为复杂系统建模提供了有效工具。

Abstract: We present a deep learning emulator for stochastic and chaotic
spatio-temporal systems, explicitly conditioned on the parameter values of the
underlying partial differential equations (PDEs). Our approach involves
pre-training the model on a single parameter domain, followed by fine-tuning on
a smaller, yet diverse dataset, enabling generalisation across a broad range of
parameter values. By incorporating local attention mechanisms, the network is
capable of handling varying domain sizes and resolutions. This enables
computationally efficient pre-training on smaller domains while requiring only
a small additional dataset to learn how to generalise to larger domain sizes.
We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky
equation and stochastically-forced beta-plane turbulence, showcasing its
ability to capture phenomena at interpolated parameter values. The emulator
provides significant computational speed-ups over conventional numerical
integration, facilitating efficient exploration of parameter space, while a
probabilistic variant of the emulator provides uncertainty quantification,
allowing for the statistical study of rare events.

</details>


### [112] [ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance](https://arxiv.org/abs/2509.09611)
*Haolan Zheng,Yanlai Chen,Jiequn Han,Yue Yu*

Main category: cs.LG

TL;DR: ReBaNO是一种新颖的数据稀疏算子学习算法，通过贪婪算法构建网络结构，实现离散化不变性并显著缩小泛化差距


<details>
  <summary>Details</summary>
Motivation: 解决多输入PDE组的算子学习问题，旨在消除泛化差距并实现严格的离散化不变性

Method: 结合降基方法和生成预训练物理信息神经网络，使用贪婪算法离线自适应构建网络结构，通过任务特定激活函数进行知识蒸馏

Result: 在内外分布测试中显著优于PCA-Net、DeepONet、FNO和CNO等先进算法，是唯一实现严格离散化不变性的算子学习算法

Conclusion: ReBaNO通过数学严谨的贪婪算法和紧凑架构，在计算成本最小化的同时实现了优异的性能和离散化不变性

Abstract: We propose a novel data-lean operator learning algorithm, the Reduced Basis
Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct
inputs. Inspired by the Reduced Basis Method and the recently introduced
Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a
mathematically rigorous greedy algorithm to build its network structure offline
adaptively from the ground up. Knowledge distillation via task-specific
activation function allows ReBaNO to have a compact architecture requiring
minimal computational cost online while embedding physics. In comparison to
state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,
and CNO, numerical results demonstrate that ReBaNO significantly outperforms
them in terms of eliminating/shrinking the generalization gap for both in- and
out-of-distribution tests and being the only operator learning algorithm
achieving strict discretization invariance.

</details>


### [113] [Explaining Concept Drift through the Evolution of Group Counterfactuals](https://arxiv.org/abs/2509.09616)
*Ignacy Stępka,Jerzy Stefanowski*

Main category: cs.LG

TL;DR: 通过分析组基反事实解释的时间演化来解释概念漏流，揭示模型决策逻辑的变化


<details>
  <summary>Details</summary>
Motivation: 动态环境中的机器学习模型遇到概念漏流问题，当前方法能够检测漏流但无法解释模型决策逻辑如何和为什么发生变化

Method: 提出三层框架：数据层（分布移动）、模型层（预测分歧）和解释层（跟踪组基反事实解释的聚类质心和反事实行动向量的演化）

Result: 该方法能够揭示模型决策边界的结构性变化，并区分不同的漏流根源（如空间数据移动vs概念重新标签）

Conclusion: 通过结合数据、模型和解释层的全局视图，实现了更全面的概念漏流诊断，为模型决策逻辑的变化提供了可解释的代理

Abstract: Machine learning models in dynamic environments often suffer from concept
drift, where changes in the data distribution degrade performance. While
detecting this drift is a well-studied topic, explaining how and why the
model's decision-making logic changes still remains a significant challenge. In
this paper, we introduce a novel methodology to explain concept drift by
analyzing the temporal evolution of group-based counterfactual explanations
(GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their
associated counterfactual action vectors before and after a drift. These
evolving GCEs act as an interpretable proxy, revealing structural changes in
the model's decision boundary and its underlying rationale. We operationalize
this analysis within a three-layer framework that synergistically combines
insights from the data layer (distributional shifts), the model layer
(prediction disagreement), and our proposed explanation layer. We show that
such holistic view allows for a more comprehensive diagnosis of drift, making
it possible to distinguish between different root causes, such as a spatial
data shift versus a re-labeling of concepts.

</details>


### [114] [Functional Groups are All you Need for Chemically Interpretable Molecular Property Prediction](https://arxiv.org/abs/2509.09619)
*Roshan Balaji,Joe Bobby,Nirav Pravinbhai Bhatt*

Main category: cs.LG

TL;DR: 提出基于功能基团的分子表示框架FGR，结合传统化学功能基团和从大数据挖掘的新功能基团，实现高性能且可解释的分子性质预测


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分子性质预测模型缺乏可解释性，阻碍化学家采用。需要开发既保持高性能又具有化学可解释性的分子表示方法

Method: FGR框架整合两种功能基团：传统化学知识定义的功能基团(FG)和通过序列模式挖掘从大分子语料库中发现的新功能基团(MFG)。利用无标签分子数据进行预训练，将分子编码到低维潜在空间，并可整合2D结构描述符

Result: 在33个基准数据集上达到state-of-the-art性能，涵盖物理化学、生物物理、量子力学、生物活性和药代动力学等多个领域，同时保持化学可解释性

Conclusion: FGR框架是开发高性能、化学可解释深度学习模型的重要进展，使化学家能够直接将预测性质与特定功能基团关联，促进对结构-性质关系的新见解

Abstract: Molecular property prediction using deep learning (DL) models has accelerated
drug and materials discovery, but the resulting DL models often lack
interpretability, hindering their adoption by chemists. This work proposes
developing molecule representations using the concept of Functional Groups (FG)
in chemistry. We introduce the Functional Group Representation (FGR) framework,
a novel approach to encoding molecules based on their fundamental chemical
substructures. Our method integrates two types of functional groups: those
curated from established chemical knowledge (FG), and those mined from a large
molecular corpus using sequential pattern mining (MFG). The resulting FGR
framework encodes molecules into a lower-dimensional latent space by leveraging
pre-training on a large dataset of unlabeled molecules. Furthermore, the
proposed framework allows the inclusion of 2D structure-based descriptors of
molecules. We demonstrate that the FGR framework achieves state-of-the-art
performance on a diverse range of 33 benchmark datasets spanning physical
chemistry, biophysics, quantum mechanics, biological activity, and
pharmacokinetics while enabling chemical interpretability. Crucially, the
model's representations are intrinsically aligned with established chemical
principles, allowing chemists to directly link predicted properties to specific
functional groups and facilitating novel insights into structure-property
relationships. Our work presents a significant step toward developing
high-performing, chemically interpretable DL models for molecular discovery.

</details>


### [115] [Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management](https://arxiv.org/abs/2509.09655)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: FG-FARL是一种离线强化学习方法，通过校准各群体安全阈值来减少伤害并在受保护子群体间均衡公平目标（覆盖率或伤害）


<details>
  <summary>Details</summary>
Motivation: 为了解决离线强化学习中存在的安全性和公平性问题，特别是在医疗保健等敏感领域需要减少伤害并确保决策支持的公平性

Method: 使用可行性引导的公平自适应强化学习（FG-FARL），校准每个群体的安全阈值，与行为克隆（BC）和HACO（混合自适应符合离线RL）基线进行比较

Result: FG-FARL在保持与基线相当的价值的同时改善了公平性指标，在医疗补助人群健康管理数据上验证了有效性

Conclusion: FG-FARL提供了一条实现更安全和更公平决策支持的实用路径，特别适用于医疗保健等需要平衡安全与公平的领域

Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [116] [TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes](https://arxiv.org/abs/2509.09525)
*Jialiang Huang,Teng Ma,Zheng Liu,Sixing Lin,Kang Chen,Jinlei Jiang,Xia Liao,Yingdi Shan,Yongwei Wu,Ning Zhang,Mengting Lu,Tao Ma,Haifeng Gong,Mingxing Zhang*

Main category: cs.DC

TL;DR: TrEnv是一个专为LLM代理优化的无服务器平台，通过可重用沙箱和内存模板等技术，显著降低了启动延迟和内存使用，在容器和VM环境中都表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有无服务器计算基础设施对LLM代理等新兴工作负载存在瓶颈，运行成本可达LLM API调用成本的70%，需要更高效的高密度无服务器平台。

Method: 设计了支持容器和VM环境的无服务器平台TrEnv，采用可重用沙箱、内存模板、浏览器共享和页面缓存绕过机制来优化执行环境。

Result: 在容器环境中P99延迟降低7倍，内存使用减少48%；在VM环境中P99延迟降低58%，内存节省61%，优于E2B等先进系统。

Conclusion: TrEnv通过协同设计有效解决了LLM代理在无服务器平台上的性能瓶颈问题，为高密度工作负载提供了高效解决方案。

Abstract: Serverless computing provides dynamic scalability, but its infrastructure
overhead becomes a bottleneck for emerging workloads such as LLM agents, which
exhibit unpredictable invocation patterns and variable resource demands. Our
analysis shows that for these agents, the cost of running on serverless
platforms can reach up to 70% of the cost of LLM API calls. This finding
motivates the need for a more efficient, high-density serverless platform. We
present TrEnv, a co-designed serverless platform that supports both container-
and VM-based environments, optimized for the unique demands of LLM agents.
TrEnv reduces startup latency and memory usage through repurposable sandboxes
and memory templates, which enable fast reuse and restoration of execution
environments. To further reduce overhead in VM-based agent workloads, TrEnv
leverages browser sharing and a page cache bypassing mechanism. Evaluations
show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in
container-based settings, and achieves up to 58% lower P99 latency and 61%
memory savings for VM-based agents compared to state-of-the-art systems like
E2B.

</details>


### [117] [A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems](https://arxiv.org/abs/2509.08969)
*Nima Karimian Kakolaki*

Main category: cs.DC

TL;DR: 这篇论文对比分析了分布式系统中的四种标识符方案（自增键、UUIDv4、UUIDv7、ULID），发现ULID在网络开销、生成速度和冲突风险方面都显著优于其他方案，是高性能分布式系统的最佳选择


<details>
  <summary>Details</summary>
Motivation: 分析分布式系统中各种标识符方案的性能差异，为高性能分布式系统选择最优的标识符生成方案

Method: 结合数学概率计算和实验室模拟分布式环境进行实验，测量生成速度、网络传输开销和冲突概率

Result: ULID网络开销减少83.7%，生成速度提高97.32%，冲突风险比UUIDv7低98.42%，在高生成速率下仍保持可忽略的冲突概率

Conclusion: ULID是高性能分布式系统的最佳选择，具有高效、时间序列化和字典排序特性，适合可扩展应用

Abstract: Distributed systems require robust, scalable identifier schemes to ensure
data uniqueness and efficient indexing across multiple nodes. This paper
presents a comprehensive analysis of the evolution of distributed identifiers,
comparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We
combine mathematical calculation of collision probabilities with empirical
experiments measuring generation speed and network transmission overhead in a
simulated distributed environment. Results demonstrate that ULIDs significantly
outperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing
generation speed by 97.32%. statistical analysis further shows ULIDs offer a
98.42% lower collision risk compared to UUIDv7, while maintaining negligible
collision probabilities even at high generation rates. These findings highlight
ULIDs as an optimal choice for high-performance distributed systems, providing
efficient, time-ordered, and lexicographically sortable identifiers suitable
for scalable applications. All source code, datasets, and analysis scripts
utilized in this research are publicly available in our dedicated repository at
https://github.com/nimakarimiank/uids-comparison. This repository contains
comprehensive documentation of the experimental setup, including configuration
files for the distributed environment, producer and consumer implementations,
and message broker integration. Additionally, it provides the data scripts and
datasets. Researchers and practitioners are encouraged to explore the
repository for full reproducibility of the experiments and to facilitate
further investigation or extension of the presented work.

</details>


### [118] [Optimizing the Variant Calling Pipeline Execution on Human Genomes Using GPU-Enabled Machines](https://arxiv.org/abs/2509.09058)
*Ajay Kumar,Praveen Rao,Peter Sanders*

Main category: cs.DC

TL;DR: 基于机器学习的GPU变异检测管道优化方法，通过预测执行时间和优化调度，在GPU集群上实现2倍加速


<details>
  <summary>Details</summary>
Motivation: 由于变异检测计算密集且基因组数据常在云端处理，需要优化GPU机器上的变异检测管道执行效率以最小化总执行时间

Method: 使用机器学习预测变异检测管道各阶段执行时间，基于基因组特征；采用柔性作业车间调度问题启发的方法生成最优执行计划，并通过机器间仔细同步执行

Result: 在公开基因组序列测试中，ML方法能有效预测执行时间；相比使用ML预测的贪心方法平均加速2倍；相比基于资源可用性的动态方法平均加速1.6倍

Conclusion: 机器学习方法能有效优化基因组变异检测管道在GPU环境中的执行效率，显著减少总执行时间

Abstract: Variant calling is the first step in analyzing a human genome and aims to
detect variants in an individual's genome compared to a reference genome. Due
to the computationally-intensive nature of variant calling, genomic data are
increasingly processed in cloud environments as large amounts of compute and
storage resources can be acquired with the pay-as-you-go pricing model. In this
paper, we address the problem of efficiently executing a variant calling
pipeline for a workload of human genomes on graphics processing unit
(GPU)-enabled machines. We propose a novel machine learning (ML)-based approach
for optimizing the workload execution to minimize the total execution time. Our
approach encompasses two key techniques: The first technique employs ML to
predict the execution times of different stages in a variant calling pipeline
based on the characteristics of a genome sequence. Using the predicted times,
the second technique generates optimal execution plans for the machines by
drawing inspiration from the flexible job shop scheduling problem. The plans
are executed via careful synchronization across different machines. We
evaluated our approach on a workload of publicly available genome sequences
using a testbed with different types of GPU hardware. We observed that our
approach was effective in predicting the execution times of variant calling
pipeline stages using ML on features such as sequence size, read quality,
percentage of duplicate reads, and average read length. In addition, our
approach achieved 2X speedup (on an average) over a greedy approach that also
used ML for predicting the execution times on the tested workload of sequences.
Finally, our approach achieved 1.6X speedup (on an average) over a dynamic
approach that executed the workload based on availability of resources without
using any ML-based time predictions.

</details>


### [119] [Coherence-Aware Task Graph Modeling for Realistic Application](https://arxiv.org/abs/2509.09094)
*Guochu Xiong,Xiangzhong Luo,Weichen Liu*

Main category: cs.DC

TL;DR: CoTAM是一个缓存一致性感知的任务图建模框架，通过解耦一致性影响、量化其权重并推断任务间依赖关系，为真实工作负载构建统一的任务图，弥补了动态行为与静态设计之间的差距。


<details>
  <summary>Details</summary>
Motivation: 多核系统中缓存一致性对性能至关重要，但现有任务图建模方法要么依赖预定义图，要么忽略一致性交互，导致设计假设与实际运行时行为存在差距。

Method: CoTAM框架通过分析一致性影响、解耦其效应、使用学习权重方案量化影响，并推断任务间依赖关系来生成一致性感知的任务图。

Result: 大量实验表明CoTAM优于隐式方法，成功弥合了动态工作负载行为与现有设计之间的差距。

Conclusion: 将缓存一致性纳入任务图建模对于准确和可推广的系统级分析至关重要，CoTAM框架为此提供了有效解决方案。

Abstract: As multicore systems continue to scale, cache coherence has emerged as a
critical determinant of system performance, with coherence behavior and task
execution closely intertwined, reshaping inter-task dependencies. Task graph
modeling provides a structured way to capture such dependencies and serves as
the foundation for many system-level design strategies. However, these
strategies typically rely on predefined task graphs, while many real-world
applications lack explicit graphs and exhibit dynamic, data-dependent behavior,
limiting the effectiveness of static approaches. To address this, several task
graph modeling methods for realistic workloads have been developed. Yet, they
either rely on implicit techniques that use application-specific features
without producing explicit graphs, or they generate graphs tailored to fixed
scheduling models, which limits generality. More importantly, they often
overlook coherence interactions, creating a gap between design assumptions and
actual runtime behavior. To overcome these limitations, we propose CoTAM, a
Coherence-Aware Task Graph Modeling framework for realistic workloads that
constructs a unified task graph reflecting runtime behavior. CoTAM analyzes the
impact of coherence by decoupling its effects from overall execution,
quantifies its influence through a learned weighting scheme, and infers
inter-task dependencies for coherence-aware graph generation. Extensive
experiments show that CoTAM outperforms implicit methods, bridging the gap
between dynamic workload behavior and existing designs while demonstrating the
importance of incorporating cache coherence into task graph modeling for
accurate and generalizable system-level analysis.

</details>


### [120] [WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge](https://arxiv.org/abs/2509.09400)
*Valerio Besozzi,Enrico Fiasco,Marco Danelutto,Patrizio Dazzi*

Main category: cs.DC

TL;DR: 这篇论文比较了WebAssembly和unikernel基于MicroVM在边缘计算中的性能。WebAssembly在轻量级函数上冷启动更快，但处理复杂工作负载时性能差；Firecracker冷启动更慢但稳定，在I/O密集型任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 边缘计算需要轻量级执行环境以减少冷启动延迟，特别是在紧急边缘计算(UEC)场景下。需要比较不同技术方案的性能特点。

Method: 开发了Limes（基于Wasmtime的WebAssembly运行时），并将其与基于Firecracker的SPARE环境进行性能对比分析。测试了各种服务器无函数工作负载。

Result: WebAssembly在轻量级函数上冷启动时间更短，但在复杂工作负载下性能受限。Firecracker冷启动时间较长但稳定，执行性能更好，特别是在I/O密集型任务中表现优异。

Conclusion: 两种技术各有优势：WebAssembly适合对冷启时间敏感的轻量级应用，而Firecracker更适合需要高性能执行的复杂工作负载。根据具体应用场景选择适合的技术方案。

Abstract: Serverless computing at the edge requires lightweight execution environments
to minimize cold start latency, especially in Urgent Edge Computing (UEC). This
paper compares WebAssembly and unikernel-based MicroVMs for serverless
workloads. We present Limes, a WebAssembly runtime built on Wasmtime, and
evaluate it against the Firecracker-based environment used in SPARE. Results
show that WebAssembly offers lower cold start times for lightweight functions
but suffers with complex workloads, while Firecracker provides higher, but
stable, cold starts and better execution performance, particularly for
I/O-heavy tasks.

</details>


### [121] [Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing](https://arxiv.org/abs/2509.09435)
*Houming Qiu,Kun Zhu,Dusit Niyato,Nguyen Cong Luong,Changyan Yi,Chen Dai*

Main category: cs.DC

TL;DR: 基于重心有理插值的近似编码分布式计算方案，免需固定恢复阈值、支持任意返回结果数量、避免极点问题，提高了计算精度和数值稳定性


<details>
  <summary>Details</summary>
Motivation: 现有编码分布式计算方案存在两大限制：必须收到固定数量的结果才能解码，以及编码/解码函数存在极点导致数值不稳定和计算错误

Method: 采用重心有理插值技术构建近似编码分布式计算方案，包含新的BRI基于梯度编码算法

Result: 实验结果显示，该方案在等待时间和近似精度方面都优于现有CDC方案

Conclusion: 该方案免需固定恢复阈值、支持任意返回结果数量、避免极点问题，能够在保持数值稳定的同时提高计算精度，为协作移动边缘计算系统提供了更灵活和稳健的解决方案

Abstract: Collaborative mobile edge computing (MEC) has emerged as a promising paradigm
to enable low-capability edge nodes to cooperatively execute
computation-intensive tasks. However, straggling edge nodes (stragglers)
significantly degrade the performance of MEC systems by prolonging computation
latency. While coded distributed computing (CDC) as an effective technique is
widely adopted to mitigate straggler effects, existing CDC schemes exhibit two
critical limitations: (i) They cannot successfully decode the final result
unless the number of received results reaches a fixed recovery threshold, which
seriously restricts their flexibility; (ii) They suffer from inherent poles in
their encoding/decoding functions, leading to decoding inaccuracies and
numerical instability in the computational results. To address these
limitations, this paper proposes an approximated CDC scheme based on
barycentric rational interpolation. The proposed CDC scheme offers several
outstanding advantages. Firstly, it can decode the final result leveraging any
returned results from workers. Secondly, it supports computations over both
finite and real fields while ensuring numerical stability. Thirdly, its
encoding/decoding functions are free of poles, which not only enhances
approximation accuracy but also achieves flexible accuracy tuning. Fourthly, it
integrates a novel BRI-based gradient coding algorithm accelerating the
training process while providing robustness against stragglers. Finally,
experimental results reveal that the proposed scheme is superior to existing
CDC schemes in both waiting time and approximate accuracy.

</details>


### [122] [Weaker Assumptions for Asymmetric Trust](https://arxiv.org/abs/2509.09493)
*Ignacio Amores-Sesar,Christian Cachin,Juan Villacis*

Main category: cs.DC

TL;DR: 这篇论文研究不对称信任分布式系统中的基础问题，提出了更弱假设的可靠广播和共识算法，充分利用了不对称信任的优势。


<details>
  <summary>Details</summary>
Motivation: 传统的对称信任模型在不对称信任分布式系统中存在限制，现有方案通过强假设过度限制了不对称信任的优势。

Method: 提出了一种新的方法来定义不对称问题，并基于此设计了可靠广播和共识算法，这些算法的假设条件比之前的方案更弱。

Result: 新算法在保持系统可用性和一致性的同时，充分利用了不对称信任的灵活性，提高了系统的效能和实用性。

Conclusion: 该研究为不对称信任分布式系统提供了更为基础和高效的解决方案，并可扩展到其他核心问题的解决。

Abstract: In distributed systems with asymmetric trust, each participant is free to
make its own trust assumptions about others, captured by an asymmetric quorum
system. This contrasts with ordinary, symmetric quorum systems and threshold
models, where trust assumptions are uniformly shared among participants.
Fundamental problems like reliable broadcast and consensus are unsolvable in
the asymmetric model if quorum systems satisfy only the classical properties of
consistency and availability. Existing approaches overcome this by introducing
stronger assumptions. We show that some of these assumptions are overly
restrictive, so much so that they effectively eliminate the benefits of
asymmetric trust. To address this, we propose a new approach to characterize
asymmetric problems and, building upon it, present algorithms for reliable
broadcast and consensus that require weaker assumptions than previous
solutions. Our methods are general and can be extended to other core problems
in systems with asymmetric trust.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [123] [μFork: Supporting POSIX fork Within a Single-Address-Space OS](https://arxiv.org/abs/2509.09439)
*John Alistair Kressel,Hugo Lefeuvre,Pierre Olivier*

Main category: cs.OS

TL;DR: μFork是一种单地址空间操作系统设计，通过CHERI技术在单地址空间内模拟POSIX进程，实现了轻量化的fork操作，在保持轻量性、兼容性和隔离性的同时支持POSIX fork。


<details>
  <summary>Details</summary>
Motivation: 单地址空间操作系统具有轻量化优势，但因为共享地址空间而不兼容多进程POSIX应用。之前的解决方案需要在轻量性、兼容性和隔离性之间做出折衷。

Method: 使用CHERI技术在单地址空间内创建子进程的内存副本，通过重定向绝对内存引用（指针）来解决地址重定位问题，同时提供用户/内核隔离和进程隔离。

Result: 在Redis快照、Nginx多工作者部署和Zygote FaaS工作者热身等真实场景中评估，μFork在关键轻量性指标上比之前的方案和传统单体操作系统快一个数量级：FaaS函数吞吐量高出24%，μ进程fork速度达54μs，比传统fork快3.7倍。

Conclusion: μFork证明了在单地址空间操作系统中可以在不折衷轻量性、兼容性和隔离性的前提下实现POSIX fork，为轻量化操作系统提供了一种新的解决方案。

Abstract: Single-address-space operating systems have well-known lightweightness
benefits that result from their central design idea: the kernel and
applications share a unique address space. This model makes these operating
systems (OSes) incompatible by design with a large class of software:
multiprocess POSIX applications. Indeed, the semantics of the primitive used to
create POSIX processes, fork, are inextricably tied to the existence of
multiple address spaces.
  Prior approaches addressing this issue trade off lightweightness,
compatibility and/or isolation. We propose {\mu}Fork, a single-address-space
operating system design supporting POSIX fork on modern hardware without
compromising on any of these key objectives. {\mu}Fork emulates POSIX processes
({\mu}processes) and achieves fork by creating for the child a copy of the
parent {\mu}process' memory at a different location within a single address
space. This approach presents two challenges: relocating the child's absolute
memory references (pointers), as well as providing user/kernel and
{\mu}processes isolation without impacting lightweightness. We address them
using CHERI. We implement {\mu}Fork and evaluate it upon three real-world
use-cases: Redis snapshots, Nginx multi-worker deployments, and Zygote FaaS
worker warm-up. {\mu}Fork outperforms previous work and traditional monolithic
OSes on key lightweightness metrics by an order of magnitude, e.g. it can offer
a fork-bound FaaS function throughput 24% higher than that of a monolithic OS,
and can fork a {\mu}process in 54{\mu}s, 3.7x faster than a traditional fork.

</details>
