<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 69]
- [cs.LG](#cs.LG) [Total: 107]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.AI](#cs.AI) [Total: 48]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training](https://arxiv.org/abs/2509.05359)
*Yanis Labrak,Richard Dufour,Mickaël Rouvier*

Main category: cs.CL

TL;DR: 本文研究了语音语言模型中离散单元表示，重点探讨了在持续预训练阶段如何优化语音建模，包括模型架构、数据表示和训练鲁棒性对语音模态适应的影响。


<details>
  <summary>Details</summary>
Motivation: 研究如何将现有的预训练语言模型有效适应到语音模态，探索离散单元表示在语音语言模型中的优化策略，特别是在持续预训练阶段的关键因素。

Method: 系统性地研究了模型架构、数据表示和训练鲁棒性对预训练阶段的影响，通过实验分析语音编码器和聚类粒度在不同模型规模下的作用，并考察聚类分布和音素对齐来研究离散词汇表的有效使用。

Result: 发现最优离散化策略随模型容量而变化，揭示了离散词汇表中存在语言学和副语言学模式，同时证明了聚类数据选择对模型鲁棒性的重要性，特别是离散化训练与目标应用领域匹配的关键性。

Conclusion: 离散单元表示在语音语言模型中具有重要作用，最优策略需要根据模型容量进行调整，领域匹配是确保模型鲁棒性的关键因素，这为语音模态的持续预训练提供了重要指导。

Abstract: This paper investigates discrete unit representations in Speech Language
Models (SLMs), focusing on optimizing speech modeling during continual
pre-training. In this paper, we systematically examine how model architecture,
data representation, and training robustness influence the pre-training stage
in which we adapt existing pre-trained language models to the speech modality.
Our experiments highlight the role of speech encoders and clustering
granularity across different model scales, showing how optimal discretization
strategies vary with model capacity. By examining cluster distribution and
phonemic alignments, we investigate the effective use of discrete vocabulary,
uncovering both linguistic and paralinguistic patterns. Additionally, we
explore the impact of clustering data selection on model robustness,
highlighting the importance of domain matching between discretization training
and target applications.

</details>


### [2] [Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection](https://arxiv.org/abs/2509.05360)
*Jerry Li,Evangelos Papalexakis*

Main category: cs.CL

TL;DR: 基于N-Gram频率矩阵构建和矩阵分解的新题检测方法，在HaluEval数据集上显著提升了幻觉检测性能，超越传统基准方法并与最先进的LLM判断方法竞争


<details>
  <summary>Details</summary>
Motivation: 大语言模型幻觉问题严重影响可靠性，现有检测方法如ROUGE、BERTScore等缺乏语义深度，需要更有效的幻觉检测方法

Method: 提出构建N-Gram频率矩阵来捐写丰富的语义结构，通过矩阵分解提取奇异值特征，使用MLP二分类器进行幻觉检测

Result: 在HaluEval数据集上表现优异，显著超越传统基准方法，与最先进的LLM判断方法相比也具有竞争力

Conclusion: 该方法通过捐写语义结构的N-Gram频率矩阵，提供了一种有效的幻觉检测新途径，在保持计算效率的同时提升了检测准确性

Abstract: Large Language Models (LLMs) have demonstrated effectiveness across a wide
variety of tasks involving natural language, however, a fundamental problem of
hallucinations still plagues these models, limiting their trustworthiness in
generating consistent, truthful information. Detecting hallucinations has
quickly become an important topic, with various methods such as uncertainty
estimation, LLM Judges, retrieval augmented generation (RAG), and consistency
checks showing promise. Many of these methods build upon foundational metrics,
such as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth
necessary to detect hallucinations effectively. In this work, we propose a
novel approach inspired by ROUGE that constructs an N-Gram frequency tensor
from LLM-generated text. This tensor captures richer semantic structure by
encoding co-occurrence patterns, enabling better differentiation between
factual and hallucinated content. We demonstrate this by applying tensor
decomposition methods to extract singular values from each mode and use these
as input features to train a multi-layer perceptron (MLP) binary classifier for
hallucinations. Our method is evaluated on the HaluEval dataset and
demonstrates significant improvements over traditional baselines, as well as
competitive performance against state-of-the-art LLM judges.

</details>


### [3] [A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs](https://arxiv.org/abs/2509.05385)
*Jiacheng Wei,Faguo Wu,Xiao Zhang*

Main category: cs.CL

TL;DR: SAGE是一个触发引导的动态微调框架，能够在推理时实现自适应更新，通过分解复杂推理任务为原子子任务来解决大语言模型无法在推理时持续学习的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理时无法持续适应和学习新数据，限制了其在复杂推理任务中的表现。

Method: SAGE包含三个核心组件：触发模块实时检测推理失败、触发缓冲模块通过流式聚类处理异常样本、Lora存储模块动态优化参数更新。

Result: 评估结果显示SAGE在原子推理子任务上通过动态知识更新表现出优秀的准确性、鲁棒性和稳定性。

Conclusion: SAGE框架有效解决了大语言模型在推理时无法持续学习的问题，为复杂推理任务提供了实用的动态适应解决方案。

Abstract: Large language models are unable to continuously adapt and learn from new
data during reasoning at inference time. To address this limitation, we propose
that complex reasoning tasks be decomposed into atomic subtasks and introduce
SAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive
updates during reasoning at inference time. SAGE consists of three key
components: (1) a Trigger module that detects reasoning failures through
multiple evaluation metrics in real time; (2) a Trigger Buffer module that
clusters anomaly samples using a streaming clustering process with HDBSCAN,
followed by stability checks and similarity-based merging; and (3) a Lora Store
module that dynamically optimizes parameter updates with an adapter pool for
knowledge retention. Evaluation results show that SAGE demonstrates excellent
accuracy, robustness, and stability on the atomic reasoning subtask through
dynamic knowledge updating during test time.

</details>


### [4] [Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate](https://arxiv.org/abs/2509.05396)
*Andrea Wynn,Harsh Satija,Gillian Hadfield*

Main category: cs.CL

TL;DR: 多机器人辩论在模型能力异质的情况下可能导致性能退化，即使强模型占多数也会因过度调整而改变正确答案


<details>
  <summary>Details</summary>
Motivation: 探索模型能力异质性对多机器人辩论动态和结果的影响，识别辩论模式的潜在失败模式

Method: 通过一系列实验分析模型在辩论过程中的行为变化，观察模型如何困调整而改变正确答案

Result: 辩论导致准确性随时间下降，模型经常从正确答案转向错误答案，偏向过度调整而非挑战错误推理

Conclusion: 天真地应用辩论模式可能造成性能退化，需要制定激励机制和提供适当设备来抵御说服力但错误的推理

Abstract: While multi-agent debate has been proposed as a promising strategy for
improving AI reasoning ability, we find that debate can sometimes be harmful
rather than helpful. The prior work has exclusively focused on debates within
homogeneous groups of agents, whereas we explore how diversity in model
capabilities influences the dynamics and outcomes of multi-agent interactions.
Through a series of experiments, we demonstrate that debate can lead to a
decrease in accuracy over time -- even in settings where stronger (i.e., more
capable) models outnumber their weaker counterparts. Our analysis reveals that
models frequently shift from correct to incorrect answers in response to peer
reasoning, favoring agreement over challenging flawed reasoning. These results
highlight important failure modes in the exchange of reasons during multi-agent
debate, suggesting that naive applications of debate may cause performance
degradation when agents are neither incentivized nor adequately equipped to
resist persuasive but incorrect reasoning.

</details>


### [5] [No Translation Needed: Forecasting Quality from Fertility and Metadata](https://arxiv.org/abs/2509.05425)
*Jessica M. Lundin,Ada Zhang,David Adelani,Cody Carroll*

Main category: cs.CL

TL;DR: 使用语言类型学和词歇率等简单特征，可以在不运行翻译系统的情况下准确预测GPT-4o的翻译质量，在FLORES-200语言库中达到高准确度


<details>
  <summary>Details</summary>
Motivation: 探索如何在不实际运行翻译系统的情况下，仅通过基础语言特征预测翻译质量，以提高多语言评估的效率

Method: 使用词歇率比例、词段计数和语言类型学元数据（语言语系、文字系统、地区）等少量特征，通过梯度提升树模型进行预测

Result: 在FLORES-200的203种语言中，模型达到良好性能（XX→英语R²=0.66，英语→XX R²=0.72），语言类型学因素对英语翻译预测重要，词歇率对多样目标语翻译更关键

Conclusion: 翻译质量受到词段层面的词歇率和更广泛的语言类型学因素的共同影响，为多语言评估和质量估计提供了新视角

Abstract: We show that translation quality can be predicted with surprising accuracy
\textit{without ever running the translation system itself}. Using only a
handful of features, token fertility ratios, token counts, and basic linguistic
metadata (language family, script, and region), we can forecast ChrF scores for
GPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient
boosting models achieve favorable performance ($R^{2}=0.66$ for
XX$\rightarrow$English and $R^{2}=0.72$ for English$\rightarrow$XX). Feature
importance analyses reveal that typological factors dominate predictions into
English, while fertility plays a larger role for translations into diverse
target languages. These findings suggest that translation quality is shaped by
both token-level fertility and broader linguistic typology, offering new
insights for multilingual evaluation and quality estimation.

</details>


### [6] [Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too](https://arxiv.org/abs/2509.05440)
*Logan Lawrence,Ashton Williamson,Alexander Shelton*

Main category: cs.CL

TL;DR: 这篇论文提出了一种直接评分方法，利用合成摘要来模拟人工排序，解决了传统成对比较方法无法赋予绝对分数的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型被广泛用于自动评价自由形式内容，但成对比较方法虽然在样本级性能上表现良好，却无法为单个摘要赋予绝对分数，而这一能力在需要阈值处理的应用场景中至关重要。

Method: 提出一种直接评分方法，通过使用合成摘要来在测试时模拟机器成对排序。该方法能够为单个摘要赋予绝对分数。

Result: 在SummEval、TopicalChat和HANNA三个元评估基准上，该方法在轴向平均样本级相关性方面与最先进的成对评估方法表现相似，分别为+0.03、-0.03和+0.05。

Conclusion: 该直接评分方法能够在保持与成对比较方法相似性能的同时，为单个摘要赋予绝对分数，解决了实际应用中的阈值需求。研究者还释放了合成上下文摘要数据以便未来研究。

Abstract: As large-language models have been increasingly used as automatic raters for
evaluating free-form content, including document summarization, dialog, and
story generation, work has been dedicated to evaluating such models by
measuring their correlations with human judgment. For \textit{sample-level}
performance, methods which operate by using pairwise comparisons between
machine-generated text perform well but often lack the ability to assign
absolute scores to individual summaries, an ability crucial for use cases that
require thresholding. In this work, we propose a direct-scoring method which
uses synthetic summaries to act as pairwise machine rankings at test time. We
show that our method performs comparably to state-of-the-art pairwise
evaluators in terms of axis-averaged sample-level correlations on the SummEval
(\textbf{+0.03}), TopicalChat (\textbf{-0.03}), and HANNA (\textbf{+0.05})
meta-evaluation benchmarks, and release the synthetic in-context summaries as
data to facilitate future work.

</details>


### [7] [From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics](https://arxiv.org/abs/2509.05484)
*Hajar Sakai,Yi-En Tseng,Mohammadsadegh Mikaeili,Joshua Bosire,Franziska Jovin*

Main category: cs.CL

TL;DR: \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5LLM\u57fa\u7840\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u533b\u9662\u7535\u8bdd\u4e2d\u5fc3\u7684\u5458\u5de5\u6d88\u606f\uff0c\u8fdb\u884c\u4e3b\u9898\u8bc6\u522b\u548c\u591a\u7c7b\u522b\u5206\u7c7b\uff0c\u6700\u4f73\u6a21\u578bo3\u8fbe\u523078.4%\u6743\u91cdF1\u5206\u6570\u3002


<details>
  <summary>Details</summary>
Motivation: \u533b\u9662\u7535\u8bdd\u4e2d\u5fc3\u4ea7\u751f\u5927\u91cf\u5458\u5de5\u6d88\u606f\u6570\u636e\uff0c\u4f46\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u9700\u8981\u6ce8\u91ca\u6570\u636e\u548c\u8f83\u9ad8\u8ba1\u7b97\u6210\u672c\uff0cLLM\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u533b\u7597\u5065\u5eb7\u5206\u6790\u65b9\u6848\u3002

Method: \u5f00\u53d1\u591a\u9636\u6bb5LLM\u57fa\u7840\u6846\u67b6\uff0c\u8bc4\u4f30\u5305\u62ec\u63a8\u7406\u3001\u901a\u7528\u548c\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u5185\u7684\u591a\u79cdLLM\u7c7b\u578b\uff0c\u5e76\u7ed3\u5408\u6570\u636e\u5b89\u5168\u63aa\u65bd\u548cHIPAA\u5408\u89c4\u8981\u6c42\u3002

Result: o3\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u523078.4%\u6743\u91cdF1\u5206\u6570\u548c79.2%\u51c6\u786e\u7387\uff0cgpt-5\u4ee5\u75.3%\u6743\u91cdF1\u5206\u6570\u548c76.2%\u51c6\u786e\u7387\u7d27\u968f\u5176\u540e\u3002\u5904\u7406\u8f93\u51fa\u96c6\u6210\u5230\u53ef\u89c6\u5316\u51b3\u7b56\u652f\u6301\u5de5\u5177\u4e2d\u3002

Conclusion: \u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5229\u7528\u5458\u5de5\u6d88\u606f\u6570\u636e\uff0c\u8bc6\u522b\u5bfc\u822a\u5458\u57f9\u8bad\u673a\u4f1a\uff0c\u652f\u6301\u6539\u5584\u60a3\u8005\u4f53\u9a8c\u548c\u62a4\u7406\u8d28\u91cf\uff0c\u4e3a\u533b\u7597\u5065\u5eb7\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u5206\u6790\u65b9\u6848\u3002

Abstract: Hospital call centers serve as the primary contact point for patients within
a hospital system. They also generate substantial volumes of staff messages as
navigators process patient requests and communicate with the hospital offices
following the established protocol restrictions and guidelines. This
continuously accumulated large amount of text data can be mined and processed
to retrieve insights; however, traditional supervised learning approaches
require annotated data, extensive training, and model tuning. Large Language
Models (LLMs) offer a paradigm shift toward more computationally efficient
methodologies for healthcare analytics. This paper presents a multi-stage
LLM-based framework that identifies staff message topics and classifies
messages by their reasons in a multi-class fashion. In the process, multiple
LLM types, including reasoning, general-purpose, and lightweight models, were
evaluated. The best-performing model was o3, achieving 78.4% weighted F1-score
and 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and
76.2% accuracy). The proposed methodology incorporates data security measures
and HIPAA compliance requirements essential for healthcare environments. The
processed LLM outputs are integrated into a visualization decision support tool
that transforms the staff messages into actionable insights accessible to
healthcare professionals. This approach enables more efficient utilization of
the collected staff messaging data, identifies navigator training
opportunities, and supports improved patient experience and care quality.

</details>


### [8] [The Token Tax: Systematic Bias in Multilingual Tokenization](https://arxiv.org/abs/2509.05486)
*Jessica M. Lundin,Ada Zhang,Nihal Karim,Hamza Louzan,Victor Wei,David Adelani,Cody Carroll*

Main category: cs.CL

TL;DR: 研究发现词汇分割效率对非洲语言性能有重要影响，词汇分割效率越低（每个词需要更多token），模型准确率就越低。推理模型在高低资源语言上都表现更好，同时token数量翻倍会导致训练成本和时间增加四倍。


<details>
  <summary>Details</summary>
Motivation: 研究词汇分割效率对形态复杂、低资源语言的不利影响，这些语言由于词汇分割效率低导致计算资源需求增加和准确率下降。

Method: 评估10个大语言模型在AfriMMLU数据集（9,000个多选题，5个科目，16种非洲语言）上的表现，分析词汇分割效率（tokens/word）与准确率的关系。

Result: 词汇分割效率可靠地预测准确率，效率越低准确率越低；推理模型在所有语言上都优于非推理模型；token数量翻倍使训练成本和时间增加四倍。

Conclusion: 需要开发形态学敏感的词汇分割方法、公平定价策略和多语言基准测试，以实现更公平的自然语言处理。

Abstract: Tokenization inefficiency imposes structural disadvantages on morphologically
complex, low-resource languages, inflating compute resources and depressing
accuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA
items; 5 subjects; 16 African languages) and show that fertility (tokens/word)
reliably predicts accuracy. Higher fertility consistently predicts lower
accuracy across all models and subjects. We further find that reasoning models
(DeepSeek, o1) consistently outperform non-reasoning peers across high and low
resource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in
prior generations. Finally, translating token inflation to economics, a
doubling in tokens results in quadrupled training cost and time, underscoring
the token tax faced by many languages. These results motivate morphologically
aware tokenization, fair pricing, and multilingual benchmarks for equitable
natural language processing (NLP).

</details>


### [9] [Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2509.05505)
*Mansi Garg,Lee-Chi Wang,Bhavesh Ghanchi,Sanjana Dumpala,Shreyash Kakde,Yen Chih Chen*

Main category: cs.CL

TL;DR: 基于RAG架构的生物医学文献问答系统，通过综合PubMed、医学百科等多源信息，使用MiniLM嵌入和Mistral-7B模型，在乳腺癌领域实现了事实一致性和语义相关性的显著提升


<details>
  <summary>Details</summary>
Motivation: 解决传统健康搜索引擎的不足以及公众获取生物医学研究信息的滞后问题，提高准确、有证据支撑的医学信息可访问性

Method: 采用检索增强生成(RAG)架构，整合PubMed文章、精选Q&A数据集和医学百科等多源信息，使用MiniLM语义嵌入和FAISS向量检索，通过QLoRA优化的Mistral-7B-v0.3模型进行答案生成

Result: 在乳腺癌文献领域的重点评估显示，系统在事实一致性和语义相关性方面比基线模型有显著提升，BERTScore(F1)指标表现优异

Conclusion: RAG增强语言模型有潜力缩小复杂生物医学文献与可访问公共健康知识之间的差距，为多语言适配、隐私保护推理和个性化医学AI系统的未来研究掘掘了道路

Abstract: This work presents a Biomedical Literature Question Answering (Q&A) system
based on a Retrieval-Augmented Generation (RAG) architecture, designed to
improve access to accurate, evidence-based medical information. Addressing the
shortcomings of conventional health search engines and the lag in public access
to biomedical research, the system integrates diverse sources, including PubMed
articles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant
information and generate concise, context-aware responses. The retrieval
pipeline uses MiniLM-based semantic embeddings and FAISS vector search, while
answer generation is performed by a fine-tuned Mistral-7B-v0.3 language model
optimized using QLoRA for efficient, low-resource training. The system supports
both general medical queries and domain-specific tasks, with a focused
evaluation on breast cancer literature demonstrating the value of
domain-aligned retrieval. Empirical results, measured using BERTScore (F1),
show substantial improvements in factual consistency and semantic relevance
compared to baseline models. The findings underscore the potential of
RAG-enhanced language models to bridge the gap between complex biomedical
literature and accessible public health knowledge, paving the way for future
work on multilingual adaptation, privacy-preserving inference, and personalized
medical AI systems.

</details>


### [10] [Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study](https://arxiv.org/abs/2509.05553)
*Serge Lionel Nikiema,Jordan Samhi,Micheline Bénédicte Moumoula,Albérick Euraste Djiré,Abdoul Kader Kaboré,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: 该研究提出双向推理作为测试AI真正理解概念的标准，发现现有模型存在认知专门化问题，并开发了对比微调(CFT)方法成功实现双向推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决AI领域一个基本问题：大型语言模型是真正理解概念还是仅仅识别模式。研究者认为真正的理解应该自然允许可逆性，即能够双向应用变换。

Method: 提出双向推理测试框架，开发对比微调(CFT)方法，使用三种示例进行训练：保持语义的正例、不同语义的负例、正向混淆示例。

Result: 实验证明CFT成功实现了双向推理，在保持正向任务能力的同时获得了强大的反向性能。

Conclusion: 双向推理既可作为评估真正理解的理论框架，也可作为开发更强大AI系统的实用训练方法。

Abstract: This research addresses a fundamental question in AI: whether large language
models truly understand concepts or simply recognize patterns. The authors
propose bidirectional reasoning,the ability to apply transformations in both
directions without being explicitly trained on the reverse direction, as a test
for genuine understanding. They argue that true comprehension should naturally
allow reversibility. For example, a model that can change a variable name like
userIndex to i should also be able to infer that i represents a user index
without reverse training. The researchers tested current language models and
discovered what they term cognitive specialization: when models are fine-tuned
on forward tasks, their performance on those tasks improves, but their ability
to reason bidirectionally becomes significantly worse. To address this issue,
they developed Contrastive Fine-Tuning (CFT), which trains models using three
types of examples: positive examples that maintain semantic meaning, negative
examples with different semantics, and forward-direction obfuscation examples.
This approach aims to develop deeper understanding rather than surface-level
pattern recognition and allows reverse capabilities to develop naturally
without explicit reverse training. Their experiments demonstrated that CFT
successfully achieved bidirectional reasoning, enabling strong reverse
performance while maintaining forward task capabilities. The authors conclude
that bidirectional reasoning serves both as a theoretical framework for
assessing genuine understanding and as a practical training approach for
developing more capable AI systems.

</details>


### [11] [Ad hoc conventions generalize to new referents](https://arxiv.org/abs/2509.05566)
*Anya Ji,Claire Augusta Bergey,Ron Eliav,Yoav Artzi,Robert D. Hawkins*

Main category: cs.CL

TL;DR: 研究表明人们在建立新共享命名系统时不是简单地创建任意标签，而是通过概念协调实现语义空间的重塑，这种协调能够泛化到未讨论的新对象上


<details>
  <summary>Details</summary>
Motivation: 探索人们如何谈论从未讨论过的事物，检验两种对立观点：一种是建立任意链接的命名系统（如专有名词），另一种是通过概念对齐形成共享描述方式

Method: 使用KiloGram数据集的302对参与者进行双人交流研究，让参与者通过重复交流为一组抽象七巧板图像建立指称惯例，然后测量他们对未讨论图像的描述对齐程度

Result: 发现了强烈的泛化证据：伙伴间的对齐程度相对于前测标签显著增加；泛化程度随视觉相似性非线性衰减（符合Shepard定律）；在不同可命名性水平的图像上都表现稳健

Conclusion: 临时约定不是任意标签，而是反映了真正的概念协调，这对指称理论和设计更具适应性的语言代理具有重要意义

Abstract: How do people talk about things they've never talked about before? One view
suggests that a new shared naming system establishes an arbitrary link to a
specific target, like proper names that cannot extend beyond their bearers. An
alternative view proposes that forming a shared way of describing objects
involves broader conceptual alignment, reshaping each individual's semantic
space in ways that should generalize to new referents. We test these competing
accounts in a dyadic communication study (N=302) leveraging the
recently-released KiloGram dataset containing over 1,000 abstract tangram
images. After pairs of participants coordinated on referential conventions for
one set of images through repeated communication, we measured the extent to
which their descriptions aligned for undiscussed images. We found strong
evidence for generalization: partners showed increased alignment relative to
their pre-test labels. Generalization also decayed nonlinearly with visual
similarity (consistent with Shepard's law) and was robust across levels of the
images' nameability. These findings suggest that ad hoc conventions are not
arbitrary labels but reflect genuine conceptual coordination, with implications
for theories of reference and the design of more adaptive language agents.

</details>


### [12] [Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation](https://arxiv.org/abs/2509.05602)
*Hongyan Xie,Yitong Yao,Yikun Ban,Zixuan Huang,Deqing Wang,Zhenhe Wu,Haoxiang Su,Chao Wang,Shuangyong Song,Xuelong Li*

Main category: cs.CL

TL;DR: CoPeD方法通过正确性感知任务设置和加权损失函数，提高小语言模型从大语言模型生成的思维链数据中学习推理的质量，减少噪声推理的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的思维链数据可能存在噪声推理，这些推理要么无法支持答案，要么对答案预测没有帮助，导致小语言模型学习到虚假的相关性，影响推理质量。

Method: 提出Chain-of-Thought Correctness Perception Distillation (CoPeD)：1) 正确性感知任务设置，鼓励模型基于正确推理预测答案并修正错误推理；2) 正确性感知加权损失，根据推理和答案的联合损失动态调整训练样本权重。

Result: 实验表明CoPeD在分布内和分布外基准推理数据集上都有效。

Conclusion: CoPeD通过改进任务设置和数据利用方式，有效提升了学生模型的推理质量，使其能够更好地从大语言模型生成的思维链数据中学习。

Abstract: Large language models (LLMs) excel at reasoning tasks but are expensive to
deploy. Thus small language models (SLMs) are fine-tuned on CoT data generated
by LLMs to copy LLMs' abilities. However, these CoT data may include noisy
rationales that either fail to substantiate the answers or contribute no
additional information to support answer prediction, which leads SLMs to
capture spurious correlations between questions and answers and compromise the
quality of reasoning. In this work, we propose Chain-of-Thought Correctness
Perception Distillation (CoPeD), which aims to improve the reasoning quality of
the student model from the perspectives of task setting and data utilization.
Firstly, we introduce a correctness-aware task setting that encourages the
student model to predict answers based on correct rationales and revise them
when they are incorrect. This setting improves the faithfulness of reasoning
and allows the model to learn from its mistakes. Then, we propose a
Correctness-Aware Weighted loss, which dynamically adjusts the contribution of
each training instance based on the combined loss of the rationale and the
answer. This strategy encourages the model to focus more on samples where the
rationale offers stronger support for the correct answer. Experiments have
shown that CoPeD is effective on both in-distribution (IND) and
out-of-distribution (OOD) benchmark reasoning datasets.

</details>


### [13] [Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation](https://arxiv.org/abs/2509.05605)
*Qiyuan Chen,Hongsen Huang,Qian Shao,Jiahe Chen,Jintai Chen,Hongxia Xu,Renjie Hua,Ren Chuan,Jian Wu*

Main category: cs.CL

TL;DR: Icon²提出了一种利用LLM表示空间内在调节机制的高效偏好数据集构建方法，通过层间方向向量编码人类偏好，双向控制生成具有明确对齐差异的响应对，显著提升对齐效果和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统偏好数据集构建方法存在分布不匹配和计算开销大的问题：依赖预收集指令导致与目标模型分布不匹配，采样多个随机响应带来巨大计算负担。

Method: 1) 提取层间方向向量编码复杂人类偏好；2) 基于内在一致性过滤自合成指令；3) 解码时应用双向内在控制引导token表示，精确生成具有明确对齐差异的响应对。

Result: Llama3-8B和Qwen2-7B在AlpacaEval 2.0上平均胜率提升13.89%，在Arena-Hard上提升13.45%，同时计算成本降低高达48.1%。

Conclusion: Icon²通过利用LLM表示空间的内在调节机制，实现了高效且定制化的偏好数据集构建，在提升对齐效果的同时显著降低了计算开销。

Abstract: Large Language Models (LLMs) require high quality preference datasets to
align with human preferences. However, conventional methods for constructing
such datasets face significant challenges: reliance on pre-collected
instructions often leads to distribution mismatches with target models, while
the need for sampling multiple stochastic responses introduces substantial
computational overhead. In this work, we explore a paradigm shift by leveraging
inherent regulation of LLMs' representation space for efficient and tailored
preference dataset construction, named Icon$^{2}$. Specifically, it first
extracts layer-wise direction vectors to encode sophisticated human preferences
and then uses these vectors to filter self-synthesized instructions based on
their inherent consistency. During decoding, bidirectional inherent control is
applied to steer token representations, enabling the precise generation of
response pairs with clear alignment distinctions. Experimental results
demonstrate significant improvements in both alignment and efficiency.
Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on
AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by
up to 48.1%.

</details>


### [14] [Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents](https://arxiv.org/abs/2509.05607)
*Qiyuan Chen,Jiahe Chen,Hongsen Huang,Qian Shao,Jintai Chen,Renjie Hua,Hongxia Xu,Ruijia Wu,Ren Chuan,Jian Wu*

Main category: cs.CL

TL;DR: 这篇论文提出了一个结构化框架来解决生成式搜索引擎优化(GSEO)的挑战，包括构建大规模测试集和自动化内容优化系统。


<details>
  <summary>Details</summary>
Motivation: 传统的SEO指标在生成式搜索引擎时代已经失效，需要新的方法来理解、测量和优化内容对生成答案的影响。

Method: 构建了CC-GSEO-Bench大规模测试集，提出多维度评估框架，并设计了一个新的多代理系统来自动化内容的战略性精炼。

Result: 实验分析揭示了内容影响的新动态特征，为创作者提供了可操作的策略，为未来GSEO研究打下了基础。

Conclusion: 该框架为生成式搜索引擎优化提供了系统性的解决方案，通过量化内容影响和自动化优化流程，有力地应对了从传统排名搜索到生成式搜索的范式转移。

Abstract: The paradigm shift from traditional ranked-based search to Generative Search
Engines has rendered conventional SEO metrics obsolete, creating an urgent need
to understand, measure, and optimize for content influence on synthesized
answers. This paper introduces a comprehensive, end-to-end framework for
Generative Search Engine Optimization (GSEO) to address this challenge. We make
two primary contributions. First, we construct CC-GSEO-Bench, a large-scale,
content-centric benchmark, and propose a multi-dimensional evaluation framework
that systematically quantifies influence, moving beyond surface-level
attribution to assess substantive semantic impact. Second, we design a novel
multi-agent system that operationalizes this framework, automating the
strategic refinement of content through a collaborative analyze-revise-evaluate
workflow. Our empirical analysis using this framework reveals novel insights
into the dynamics of content influence, offering actionable strategies for
creators and establishing a principled foundation for future GSEO research.

</details>


### [15] [New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR](https://arxiv.org/abs/2509.05609)
*Xugang Lu,Peng Shen,Yu Tsao,Hisashi Kawai*

Main category: cs.CL

TL;DR: 通过将声学-语言表征对齐视为检测问题，提出了一种基于不平衡最优运输的对齐模型，能够灵活处理多对一、一对多和无对应关系，提高ASR系统性能。


<details>
  <summary>Details</summary>
Motivation: 解决声学和语言表征之间的对齐挑战，这种对齐具有内在的结构不对称性（多对一、一对多、无对应），传统方法难以灵活处理这种不平衡的匹配条件。

Method: 提出一种不平衡最优运输基于的对齐模型，通过软匹配和部分匹配来显式处理分布不匹配和结构不对称性，确保每个语言标记都有至少一个声学观测基础。

Result: 在基于CTC的ASR系统中进行评估，实验结果表明该方法能够灵活控制匹配程度，显著提高ASR性能。

Conclusion: 将对齐问题框架为检测问题并采用不平衡最优运输方法，能够有效处理声学-语言表征间的结构不对称性，为ASR知识转移提供了一种灵活且有效的对齐方法。

Abstract: Aligning acoustic and linguistic representations is a central challenge to
bridge the pre-trained models in knowledge transfer for automatic speech
recognition (ASR). This alignment is inherently structured and asymmetric:
while multiple consecutive acoustic frames typically correspond to a single
linguistic token (many-to-one), certain acoustic transition regions may relate
to multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often
include frames with no linguistic counterpart, such as background noise or
silence may lead to imbalanced matching conditions. In this work, we take a new
insight to regard alignment and matching as a detection problem, where the goal
is to identify meaningful correspondences with high precision and recall
ensuring full coverage of linguistic tokens while flexibly handling redundant
or noisy acoustic frames in transferring linguistic knowledge for ASR. Based on
this new insight, we propose an unbalanced optimal transport-based alignment
model that explicitly handles distributional mismatch and structural
asymmetries with soft and partial matching between acoustic and linguistic
modalities. Our method ensures that every linguistic token is grounded in at
least one acoustic observation, while allowing for flexible, probabilistic
mappings from acoustic to linguistic units. We evaluate our proposed model with
experiments on an CTC-based ASR system with a pre-trained language model for
knowledge transfer. Experimental results demonstrate the effectiveness of our
approach in flexibly controlling degree of matching and hence to improve ASR
performance.

</details>


### [16] [From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics](https://arxiv.org/abs/2509.05617)
*Shay Dahary,Avi Edana,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

TL;DR: 该论文研究了歌曲歌词的多标签情感归因任务，通过构建人工标注数据集并评估零样本LLM和微调BERT模型，揭示了不同模型在歌词情感识别中的表现差异。


<details>
  <summary>Details</summary>
Motivation: 歌词的情感内容对听众体验和音乐偏好有重要影响，需要可靠的方法来识别歌词中的复杂情感。

Method: 使用平均意见得分(MOS)方法构建人工标注数据集，评估多个公开LLM的零样本性能，并微调BERT模型进行多标签情感分数预测。

Result: 实验结果显示零样本和微调模型在捕捉歌词细腻情感内容方面各有优势和局限性。

Conclusion: LLM在创意文本情感识别方面具有潜力，为基于情感的音乐信息检索应用提供了模型选择策略的见解。

Abstract: The emotional content of song lyrics plays a pivotal role in shaping listener
experiences and influencing musical preferences. This paper investigates the
task of multi-label emotional attribution of song lyrics by predicting six
emotional intensity scores corresponding to six fundamental emotions. A
manually labeled dataset is constructed using a mean opinion score (MOS)
approach, which aggregates annotations from multiple human raters to ensure
reliable ground-truth labels. Leveraging this dataset, we conduct a
comprehensive evaluation of several publicly available large language models
(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model
specifically for predicting multi-label emotion scores. Experimental results
reveal the relative strengths and limitations of zero-shot and fine-tuned
models in capturing the nuanced emotional content of lyrics. Our findings
highlight the potential of LLMs for emotion recognition in creative texts,
providing insights into model selection strategies for emotion-based music
information retrieval applications. The labeled dataset is available at
https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.

</details>


### [17] [Few-Shot Query Intent Detection via Relation-Aware Prompt Learning](https://arxiv.org/abs/2509.05635)
*Liang Zhang,Yuan Li,Shijie Zhang,Zheng Zhang,Xitong Li*

Main category: cs.CL

TL;DR: SAID框架首次统一整合文本和关系结构信息进行预训练，通过QueryAdapt机制在关系标记级别生成意图特定的关系标记，显著提升少样本意图检测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注文本数据，忽略了对话系统中关键的结构信息（如查询-查询关系和查询-回答关系），导致意图检测效果受限

Method: 提出SAID框架，统一整合文本和关系结构信息进行预训练；设计QueryAdapt机制，在关系标记级别生成意图特定的关系标记，实现更细粒度的知识迁移

Result: 在两个真实数据集上的大量实验结果表明，SAID显著优于最先进的方法

Conclusion: SAID通过有效整合文本和结构信息，为少样本意图检测提供了新的解决方案，证明了结构信息在对话系统中的重要性

Abstract: Intent detection is a crucial component of modern conversational systems,
since accurately identifying user intent at the beginning of a conversation is
essential for generating effective responses. Recent efforts have focused on
studying this problem under a challenging few-shot scenario. These approaches
primarily leverage large-scale unlabeled dialogue text corpora to pretrain
language models through various pretext tasks, followed by fine-tuning for
intent detection with very limited annotations. Despite the improvements
achieved, existing methods have predominantly focused on textual data,
neglecting to effectively capture the crucial structural information inherent
in conversational systems, such as the query-query relation and query-answer
relation. To address this gap, we propose SAID, a novel framework that
integrates both textual and relational structure information in a unified
manner for model pretraining for the first time. Building on this framework, we
further propose a novel mechanism, the query-adaptive attention network
(QueryAdapt), which operates at the relation token level by generating
intent-specific relation tokens from well-learned query-query and query-answer
relations explicitly, enabling more fine-grained knowledge transfer. Extensive
experimental results on two real-world datasets demonstrate that SAID
significantly outperforms state-of-the-art methods.

</details>


### [18] [LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding](https://arxiv.org/abs/2509.05657)
*Yuxuan Hu,Jihao Liu,Ke Wang,Jinliang Zhen,Weikang Shi,Manyuan Zhang,Qi Dou,Rui Liu,Aojun Zhou,Hongsheng Li*

Main category: cs.CL

TL;DR: LM-Searcher是一个基于大语言模型的跨领域神经架构搜索框架，通过通用数值编码NCode和排名任务重构，无需领域特定调优即可实现高性能架构搜索。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的神经架构搜索方法严重依赖提示工程和领域特定调优，限制了其在实际应用中的实用性和可扩展性。

Method: 提出NCode通用数值字符串表示法编码神经架构，将NAS问题重构为排名任务，使用基于剪枝的子空间采样策略生成指令调优样本训练LLM。

Result: 在领域内（如图像分类CNN）和跨领域（如分割和生成的LoRA配置）任务中均达到竞争性性能。

Conclusion: LM-Searcher为灵活且可泛化的基于LLM的架构搜索建立了新范式，具有强大的跨领域适应能力。

Abstract: Recent progress in Large Language Models (LLMs) has opened new avenues for
solving complex optimization problems, including Neural Architecture Search
(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt
engineering and domain-specific tuning, limiting their practicality and
scalability across diverse tasks. In this work, we propose LM-Searcher, a novel
framework that leverages LLMs for cross-domain neural architecture optimization
without the need for extensive domain-specific adaptation. Central to our
approach is NCode, a universal numerical string representation for neural
architectures, which enables cross-domain architecture encoding and search. We
also reformulate the NAS problem as a ranking task, training LLMs to select
high-performing architectures from candidate pools using instruction-tuning
samples derived from a novel pruning-based subspace sampling strategy. Our
curated dataset, encompassing a wide range of architecture-performance pairs,
encourages robust and transferable learning. Comprehensive experiments
demonstrate that LM-Searcher achieves competitive performance in both in-domain
(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA
configurations for segmentation and generation) tasks, establishing a new
paradigm for flexible and generalizable LLM-based architecture search. The
datasets and models will be released at https://github.com/Ashone3/LM-Searcher.

</details>


### [19] [Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning](https://arxiv.org/abs/2509.05660)
*Hong Su*

Main category: cs.CL

TL;DR: 本文提出了一种扩展方法重用范围的新方法，允许在问题相似度较低或存在隐藏相似性的情况下进行跨问题方法重用，通过分离问题和解决方案来指导LLM专注于解决方案迁移而非问题识别。


<details>
  <summary>Details</summary>
Motivation: 现有方法重用技术通常要求问题高度相似，限制了应用范围。本文旨在扩展方法重用的适用范围，使其能够处理相似度较低或具有隐藏相似性的问题。

Method: 首先将问题和解决方案分离，而不是直接将问题-解决方案对输入LLM。然后指导LLM将解决方案适配到新的相关问题上，使其专注于解决方案迁移而非问题识别。该方法还扩展到仅共享部分特征或隐藏特征的情况。

Result: 实验验证表明，该范围扩展方法提高了筛选出可重用解决方案的概率，从而提升了跨问题方法重用的有效性。

Conclusion: 通过分离问题和解决方案并指导LLM专注于解决方案迁移，可以显著扩展方法重用的适用范围，使其能够处理相似度较低或具有隐藏相似性的问题，提高跨问题方法重用的效果。

Abstract: Large language models (LLMs) have been widely applied to assist in finding
solutions for diverse questions. Prior work has proposed representing a method
as a pair of a question and its corresponding solution, enabling method reuse.
However, existing approaches typically require the questions to be highly
similar. In this paper, we extend the scope of method reuse to address
questions with low similarity or with hidden similarities that are not
explicitly observable. For questions that are similar in a general-specific
sense (i.e., broader or narrower in scope), we propose to first separate the
question and solution, rather than directly feeding the pair to the LLM. The
LLM is then guided to adapt the solution to new but related questions, allowing
it to focus on solution transfer rather than question recognition. Furthermore,
we extend this approach to cases where questions only share partial features or
hidden characteristics. This enables cross-question method reuse beyond
conventional similarity constraints. Experimental verification shows that our
scope-extension approach increases the probability of filtering out reusable
solutions, thereby improving the effectiveness of cross-question method reuse.

</details>


### [20] [Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian](https://arxiv.org/abs/2509.05668)
*Michael Hoffmann,Jophin John,Stefan Schweter,Gokul Ramakrishnan,Hoi-Fong Mak,Alice Zhang,Dmitry Gaynullin,Nicolay J. Hammer*

Main category: cs.CL

TL;DR: Llama-GENBA-10B是一个10B参数的三语基础模型，基于Llama 3.1-8B构建，针对英语、德语和巴伐利亚语进行持续预训练，解决了英语中心偏见问题，在巴伐利亚语上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型中的英语中心偏见问题，为德语NLP社区提供支持，并促进低资源语言巴伐利亚语的发展。

Method: 基于Llama 3.1-8B扩展到10B参数，使用164B tokens（82B英语、82B德语、80M巴伐利亚语）进行持续预训练，创建统一的分词器，优化架构和语言比例超参数，建立首个标准化三语评估套件。

Result: 模型在跨语言性能上表现强劲，微调变体在巴伐利亚语上超越Apertus-8B-2509和gemma-2-9b，成为该语言类别最佳模型，同时在英语上优于EuroLLM，在德语上与其相当。

Conclusion: 该研究为整合低资源语言的包容性基础模型提供了蓝图，展示了高效的大规模多语言预训练方法，并记录了能源使用情况。

Abstract: We present Llama-GENBA-10B, a trilingual foundation model addressing
English-centric bias in large language models. Built on Llama 3.1-8B and scaled
to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens
(82B English, 82B German, and 80M Bavarian), balancing resources while
preventing English dominance. Targeted at the German NLP community, the model
also promotes Bavarian as a low-resource language. Development tackled four
challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)
creating a unified tokenizer for English, German, and Bavarian, (3) optimizing
architecture and language-ratio hyperparameters for cross-lingual transfer, and
(4) establishing the first standardized trilingual evaluation suite by
translating German benchmarks into Bavarian. Evaluations show that
Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned
variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing
itself as the best model in its class for this language, while also
outperforming EuroLLM in English and matching its results in German. Training
on the Cerebras CS-2 demonstrated efficient large-scale multilingual
pretraining with documented energy use, offering a blueprint for inclusive
foundation models that integrate low-resource languages.

</details>


### [21] [Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models](https://arxiv.org/abs/2509.05691)
*Ningyuan Deng,Hanyu Duan,Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 嵌入模型在处理文本中数字细节方面表现差强，尤其在金融和健康领域存在显著缺陷


<details>
  <summary>Details</summary>
Motivation: 当前嵌入模型评测通常涉及不需要理解细微数字信息的任务，而在金融、健康等领域数字精确性至关重要，因此需要考察嵌入模型能否准确编码数字内容

Method: 使用金融上下文中的合成数据，对13个广泛使用的文本嵌入模型进行评估

Result: 发现嵌入模型普遍在准确捕捉数字细节方面表现差强

Conclusion: 需要加强嵌入模型的数字处理能力，以支持更可靠的NLP系统在数字敏感领域的应用

Abstract: Text embedding models are widely used in natural language processing
applications. However, their capability is often benchmarked on tasks that do
not require understanding nuanced numerical information in text. As a result,
it remains unclear whether current embedding models can precisely encode
numerical content, such as numbers, into embeddings. This question is critical
because embedding models are increasingly applied in domains where numbers
matter, such as finance and healthcare. For example, Company X's market share
grew by 2\% should be interpreted very differently from Company X's market
share grew by 20\%, even though both indicate growth in market share. This
study aims to examine whether text embedding models can capture such nuances.
Using synthetic data in a financial context, we evaluate 13 widely used text
embedding models and find that they generally struggle to capture numerical
details accurately. Our further analyses provide deeper insights into embedding
numeracy, informing future research to strengthen embedding model-based NLP
systems with improved capacity for handling numerical content.

</details>


### [22] [A Survey of the State-of-the-Art in Conversational Question Answering Systems](https://arxiv.org/abs/2509.05716)
*Manoj Madushanka Perera,Adnan Mahmood,Kasun Eranda Wijethilake,Fahmida Islam,Maryam Tahermazandarani,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 这是一份关于对话式问答系统(ConvQA)的综述性论文，完整分析了该领域的最新进展，包括核心组件、机器学习技术、大语言模型以及数据集等方面。


<details>
  <summary>Details</summary>
Motivation: 对话式问答系统在NLP领域趋于重要地位，能够支持机器进行动态和上下文感知的对话。这些能力在客服、教育、法律、医疗等领域都有广泛应用。需要对该领域的最新进展进行系统性的综述分析。

Method: 论文采用综述性分析方法，首先检视ConvQA系统的三大核心组件：历史选择、问题理解和答案预测，分析它们如何协同保证多轮对话的连贯性和相关性。进而研究了加强学习、对比学习、迁移学习等先进机器学习技术的应用，以及RoBERTa、GPT-4、Gemini 2.0 Flash、Mistral 7B、LLaMA 3等大语言模型的关键作用。

Result: 论文提供了对对话式问答领域的全面概览，详细分析了各种先进技术在提高ConvQA系统准确性和效率方面的效果。大语言模型通过数据可扩展性和架构进步展现了重大影响。同时还完整分析了关键的ConvQA数据集。

Conclusion: 这份综述为对话式问答领域提供了丰富的视角，并指明了未来的研究方向。工作对领域的发展进行了系统性的总结，为该领域的进一步发展提供了有价值的指导。

Abstract: Conversational Question Answering (ConvQA) systems have emerged as a pivotal
area within Natural Language Processing (NLP) by driving advancements that
enable machines to engage in dynamic and context-aware conversations. These
capabilities are increasingly being applied across various domains, i.e.,
customer support, education, legal, and healthcare where maintaining a coherent
and relevant conversation is essential. Building on recent advancements, this
survey provides a comprehensive analysis of the state-of-the-art in ConvQA.
This survey begins by examining the core components of ConvQA systems, i.e.,
history selection, question understanding, and answer prediction, highlighting
their interplay in ensuring coherence and relevance in multi-turn
conversations. It further investigates the use of advanced machine learning
techniques, including but not limited to, reinforcement learning, contrastive
learning, and transfer learning to improve ConvQA accuracy and efficiency. The
pivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,
Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact
through data scalability and architectural advancements. Additionally, this
survey presents a comprehensive analysis of key ConvQA datasets and concludes
by outlining open research directions. Overall, this work offers a
comprehensive overview of the ConvQA landscape and provides valuable insights
to guide future advancements in the field.

</details>


### [23] [Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language Models](https://arxiv.org/abs/2509.05719)
*Donya Rooein,Flor Miriam Plaza-del-Arco,Debora Nozza,Dirk Hovy*

Main category: cs.CL

TL;DR: 这篇论文对法尔语在主观性NLP任务中的真实资源状况进行了批判性分析，发现虽然数字文本数量不断增长，但在情感分析、情绪分析和毒性检测任务中仍面临着严重的数据质量和可用性挑战


<details>
  <summary>Details</summary>
Motivation: 尽管法尔语被标签为中等资源语言，但研究者们认为需要更详细地检查其在主观性NLP任务中的真实状况，以更准确地评估语言资源的实际水平

Method: 研究团队重点分析了三个主观性任务（情感分析、情绪分析、毒性检测），综述110篇相关发表的文献，并评估了现有数据集的可用性和质量

Result: 研究发现公开可用的数据集极其缺乏，现有数据集缺少关键人口统计因素（如年龄、性别），预测模型在不同数据集和模型之间表现极不稳定

Conclusion: 仅仅数据量的增长不能实质性改善语言在NLP领域的前景，数据质量、多样性和充分的人口统计信息对主观性语言建模至关重要

Abstract: Given Farsi's speaker base of over 127 million people and the growing
availability of digital text, including more than 1.3 million articles on
Wikipedia, it is considered a middle-resource language. However, this label
quickly crumbles when the situation is examined more closely. We focus on three
subjective tasks (Sentiment Analysis, Emotion Analysis, and Toxicity Detection)
and find significant challenges in data availability and quality, despite the
overall increase in data availability. We review 110 publications on subjective
tasks in Farsi and observe a lack of publicly available datasets. Furthermore,
existing datasets often lack essential demographic factors, such as age and
gender, that are crucial for accurately modeling subjectivity in language. When
evaluating prediction models using the few available datasets, the results are
highly unstable across both datasets and models. Our findings indicate that the
volume of data is insufficient to significantly improve a language's prospects
in NLP.

</details>


### [24] [QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing](https://arxiv.org/abs/2509.05729)
*Charles M. Varmantchaonala,Niclas GÖtting,Nils-Erik SchÜtte,Jean Louis E. K. Fendji,Christopher Gies*

Main category: cs.CL

TL;DR: 量子自然语言处理新方法QCSE，通过五种上下文矩阵计算方法，利用量子计算特性学习上下文敏感的词嵌入表示，在低资源语言Fulani和英语语料上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 利用量子计算的特有性质（如表达力）来处理自然语言的复杂上下文关系，并为低资源语言提供更好的NLP解决方案。

Method: 提出QCSE预训练量子上下文敏感嵌入模型，采用5种上下文矩阵计算方法（指数衰减、正弦调制、相位移、哈希变换等）来创建基于语境的词表示。

Result: 在Fulani（低资源非洲语言）和英语语料上的评估显示，QCSE能够抓取上下文敏感性，并利用量子系统的表达力来表示丰富的语言信息。

Conclusion: 这项工作展示了量子计算在NLP中的强大潜力，为应对各种语言任务和领域的真实世界语言挑战开启了新途径。

Abstract: Quantum Natural Language Processing (QNLP) offers a novel approach to
encoding and understanding the complexity of natural languages through the
power of quantum computation. This paper presents a pretrained quantum
context-sensitive embedding model, called QCSE, that captures context-sensitive
word embeddings, leveraging the unique properties of quantum systems to learn
contextual relationships in languages. The model introduces quantum-native
context learning, enabling the utilization of quantum computers for linguistic
tasks. Central to the proposed approach are innovative context matrix
computation methods, designed to create unique, representations of words based
on their surrounding linguistic context. Five distinct methods are proposed and
tested for computing the context matrices, incorporating techniques such as
exponential decay, sinusoidal modulation, phase shifts, and hash-based
transformations. These methods ensure that the quantum embeddings retain
context sensitivity, thereby making them suitable for downstream language tasks
where the expressibility and properties of quantum systems are valuable
resources. To evaluate the effectiveness of the model and the associated
context matrix methods, evaluations are conducted on both a Fulani corpus, a
low-resource African language, dataset of small size and an English corpus of
slightly larger size. The results demonstrate that QCSE not only captures
context sensitivity but also leverages the expressibility of quantum systems
for representing rich, context-aware language information. The use of Fulani
further highlights the potential of QNLP to mitigate the problem of lack of
data for this category of languages. This work underscores the power of quantum
computation in natural language processing (NLP) and opens new avenues for
applying QNLP to real-world linguistic challenges across various tasks and
domains.

</details>


### [25] [Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification](https://arxiv.org/abs/2509.05741)
*Fernando Gabriela García,Qiyang Shi,Zilin Feng*

Main category: cs.CL

TL;DR: VeriFact-CoT是一种通过事实验证-反思-引用整合的多阶段机制来减少LLM幻觉和缺乏可信引用的问题的新方法


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在生成复杂、事实敏感内容时普遍存在的幻觉问题和缺乏可信引用来源的问题

Method: 采用多阶段的'事实验证-反思-引用整合'机制，使LLM能够批判性地自我检查和修订其中间推理步骤和最终答案

Result: 显著提高了生成输出的客观准确性、可信度和可追溯性

Conclusion: 该方法使LLM在需要高保真度的应用场景（如科学研究、新闻报道和法律咨询）中更加可靠

Abstract: This research introduces VeriFact-CoT (Verified Factual Chain-of-Thought), a
novel method designed to address the pervasive issues of hallucination and the
absence of credible citation sources in Large Language Models (LLMs) when
generating complex, fact-sensitive content. By incorporating a multi-stage
mechanism of 'fact verification-reflection-citation integration,' VeriFact-CoT
empowers LLMs to critically self-examine and revise their intermediate
reasoning steps and final answers. This process significantly enhances the
objective accuracy, trustworthiness, and traceability of the generated outputs,
making LLMs more reliable for applications demanding high fidelity such as
scientific research, news reporting, and legal consultation.

</details>


### [26] [LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization](https://arxiv.org/abs/2509.05863)
*Luis Felipe Chary,Miguel Arjona Ramirez*

Main category: cs.CL

TL;DR: LatinX是一个多语言文本转语音模型，用于语音到语音翻译，能够在跨语言时保持源说话人的身份特征。该模型采用三阶段训练策略，包括预训练、监督微调和DPO对齐，在英语和罗曼语系语言上表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在跨语言语音翻译中保持说话人身份特征的多语言TTS模型，解决现有系统在说话人相似性保持方面的不足。

Method: 使用12层仅解码器Transformer架构，采用三阶段训练：文本到音频映射预训练、零样本语音克隆监督微调、基于WER和说话人相似性指标的DPO对齐。

Result: DPO版本的LatinX相比微调基线持续降低WER并提高客观相似性。人工评估显示其感知说话人相似性优于强基线XTTSv2，但揭示了客观与主观测量之间的差距。

Conclusion: LatinX在多语言语音翻译中有效保持说话人身份，未来工作包括平衡偏好信号和低延迟架构的探索。

Abstract: We present LatinX, a multilingual text-to-speech (TTS) model for cascaded
speech-to-speech translation that preserves the source speaker's identity
across languages. LatinX is a 12-layer decoder-only Transformer trained in
three stages: (i) pre-training for text-to-audio mapping, (ii) supervised
fine-tuning for zero-shot voice cloning, and (iii) alignment with Direct
Preference Optimization (DPO) using automatically labeled pairs based on Word
Error Rate (WER) and speaker-similarity metrics. Trained on English and Romance
languages with emphasis on Portuguese, LatinX with DPO consistently reduces WER
and improves objective similarity over the fine-tuned baseline. Human
evaluations further indicate stronger perceived speaker similarity than a
strong baseline (XTTSv2), revealing gaps between objective and subjective
measures. We provide cross-lingual analyses and discuss balanced preference
signals and lower-latency architectures as future work.

</details>


### [27] [ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula](https://arxiv.org/abs/2509.05867)
*ZiXuan Zhang,Bowen Hao,Yingjie Li,Hongzhi Yin*

Main category: cs.CL

TL;DR: 这篇论文提出了ZhiFangDanTai框架，通过结合图谱检索增强生成和大语言模型微调技术，解决中医方前生成中的细节缺失和解释不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有中医方前生成模型在方前组成、权家臣使角色、功效、禁忌等细节信息上存在不足，数据集缺乏详细的舌谋脉诊断等信息，限制了模型输出的深度。

Method: 提出ZhiFangDanTai框架，结合图谱检索增强生成(GraphRAG)和LLM微调技术，通过GraphRAG检索和综合结构化中医知识，并构建增强指令数据集来改善LLM的信息整合能力。

Result: 在收集的数据集和临床数据集上的实验结果显示，ZhiFangDanTai在中医方前任务上较现有最先进模型取得了显著改进。

Conclusion: 该研究提供了理论证明，证明结合GraphRAG与微调技术可以降低中医方前任务中的泛化误差和幻觉率，为中医方前生成提供了有效的解决方案。

Abstract: Traditional Chinese Medicine (TCM) formulas play a significant role in
treating epidemics and complex diseases. Existing models for TCM utilize
traditional algorithms or deep learning techniques to analyze formula
relationships, yet lack comprehensive results, such as complete formula
compositions and detailed explanations. Although recent efforts have used TCM
instruction datasets to fine-tune Large Language Models (LLMs) for explainable
formula generation, existing datasets lack sufficient details, such as the
roles of the formula's sovereign, minister, assistant, courier; efficacy;
contraindications; tongue and pulse diagnosis-limiting the depth of model
outputs. To address these challenges, we propose ZhiFangDanTai, a framework
combining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM
fine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured
TCM knowledge into concise summaries, while also constructing an enhanced
instruction dataset to improve LLMs' ability to integrate retrieved
information. Furthermore, we provide novel theoretical proofs demonstrating
that integrating GraphRAG with fine-tuning techniques can reduce generalization
error and hallucination rates in the TCM formula task. Experimental results on
both collected and clinical datasets demonstrate that ZhiFangDanTai achieves
significant improvements over state-of-the-art models. Our model is
open-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.

</details>


### [28] [MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries](https://arxiv.org/abs/2509.05878)
*François Grolleau,Emily Alsentzer,Timothy Keyes,Philip Chung,Akshay Swaminathan,Asad Aali,Jason Hom,Tridu Huynh,Thomas Lew,April S. Liang,Weihan Chu,Natasha Z. Steele,Christina F. Lin,Jingkun Yang,Kameron C. Black,Stephen P. Ma,Fateme N. Haredasht,Nigam H. Shah,Kevin Schulman,Jonathan H. Chen*

Main category: cs.CL

TL;DR: 这篇论文提出了MedFactEval评估框架和MedAgentBrief生成流程，用于解决LLM生成临床文本中事实准确性评估的挑战，通过多LLM投票机制实现了与专家评审高度一致的效果。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型生成临床文本中事实准确性评估的挑战，因为专家审查方式无法满足连续质量保障的大规模需求。

Method: 提出MedFactEval框架：临床医生定义关键事实，通过"LLM院士团"（多个LLM多数投票）评估生成摘要中的事实包含情况；同时提出MedAgentBrief多步流程来生成高质量出院摘要。

Result: MedFactEval LLM院士团与7名医生组成的金标准引用达到很高一致性（Cohen's kappa=81%），表现统计上不差于单个人类专家（kappa=67%，P < 0.001）。

Conclusion: 该研究提供了稳健的评估框架和高性能生成流程，为生成式AI在临床工作流中的负责任部署提供了全面解决方案。

Abstract: Evaluating factual accuracy in Large Language Model (LLM)-generated clinical
text is a critical barrier to adoption, as expert review is unscalable for the
continuous quality assurance these systems require. We address this challenge
with two complementary contributions. First, we introduce MedFactEval, a
framework for scalable, fact-grounded evaluation where clinicians define
high-salience key facts and an "LLM Jury"--a multi-LLM majority vote--assesses
their inclusion in generated summaries. Second, we present MedAgentBrief, a
model-agnostic, multi-step workflow designed to generate high-quality, factual
discharge summaries. To validate our evaluation framework, we established a
gold-standard reference using a seven-physician majority vote on
clinician-defined key facts from inpatient cases. The MedFactEval LLM Jury
achieved almost perfect agreement with this panel (Cohen's kappa=81%), a
performance statistically non-inferior to that of a single human expert
(kappa=67%, P < 0.001). Our work provides both a robust evaluation framework
(MedFactEval) and a high-performing generation workflow (MedAgentBrief),
offering a comprehensive approach to advance the responsible deployment of
generative AI in clinical workflows.

</details>


### [29] [Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues](https://arxiv.org/abs/2509.05882)
*Abhijnan Nath,Carine Graff,Nikhil Krishnaswamy*

Main category: cs.CL

TL;DR: 本文研究了不同对齐方法对LLM在多轮多方协作中作为合作伙伴的有效性影响，提出了摩擦代理干预机制和反事实评估框架，发现摩擦感知方法在促进共识达成和任务结果正确性方面显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地作为人类"协作者"融入工作流程，需要确保其在多轮交互中的行为可预测且可靠。现有的对齐技术通常在简化单用户设置下开发，未能考虑长期多方交互的动态特性。

Method: 使用角色扮演方法评估不同训练方式的摩擦代理在协作任务对话中的干预效果，提出新颖的反事实评估框架量化摩擦干预如何改变群体协作轨迹和信念对齐。

Result: 摩擦感知方法在帮助达成共识（共同认可的任务相关命题）和任务结果正确性方面显著优于常见对齐基线方法。

Conclusion: 针对多轮多方协作场景设计的专门对齐方法（如摩擦感知方法）比通用对齐技术更有效，这对于开发可靠的AI协作者具有重要意义。

Abstract: As Large Language Models (LLMs) integrate into diverse workflows, they are
increasingly being considered "collaborators" with humans. If such AI
collaborators are to be reliable, their behavior over multiturn interactions
must be predictable, validated and verified before deployment. Common alignment
techniques are typically developed under simplified single-user settings and do
not account for the dynamics of long-horizon multiparty interactions. This
paper examines how different alignment methods affect LLM agents' effectiveness
as partners in multiturn, multiparty collaborations. We study this question
through the lens of friction agents that intervene in group dialogues to
encourage the collaborative group to slow down and reflect upon their reasoning
for deliberative decision-making. Using a roleplay methodology, we evaluate
interventions from differently-trained friction agents in collaborative task
conversations. We propose a novel counterfactual evaluation framework that
quantifies how friction interventions change the trajectory of group
collaboration and belief alignment. Our results show that a friction-aware
approach significantly outperforms common alignment baselines in helping both
convergence to a common ground, or agreed-upon task-relevant propositions, and
correctness of task outcomes.

</details>


### [30] [Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling](https://arxiv.org/abs/2509.05908)
*Yue Gu,Zhihao Du,Ying Shi,Shiliang Zhang,Qian Chen,Jiqing Han*

Main category: cs.CL

TL;DR: 通过精细化语义相关性聚合模型，在不同细度层面识别最相关的偏置信息，解决了跨注意力模型在长偏置列表下效果下降的问题，显著提升了上下文语音识别的性能。


<details>
  <summary>Details</summary>
Motivation: 跨注意力模型在处理长偏置列表时效果会下降，而实际上只有少量偏置信息与具体识别任务相关，需要精准抓取最相关信息来提升效果。

Method: 提出PSC-Joint方法，从粗细到细粒度定义三种语义相关性（列表级、短语级、标记级），通过聚合模型获得其交集来突出最相关信息，并使用分组竞争筛选机制降低计算成本。

Result: 在AISHELL-1和KeSpeech数据集上，相较基线模型实现了F1分数平均相对提升21.34%和28.46%，适应了不同长度偏置列表。

Conclusion: PSC-Joint方法通过精细化的语义相关性模型，有效解决了偏置信息量变化对跨注意力模型的影响，显著提升了上下文语音识别的性能和稳定性。

Abstract: Recently, cross-attention-based contextual automatic speech recognition (ASR)
models have made notable advancements in recognizing personalized biasing
phrases. However, the effectiveness of cross-attention is affected by
variations in biasing information volume, especially when the length of the
biasing list increases significantly. We find that, regardless of the length of
the biasing list, only a limited amount of biasing information is most relevant
to a specific ASR intermediate representation. Therefore, by identifying and
integrating the most relevant biasing information rather than the entire
biasing list, we can alleviate the effects of variations in biasing information
volume for contextual ASR. To this end, we propose a purified semantic
correlation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and
calculate three semantic correlations between the ASR intermediate
representations and biasing information from coarse to fine: list-level,
phrase-level, and token-level. Then, the three correlations are jointly modeled
to produce their intersection, so that the most relevant biasing information
across various granularities is highlighted and integrated for contextual
recognition. In addition, to reduce the computational cost introduced by the
joint modeling of three semantic correlations, we also propose a purification
mechanism based on a grouped-and-competitive strategy to filter out irrelevant
biasing phrases. Compared with baselines, our PSC-Joint approach achieves
average relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46%
on KeSpeech, across biasing lists of varying lengths.

</details>


### [31] [Accelerating Large Language Model Inference via Early-Exiting Algorithms](https://arxiv.org/abs/2509.05915)
*Sangmin Bae*

Main category: cs.CL

TL;DR: 这篇论文解决了大语言模型中适应性计算方法与系统效率的冲突，通过算法与架构的协同设计，实现了计算效率和模型性能的最优平衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署面临高计算成本问题，而早期退出等适应性计算方法虽能节省计算，但在批处理推理中反而导致系统瓶颈，降低通用性。

Method: 提出高效并行解码机制减少开销；利甦深度参数共享架构减缓同步问题；预训练轻量路由器动态分配最优递归深度。

Result: 建立了效率与性能的新帕索托边界，在单个模型内同时优化了适应性计算和参数效率。

Conclusion: 通过算法与架构的协同设计，成功解决了动态计算与系统效率的冲突，为大语言模型的高效部署提供了可行方案。

Abstract: Large language models have achieved remarkable capabilities, but their
practical deployment is hindered by significant computational costs. While
adaptive computation methods like early-exiting promise to reduce these costs,
they introduce a fundamental conflict: the per-token dynamism intended to save
computation often creates system-level bottlenecks that can paradoxically
reduce throughput in batched inference. This dissertation resolves this
conflict by co-designing adaptive algorithms and model architectures to strike
an optimal balance between dynamism and efficiency. To this end, our work first
addresses critical sources of overhead in conventional early-exiting by
proposing an efficient parallel decoding mechanism. We then show that deep
parameter sharing provides an architectural foundation that not only yields
compact, parameter-efficient models but also inherently mitigates the critical
synchronization issues affecting dynamic inference. Finally, this work presents
a unified framework where lightweight routers are pretrained to dynamically
assign an optimal recursion depth for each token. This approach establishes a
new Pareto frontier between efficiency and performance by effectively
optimizing for both adaptive computation and parameter efficiency within a
single model.

</details>


### [32] [KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino](https://arxiv.org/abs/2509.06065)
*Lorenzo Alfred Nery,Ronald Dawson Catignas,Thomas James Tiam-Lee*

Main category: cs.CL

TL;DR: KatotohananQA是TruthfulQA基准的菲律宾语翻译版本，用于评估LLM在低资源语言中的真实性表现。研究发现英语和菲律宾语之间存在显著性能差距，GPT-5系列模型展现出较强的多语言鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有真实性基准主要针对英语，缺乏对低资源语言的评估，限制了LLM在多语言环境中的可靠应用。

Method: 将TruthfulQA基准翻译成菲律宾语(KatotohananQA)，使用二元选择框架评估7个免费专有模型，比较英语和菲律宾语版本的表现。

Result: 英语和菲律宾语真实性表现存在显著差距；新版OpenAI模型(GPT-5系列)展现更好的多语言鲁棒性；不同问题类型、类别和主题的跨语言迁移鲁棒性存在差异。

Conclusion: 需要更广泛的多语言评估来确保LLM使用的公平性和可靠性，某些问题类型在多语言迁移中表现较弱。

Abstract: Large Language Models (LLMs) achieve remarkable performance across various
tasks, but their tendency to produce hallucinations limits reliable adoption.
Benchmarks such as TruthfulQA have been developed to measure truthfulness, yet
they are primarily available in English, leaving a gap in evaluating LLMs in
low-resource languages. To address this, we present KatotohananQA, a Filipino
translation of the TruthfulQA benchmark. Seven free-tier proprietary models
were assessed using a binary-choice framework. Findings show a significant
performance gap between English and Filipino truthfulness, with newer OpenAI
models (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness.
Results also reveal disparities across question characteristics, suggesting
that some question types, categories, and topics are less robust to
multilingual transfer which highlight the need for broader multilingual
evaluation to ensure fairness and reliability in LLM usage.

</details>


### [33] [Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis](https://arxiv.org/abs/2509.06074)
*Zhenqi Jia,Rui Liu,Berrak Sisman,Haizhou Li*

Main category: cs.CL

TL;DR: 提出了MFCIG-CSS系统，通过构建语义和韵律交互图来建模对话历史中的细粒度词级交互，提升对话语音合成的韵律自然性


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了对话历史中词级别的细粒度语义和韵律交互建模，导致生成的语音韵律不够自然

Method: 构建多模态细粒度对话交互图（语义交互图和韵律交互图），编码词级语义和韵律的交互特征及其对后续话语的影响

Result: 在DailyTalk数据集上的实验表明，MFCIG-CSS在韵律表现力方面优于所有基线模型

Conclusion: MFCIG-CSS通过细粒度的多模态上下文交互建模，有效提升了对话语音合成的韵律自然性

Abstract: Conversational Speech Synthesis (CSS) aims to generate speech with natural
prosody by understanding the multimodal dialogue history (MDH). The latest work
predicts the accurate prosody expression of the target utterance by modeling
the utterance-level interaction characteristics of MDH and the target
utterance. However, MDH contains fine-grained semantic and prosody knowledge at
the word level. Existing methods overlook the fine-grained semantic and
prosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a
novel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our
approach constructs two specialized multimodal fine-grained dialogue
interaction graphs: a semantic interaction graph and a prosody interaction
graph. These two interaction graphs effectively encode interactions between
word-level semantics, prosody, and their influence on subsequent utterances in
MDH. The encoded interaction features are then leveraged to enhance synthesized
speech with natural conversational prosody. Experiments on the DailyTalk
dataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of
prosodic expressiveness. Code and speech samples are available at
https://github.com/AI-S2-Lab/MFCIG-CSS.

</details>


### [34] [Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge](https://arxiv.org/abs/2509.06079)
*Hao Liang,Ruitao Wu,Bohan Zeng,Junbo Niu,Wentao Zhang,Bin Dong*

Main category: cs.CL

TL;DR: 提出了一种caption辅助的多模态推理框架，在ICML 2025 AI for Math Workshop的SeePhys挑战中获得第一名，并在MathVerse几何推理基准上验证了泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决多模态推理的挑战，当前即使是GPT-3等最先进模型在多模态场景中表现不佳，需要有效桥接视觉和文本模态的方法

Method: 采用caption辅助的推理框架，通过生成描述性文本来连接视觉和文本信息，实现多模态推理

Result: 在SeePhys挑战中获得第一名，证明了方法的有效性和鲁棒性；在MathVerse几何推理基准上展示了良好的泛化性能

Conclusion: 该caption辅助推理框架能够有效解决多模态推理问题，具有很好的效果和泛化能力，代码已开源

Abstract: Multimodal reasoning remains a fundamental challenge in artificial
intelligence. Despite substantial advances in text-based reasoning, even
state-of-the-art models such as GPT-o3 struggle to maintain strong performance
in multimodal scenarios. To address this gap, we introduce a caption-assisted
reasoning framework that effectively bridges visual and textual modalities. Our
approach achieved 1st place in the ICML 2025 AI for Math Workshop \& Challenge
2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we
validate its generalization on the MathVerse benchmark for geometric reasoning,
demonstrating the versatility of our method. Our code is publicly available at
https://github.com/OpenDCAI/SciReasoner.

</details>


### [35] [Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models](https://arxiv.org/abs/2509.06100)
*Kefan Cao,Shuaicheng Wu*

Main category: cs.CL

TL;DR: OLieRA是一种新的LLM微调方法，通过引入李群理论和乘法更新来保持参数几何结构，同时在任务子空间应用正交约束，解决了顺序多任务学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有参数正则化方法如O-LoRA和N-LoRA通过强制低秩子空间正交性来减轻任务干扰，但忽略了传统加法微调会破坏LLM参数的内在几何结构，限制了性能表现。

Method: 提出OLieRA方法，将李群理论引入LLM微调：利用乘法更新保持参数几何结构，同时对任务子空间应用正交约束。

Result: 实验表明OLieRA在标准持续学习基准上达到最先进结果，在大量任务设置中保持顶级性能。

Conclusion: 保持参数空间的几何结构对于LLM的顺序多任务学习至关重要，OLieRA通过结合李群理论和正交约束有效解决了灾难性遗忘问题。

Abstract: Large language models (LLMs) are prone to catastrophic forgetting in
sequential multi-task settings. Parameter regularization methods such as O-LoRA
and N-LoRA alleviate task interference by enforcing low-rank subspace
orthogonality, but they overlook the fact that conventional additive
fine-tuning disrupts the intrinsic geometric structure of LLM parameters,
limiting performance. Our key insight is that the parameter space of LLMs
possesses a geometric structure, which must be preserved in addition to
enforcing orthogonality. Based on this, we propose Orthogonal Low-rank
Adaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM
fine-tuning: leveraging multiplicative updates to preserve parameter geometry
while applying orthogonality constraints to task subspaces. Experiments
demonstrate that OLieRA achieves state-of-the-art results on the Standard CL
benchmark and remains among the top-performing methods in the Large Number of
Tasks setting.

</details>


### [36] [Benchmarking Gender and Political Bias in Large Language Models](https://arxiv.org/abs/2509.06164)
*Jinrui Yang,Xudong Han,Timothy Baldwin*

Main category: cs.CL

TL;DR: EuroParlVote是一个新的基准数据集，用于评估LLM在政治敏感语境中的表现，揭示了LLM在性别分类和投票预测任务中存在系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 需要评估大型语言模型在政治敏感语境中的表现，特别是在涉及欧洲议会辩论和投票等政治决策场景中的公平性和偏见问题。

Method: 构建EuroParlVote数据集，包含欧洲议会辩论演讲、投票结果和议员人口统计信息，并在两个任务上评估LLM：性别分类和投票预测。

Result: 发现LLM经常将女性议员误分类为男性，在模拟女性演讲者投票时准确性降低，政治倾向上偏向中间派而低估极左和极右派，GPT-4o等专有模型在鲁棒性和公平性上优于开源模型。

Conclusion: LLM在政治语境中存在显著偏见，需要更多研究来提升NLP在政治场景中的公平性和问责性，EuroParlVote数据集将支持相关研究。

Abstract: We introduce EuroParlVote, a novel benchmark for evaluating large language
models (LLMs) in politically sensitive contexts. It links European Parliament
debate speeches to roll-call vote outcomes and includes rich demographic
metadata for each Member of the European Parliament (MEP), such as gender, age,
country, and political group. Using EuroParlVote, we evaluate state-of-the-art
LLMs on two tasks -- gender classification and vote prediction -- revealing
consistent patterns of bias. We find that LLMs frequently misclassify female
MEPs as male and demonstrate reduced accuracy when simulating votes for female
speakers. Politically, LLMs tend to favor centrist groups while underperforming
on both far-left and far-right ones. Proprietary models like GPT-4o outperform
open-weight alternatives in terms of both robustness and fairness. We release
the EuroParlVote dataset, code, and demo to support future research on fairness
and accountability in NLP within political contexts.

</details>


### [37] [Understanding the Influence of Synthetic Data for Text Embedders](https://arxiv.org/abs/2509.06184)
*Jacob Mitchell Springer,Vaibhav Adlakha,Siva Reddy,Aditi Raghunathan,Marius Mosbach*

Main category: cs.CL

TL;DR: 本研究分析了合成数据在文本嵌入模型中的作用，发现其改进效果有限且存在任务间的性能权衡，挑战了合成数据能构建更鲁棒通用嵌入模型的观念。


<details>
  <summary>Details</summary>
Motivation: 当前通用文本嵌入模型的发展依赖于合成LLM生成数据，但缺乏公开可用的合成数据集阻碍了对其泛化作用的研究。

Method: 首先复现并公开了Wang等人提出的合成数据集，然后系统分析了合成数据在何处以及如何改善模型泛化性能。

Result: 合成数据确实能带来性能提升，但改进效果稀疏且高度局限于特定数据集，同时存在任务间的性能权衡（提升一个任务可能降低另一个任务的性能）。

Conclusion: 当前合成数据方法在构建通用嵌入模型方面存在局限性，训练合成数据并不总能带来跨任务的更鲁棒嵌入模型。

Abstract: Recent progress in developing general purpose text embedders has been driven
by training on ever-growing corpora of synthetic LLM-generated data.
Nonetheless, no publicly available synthetic dataset exists, posing a barrier
to studying its role for generalization. To address this issue, we first
reproduce and publicly release the synthetic data proposed by Wang et al.
(Mistral-E5). Our synthetic data is high quality and leads to consistent
improvements in performance. Next, we critically examine where exactly
synthetic data improves model generalization. Our analysis reveals that
benefits from synthetic data are sparse and highly localized to individual
datasets. Moreover, we observe trade-offs between the performance on different
categories and data that benefits one task, degrades performance on another.
Our findings highlight the limitations of current synthetic data approaches for
building general-purpose embedders and challenge the notion that training on
synthetic data leads to more robust embedding models across tasks.

</details>


### [38] [Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation](https://arxiv.org/abs/2509.06196)
*Mohamed T. Younes,Omar Walid,Khaled Shaban,Ali Hamdi,Mai Hassan*

Main category: cs.CL

TL;DR: 通过细调大语言模型特别是Phi-4模型，使用合成数据集和解析的简历数据进行培训，在招聘自动化任务中实现了显著的性能提升，F1分数达到90.62%。


<details>
  <summary>Details</summary>
Motivation: 解决通用大语言模型在招聘任务中的局限性，提高招聘工作流的准确性和效率。

Method: 创建了使用标准JSON格式的合成数据集，并使用DeepSeek LLM解析简历数据进行训练，对多个大语言模型进行细调。

Result: 细调后的Phi-4模型在准确匹配、F1分数、BLEU、ROUGE等指标上都显著提升，F1分数达到90.62%，显示了在招聘任务中的优异精准度和召回率。

Conclusion: 细调的大语言模型有潜力革命招聘工作流，能够提供更准确的候选人-岗位匹配，为招聘自动化领域带来重大改进。

Abstract: This paper presents a novel approach to recruitment automation. Large
Language Models (LLMs) were fine-tuned to improve accuracy and efficiency.
Building upon our previous work on the Multilayer Large Language Model-Based
Robotic Process Automation Applicant Tracking (MLAR) system . This work
introduces a novel methodology. Training fine-tuned LLMs specifically tuned for
recruitment tasks. The proposed framework addresses the limitations of generic
LLMs by creating a synthetic dataset that uses a standardized JSON format. This
helps ensure consistency and scalability. In addition to the synthetic data
set, the resumes were parsed using DeepSeek, a high-parameter LLM. The resumes
were parsed into the same structured JSON format and placed in the training
set. This will help improve data diversity and realism. Through
experimentation, we demonstrate significant improvements in performance
metrics, such as exact match, F1 score, BLEU score, ROUGE score, and overall
similarity compared to base models and other state-of-the-art LLMs. In
particular, the fine-tuned Phi-4 model achieved the highest F1 score of 90.62%,
indicating exceptional precision and recall in recruitment tasks. This study
highlights the potential of fine-tuned LLMs. Furthermore, it will revolutionize
recruitment workflows by providing more accurate candidate-job matching.

</details>


### [39] [MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment](https://arxiv.org/abs/2509.06200)
*Omar Walid,Mohamed T. Younes,Khaled Shaban,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: MSLEF是一个多段集成框架，通过LLM微调提升简历解析精度，采用加权投票集成多个专业模型，在多项指标上显著优于单模型系统


<details>
  <summary>Details</summary>
Motivation: 解决单模型简历解析系统在处理多样化简历格式和结构时的局限性，提高招聘自动化中简历解析的准确性和适应性

Method: 采用分段感知架构，集成多个微调的LLM（Gemma 9B、LLaMA 3.1 8B、Phi-4 14B），每个模型专门处理特定简历段落，使用加权投票和Gemini-2.5-Flash作为高级聚合器

Result: 在Exact Match、F1分数、BLEU、ROUGE和Recruitment Similarity等指标上取得显著提升，RS指标相比最佳单模型提升高达+7%

Conclusion: MSLEF的分段感知设计增强了跨不同简历布局的泛化能力，使其能够高度适应现实招聘场景，确保精确可靠的候选人信息表示

Abstract: This paper presents MSLEF, a multi-segment ensemble framework that employs
LLM fine-tuning to enhance resume parsing in recruitment automation. It
integrates fine-tuned Large Language Models (LLMs) using weighted voting, with
each model specializing in a specific resume segment to boost accuracy.
Building on MLAR , MSLEF introduces a segment-aware architecture that leverages
field-specific weighting tailored to each resume part, effectively overcoming
the limitations of single-model systems by adapting to diverse formats and
structures. The framework incorporates Gemini-2.5-Flash LLM as a high-level
aggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4
14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score,
BLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best
single model by up to +7% in RS. Its segment-aware design enhances
generalization across varied resume layouts, making it highly adaptable to
real-world hiring scenarios while ensuring precise and reliable candidate
representation.

</details>


### [40] [No Encore: Unlearning as Opt-Out in Music Generation](https://arxiv.org/abs/2509.06277)
*Jinju Kim,Taehan Kim,Abdul Waheed,Rita Singh*

Main category: cs.CL

TL;DR: 本文探索了在文本到音乐生成模型中应用机器遗忘技术，以防止无意中使用受版权保护的内容，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: AI音乐生成系统存在利用受版权创作的风险，引发伦理和法律担忧，需要防止无意中使用创意内容。

Method: 将现有的机器遗忘方法应用于预训练的文本到音乐基线模型，分析其在遗忘预训练数据集方面的有效性。

Result: 提供了在音乐生成中应用遗忘技术的挑战性见解，为未来研究奠定了基础分析。

Conclusion: 机器遗忘技术在音乐生成模型中具有应用潜力，但面临挑战，需要进一步研究来完善这一技术。

Abstract: AI music generation is rapidly emerging in the creative industries, enabling
intuitive music generation from textual descriptions. However, these systems
pose risks in exploitation of copyrighted creations, raising ethical and legal
concerns. In this paper, we present preliminary results on the first
application of machine unlearning techniques from an ongoing research to
prevent inadvertent usage of creative content. Particularly, we explore
existing methods in machine unlearning to a pre-trained Text-to-Music (TTM)
baseline and analyze their efficacy in unlearning pre-trained datasets without
harming model performance. Through our experiments, we provide insights into
the challenges of applying unlearning in music generation, offering a
foundational analysis for future works on the application of unlearning for
music generative models.

</details>


### [41] [Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?](https://arxiv.org/abs/2509.06350)
*Junjie Mu,Zonghao Ying,Zhekui Fan,Zonglei Jing,Yaoyuan Zhang,Zhengmin Yu,Wenxin Zhang,Quanchen Zou,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: Mask-GCG是一种改进的越狱攻击方法，通过可学习的token掩码识别后缀中关键token，修剪低影响力token来减少冗余和计算开销，同时保持攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的GCG越狱攻击方法使用固定长度的后缀，但其中可能存在token冗余问题未被探索，需要开发更高效的方法来减少计算开销并提高攻击效率。

Method: 提出Mask-GCG方法，使用可学习的token掩码机制识别后缀中高影响力token位置，增加这些位置的更新概率，同时修剪低影响力token，从而减少梯度空间大小和计算负担。

Result: 实验表明后缀中大部分token对攻击成功很重要，但修剪少数低影响力token不会影响损失值或攻击成功率，揭示了LLM提示中的token冗余现象。

Conclusion: 该方法不仅提高了越狱攻击的效率，还为从越狱攻击角度开发高效和可解释的LLM提供了见解。

Abstract: Jailbreak attacks on Large Language Models (LLMs) have demonstrated various
successful methods whereby attackers manipulate models into generating harmful
responses that they are designed to avoid. Among these, Greedy Coordinate
Gradient (GCG) has emerged as a general and effective approach that optimizes
the tokens in a suffix to generate jailbreakable prompts. While several
improved variants of GCG have been proposed, they all rely on fixed-length
suffixes. However, the potential redundancy within these suffixes remains
unexplored. In this work, we propose Mask-GCG, a plug-and-play method that
employs learnable token masking to identify impactful tokens within the suffix.
Our approach increases the update probability for tokens at high-impact
positions while pruning those at low-impact positions. This pruning not only
reduces redundancy but also decreases the size of the gradient space, thereby
lowering computational overhead and shortening the time required to achieve
successful attacks compared to GCG. We evaluate Mask-GCG by applying it to the
original GCG and several improved variants. Experimental results show that most
tokens in the suffix contribute significantly to attack success, and pruning a
minority of low-impact tokens does not affect the loss values or compromise the
attack success rate (ASR), thereby revealing token redundancy in LLM prompts.
Our findings provide insights for developing efficient and interpretable LLMs
from the perspective of jailbreak attacks.

</details>


### [42] [PL-CA: A Parametric Legal Case Augmentation Framework](https://arxiv.org/abs/2509.06356)
*Ao Chang,Yubo Chen,Jun Zhao*

Main category: cs.CL

TL;DR: PL-CA提出参数化RAG框架，将法律知识编码到参数向量中并通过LoRA集成到LLM前馈网络，缓解上下文压力，同时构建了专家标注的多任务法律数据集。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法直接将检索文档注入模型上下文，受限于有限上下文窗口，引入额外计算开销并干扰模型注意力，且现有基准缺乏专家标注和真实多任务场景。

Method: 提出参数化RAG(P-RAG)框架，对语料知识进行数据增强并编码为参数向量，通过LoRA将参数化知识集成到LLM的前馈网络中。

Result: 实验结果表明该方法减少了长上下文的开销，在下游任务上保持与传统RAG相当的竞争性能。

Conclusion: PL-CA方法有效缓解了模型上下文压力，同时维持了任务性能，为法律领域RAG应用提供了新思路。

Abstract: Conventional RAG is considered one of the most effective methods for
addressing model knowledge insufficiency and hallucination, particularly in the
judicial domain that requires high levels of knowledge rigor, logical
consistency, and content integrity. However, the conventional RAG method only
injects retrieved documents directly into the model's context, which severely
constrains models due to their limited context windows and introduces
additional computational overhead through excessively long contexts, thereby
disrupting models' attention and degrading performance on downstream tasks.
Moreover, many existing benchmarks lack expert annotation and focus solely on
individual downstream tasks while real-world legal scenarios consist of
multiple mixed legal tasks, indicating conventional benchmarks' inadequacy for
reflecting models' true capabilities. To address these limitations, we propose
PL-CA, which introduces a parametric RAG (P-RAG) framework to perform data
augmentation on corpus knowledge and encode this legal knowledge into
parametric vectors, and then integrates this parametric knowledge into the
LLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context
pressure. Additionally, we also construct a multi-task legal dataset comprising
more than 2000 training and test instances, which are all expert-annotated and
manually verified. We conduct our experiments on our dataset, and the
experimental results demonstrate that our method reduces the overhead
associated with excessively long contexts while maintaining competitive
performance on downstream tasks compared to conventional RAG. Our code and
dataset are provided in the appendix.

</details>


### [43] [Do LLMs exhibit the same commonsense capabilities across languages?](https://arxiv.org/abs/2509.06401)
*Ivan Martínez-Murillo,Elena Lloret,Paloma Moreda,Albert Gatt*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型的多语言常识生成能力，通过MULTICOM基准测试评估了多种开源LLM在英语、西班牙语、荷兰语和瓦伦西亚语上的表现，发现英语表现最佳，资源较少语言表现较差。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在多语言环境下的常识生成能力，特别是针对资源较少语言的表现，以了解当前LLM在多语言常识推理方面的局限性。

Method: 构建MULTICOM基准测试（扩展自COCOTEROS数据集，涵盖4种语言），评估多个开源LLM（LLaMA、Qwen、Gemma等），采用自动指标、LLM作为评判器（Prometheus、JudgeLM）和人工标注相结合的评价方法。

Result: 英语表现最佳，资源较少语言（西班牙语、荷兰语、瓦伦西亚语）表现显著较差；上下文支持对低资源语言有一定帮助但效果不一。

Conclusion: 当前LLM在多语言常识生成方面存在明显局限性，特别是在资源较少语言上表现不佳，需要进一步改进多语言能力。

Abstract: This paper explores the multilingual commonsense generation abilities of
Large Language Models (LLMs). To facilitate this investigation, we introduce
MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four
languages: English, Spanish, Dutch, and Valencian. The task involves generating
a commonsensical sentence that includes a given triplet of words. We evaluate a
range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and
Salamandra, on this benchmark. Our evaluation combines automatic metrics,
LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human
annotations. Results consistently show superior performance in English, with
significantly lower performance in less-resourced languages. While contextual
support yields mixed results, it tends to benefit underrepresented languages.
These findings underscore the current limitations of LLMs in multilingual
commonsense generation. The dataset is publicly available at
https://huggingface.co/datasets/gplsi/MULTICOM.

</details>


### [44] [WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents](https://arxiv.org/abs/2509.06501)
*Junteng Liu,Yunji Li,Chi Zhang,Jingyang Li,Aili Chen,Ke Ji,Weiyu Cheng,Zijia Wu,Chengyu Du,Qidi Xu,Jiayuan Song,Zhengmao Zhu,Wenhu Chen,Pengyu Zhao,Junxian He*

Main category: cs.CL

TL;DR: WebExplorer-8B是一个8B参数的开源网页代理模型，通过创新的数据生成方法和强化学习训练，在复杂信息搜索任务中达到了最先进的性能，甚至超越了更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的开源网页代理要么在复杂任务上信息搜索能力有限，要么缺乏透明实现，主要挑战在于缺乏具有挑战性的信息搜索数据。

Method: 提出WebExplorer系统化数据生成方法，使用基于模型的探索和迭代式长到短查询演化，创建需要多步推理和复杂网页导航的查询-答案对。通过监督微调后接强化学习训练得到WebExplorer-8B模型。

Result: WebExplorer-8B在多个信息搜索基准测试中达到同规模最佳性能，支持128K上下文长度和最多100次工具调用，在BrowseComp-en/zh上超越WebSailor-72B，在WebWalkerQA和FRAMES上达到100B参数以下模型最佳性能，在HLE基准上也有强泛化能力。

Conclusion: 该方法为长视野网页代理的发展提供了实用路径，证明了通过高质量数据生成和强化学习可以训练出高效的小规模网页代理模型。

Abstract: The paradigm of Large Language Models (LLMs) has increasingly shifted toward
agentic applications, where web browsing capabilities are fundamental for
retrieving information from diverse online sources. However, existing
open-source web agents either demonstrate limited information-seeking abilities
on complex tasks or lack transparent implementations. In this work, we identify
that the key challenge lies in the scarcity of challenging data for information
seeking. To address this limitation, we introduce WebExplorer: a systematic
data generation approach using model-based exploration and iterative,
long-to-short query evolution. This method creates challenging query-answer
pairs that require multi-step reasoning and complex web navigation. By
leveraging our curated high-quality dataset, we successfully develop advanced
web agent WebExplorer-8B through supervised fine-tuning followed by
reinforcement learning. Our model supports 128K context length and up to 100
tool calling turns, enabling long-horizon problem solving. Across diverse
information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art
performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able
to effectively search over an average of 16 turns after RL training, achieving
higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best
performance among models up to 100B parameters on WebWalkerQA and FRAMES.
Beyond these information-seeking tasks, our model also achieves strong
generalization on the HLE benchmark even though it is only trained on
knowledge-intensive QA data. These results highlight our approach as a
practical path toward long-horizon web agents.

</details>


### [45] [Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training](https://arxiv.org/abs/2509.06518)
*Andrei Baroian,Kasper Notebomer*

Main category: cs.CL

TL;DR: 本文提出了三种新的层间缩放变体（Framed、Reverse、Crown），通过重新分配FFN宽度和注意力头来改进传统各向同性的Transformer架构，在固定参数预算下实现了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer使用均匀的层尺寸，忽略了不同深度层的功能多样性和计算需求差异，需要探索更有效的层间架构设计。

Method: 基于层间缩放和剪枝技术，提出了三种LWS变体：Framed、Reverse和Crown，使用两或三点线性插值在预训练阶段重新分配FFN宽度和注意力头。

Result: 在180M参数和5B tokens的训练条件下，所有模型都收敛到相似的损失，相比同等成本各向同性基线获得更好性能，且训练吞吐量没有显著下降。

Conclusion: 这是预训练层间架构设计空间的初步探索，未来需要在更大规模的tokens和参数上进行实验以充分评估其潜力。

Abstract: Transformer-based language models traditionally use uniform (isotropic) layer
sizes, yet they ignore the diverse functional roles that different depths can
play and their computational capacity needs. Building on Layer-Wise Scaling
(LWS) and pruning literature, we introduce three new LWS variants - Framed,
Reverse, and Crown - that redistribute FFN widths and attention heads via two
or three-point linear interpolation in the pre-training stage. We present the
first systematic ablation of LWS and its variants, on a fixed budget of 180M
parameters, trained on 5B tokens. All models converge to similar losses and
achieve better performance compared to an equal-cost isotropic baseline,
without a substantial decrease in training throughput. This work represents an
initial step into the design space of layer-wise architectures for
pre-training, but future work should scale experiments to orders of magnitude
more tokens and parameters to fully assess their potential.

</details>


### [46] [LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection](https://arxiv.org/abs/2509.06524)
*Jian Wu,Hang Yu,Bingchang Liu,Wenjie Yang,Peng Di,Jianguo Li,Yue Zhang*

Main category: cs.CL

TL;DR: LAMDAS是一种新颖的数据选择方法，利用预训练LLM作为隐式分类器，将数据选择重构为单类分类问题，仅需少量参考数据就能高效筛选高质量领域数据。


<details>
  <summary>Details</summary>
Motivation: 领域适配LLM面临高质量人工标注数据稀缺的瓶颈，大量未检查数据直接用于微调会引入噪声降低性能，需要准确高效的数据选择方法。

Method: 利用预训练LLM本身作为隐式分类器，避免显式特征工程和计算密集型优化过程，将数据选择重构为单类分类问题，基于小规模参考数据集识别属于目标领域的候选数据。

Result: 实验表明LAMDAS不仅能用少量数据超越全数据训练性能，还在多种场景下优于9个SOTA基线方法，在性能提升和计算效率之间达到最佳平衡。

Conclusion: LAMDAS提供了一种高效准确的领域数据选择解决方案，解决了现有方法无法同时兼顾准确性和效率的问题，为LLM领域适配提供了实用工具。

Abstract: Adapting large language models (LLMs) to specific domains often faces a
critical bottleneck: the scarcity of high-quality, human-curated data. While
large volumes of unchecked data are readily available, indiscriminately using
them for fine-tuning risks introducing noise and degrading performance.
Strategic data selection is thus crucial, requiring a method that is both
accurate and efficient. Existing approaches, categorized as similarity-based
and direct optimization methods, struggle to simultaneously achieve these
goals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for
domain-specific DAta Selection), a novel approach that leverages the
pre-trained LLM itself as an implicit classifier, thereby bypassing explicit
feature engineering and computationally intensive optimization process. LAMDAS
reframes data selection as a one-class classification problem, identifying
candidate data that "belongs" to the target domain defined by a small reference
dataset. Extensive experimental results demonstrate that LAMDAS not only
exceeds the performance of full-data training using a fraction of the data but
also outperforms nine state-of-the-art (SOTA) baselines under various
scenarios. Furthermore, LAMDAS achieves the most compelling balance between
performance gains and computational efficiency compared to all evaluated
baselines.

</details>


### [47] [SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion](https://arxiv.org/abs/2509.06531)
*Mengxue Yang,Chun Yang,Jiaqi Zhu,Jiafan Li,Jingqi Zhang,Yuyang Li,Ying Li*

Main category: cs.CL

TL;DR: SLiNT是一个结构感知的语言模型框架，通过注入知识图谱结构信息和对比学习，在冻结的LLM骨干网络上实现鲁棒的链接预测


<details>
  <summary>Details</summary>
Motivation: 解决知识图谱链接预测中LLM对结构信号利用不足导致的结构稀疏性和语义模糊问题，特别是在不完整或零样本设置下

Method: 采用模块化框架：1) SGNE检索伪邻居丰富稀疏实体；2) DHCL通过硬正负样本插值进行细粒度监督；3) GDDI进行token级结构感知干预同时保持LLM核心参数

Result: 在WN18RR和FB15k-237数据集上达到优于或竞争性的性能，相比基于嵌入和生成的基线方法

Conclusion: 证明了结构感知表示学习对于可扩展知识图谱补全的有效性

Abstract: Link prediction in knowledge graphs requires integrating structural
information and semantic context to infer missing entities. While large
language models offer strong generative reasoning capabilities, their limited
exploitation of structural signals often results in structural sparsity and
semantic ambiguity, especially under incomplete or zero-shot settings. To
address these challenges, we propose SLiNT (Structure-aware Language model with
Injection and coNtrastive Training), a modular framework that injects
knowledge-graph-derived structural context into a frozen LLM backbone with
lightweight LoRA-based adaptation for robust link prediction. Specifically,
Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to
enrich sparse entities and mitigate missing context; Dynamic Hard Contrastive
Learning (DHCL) introduces fine-grained supervision by interpolating hard
positives and negatives to resolve entity-level ambiguity; and
Gradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware
intervention while preserving the core LLM parameters. Experiments on WN18RR
and FB15k-237 show that SLiNT achieves superior or competitive performance
compared with both embedding-based and generation-based baselines,
demonstrating the effectiveness of structure-aware representation learning for
scalable knowledge graph completion.

</details>


### [48] [HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models](https://arxiv.org/abs/2509.06596)
*Xin Tong,Zhi Lin,Jingya Wang,Bo Jin*

Main category: cs.CL

TL;DR: HAVE是一个无需微调的参数自由解码框架，通过头部自适应门控和价值校准来解决LLM幻觉问题，在多个QA基准测试中有效减少幻觉并优于现有方法


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在检索增强生成长上下文时即使有相关证据仍会产生幻觉，主要原因是头部重要性处理与输入无关，以及原始注意力权重不能准确反映每个token的真实贡献

Method: 提出HAVE框架：1）头部自适应门控 - 实例级软重加权注意力头；2）价值校准 - 通过价值向量幅度增强注意力来近似回写贡献；3）轻量级不确定性缩放策略融合证据与语言模型分布

Result: 在多个QA基准测试和不同LLM家族上的实验表明，HAVE能持续减少幻觉，优于包括DAGCD在内的强基线方法，且开销适中

Conclusion: HAVE框架透明、可复现，可与现成LLM无缝集成，无需微调且单次前向传播即可工作，推动了实际场景中的可信生成

Abstract: Large Language Models (LLMs) often produce hallucinations in
retrieval-augmented or long-context generation, even when relevant evidence is
present. This stems from two issues: head importance is treated as
input-agnostic, and raw attention weights poorly reflect each token's true
contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a
parameter-free decoding framework that directly addresses both challenges. HAVE
introduces head-adaptive gating, which performs instance-level soft reweighing
of attention heads, and value calibration, which augments attention with the
magnitude of value vectors to approximate write-back contribution. Together,
these modules construct token-level evidence aligned with model updates and
fuse it with the LM distribution through a lightweight uncertainty-scaled
policy. HAVE requires no finetuning and operates in a single forward pass,
making it efficient and broadly applicable. Experiments across multiple QA
benchmarks and LLM families demonstrate that HAVE consistently reduces
hallucinations and outperforms strong baselines, including DAGCD, with modest
overhead. The framework is transparent, reproducible, and readily integrates
with off-the-shelf LLMs, advancing trustworthy generation in real-world
settings.

</details>


### [49] [Guided Decoding and Its Critical Role in Retrieval-Augmented Generation](https://arxiv.org/abs/2509.06631)
*Özgür Uğur,Musa Yılmaz,Esra Şavirdi,Özay Ezerceli,Mahmut El Huseyni,Selva Taş,Reyhan Bayraktar*

Main category: cs.CL

TL;DR: 本研究比较了三种引导解码方法（Outlines、XGrammar、LM Format Enforcer）在RAG系统中的性能，分析了不同多轮提示设置下的成功率和幻觉率，为结构化输出生成提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各类应用中的集成，需要确保输出符合预期格式并减少幻觉，特别是在检索增强生成（RAG）系统中保证结构化可靠响应的需求日益重要。

Method: 研究比较了三种引导解码方法（Outlines、XGrammar、LM Format Enforcer），在不同多轮提示设置（0轮、1轮、2轮）下进行评估，通过成功率、幻觉率和输出质量等指标进行分析。

Result: 研究发现多轮交互对引导解码性能有显著影响，揭示了意外的性能变化模式，这些发现可以为特定用例的方法选择提供参考。

Conclusion: 该研究推进了对RAG系统中结构化输出生成的理解，为大语言模型部署提供了理论见解和实践指导，揭示了多轮交互在引导解码中的重要作用。

Abstract: The integration of Large Language Models (LLMs) into various applications has
driven the need for structured and reliable responses. A key challenge in
Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align
with expected formats while minimizing hallucinations. This study examines the
role of guided decoding in RAG systems, comparing three methods, Outlines,
XGrammar, and LM Format Enforcer, across different multi-turn prompting setups
(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,
and output quality, we provide insights into their performance and
applicability. Our findings reveal how multi-turn interactions influence guided
decoding, uncovering unexpected performance variations that can inform method
selection for specific use cases. This work advances the understanding of
structured output generation in RAG systems, offering both theoretical insights
and practical guidance for LLM deployment.

</details>


### [50] [Modelling Intertextuality with N-gram Embeddings](https://arxiv.org/abs/2509.06637)
*Yi Xing*

Main category: cs.CL

TL;DR: 提出了一种基于n-gram嵌入的量化互文性分析模型，通过文本对比较和网络分析来量化和可视化文学文本间的互文关系


<details>
  <summary>Details</summary>
Motivation: 互文性是文学研究中的核心概念，但传统分析方法难以进行大规模量化分析，需要开发可扩展的计算方法来捕捉和量化文本间的互文关系

Method: 使用n-gram嵌入进行文本对比较，计算嵌入向量的相似度并取平均值作为整体互文性指标，结合网络分析揭示中心性和社区结构

Result: 在4个已知互文性程度的文本上验证有效，并在267个多样化文本上测试了可扩展性，网络分析成功揭示了中心性和社区结构

Conclusion: 该方法能够有效捕捉和量化互文关系，为大规模文学文本分析提供了高效的计算框架

Abstract: Intertextuality is a central tenet in literary studies. It refers to the
intricate links between literary texts that are created by various types of
references. This paper proposes a new quantitative model of intertextuality to
enable scalable analysis and network-based insights: perform pairwise
comparisons of the embeddings of n-grams from two texts and average their
results as the overall intertextuality. Validation on four texts with known
degrees of intertextuality, alongside a scalability test on 267 diverse texts,
demonstrates the method's effectiveness and efficiency. Network analysis
further reveals centrality and community structures, affirming the approach's
success in capturing and quantifying intertextual relationships.

</details>


### [51] [Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval](https://arxiv.org/abs/2509.06650)
*Hao Lin,Peitong Xie,Jingxue Chen,Jie Lin,Qingkun Tang,Qianchun Lu*

Main category: cs.CL

TL;DR: MoLER是一个基于混合损失增强强化学习的领域感知RAG方法，通过两阶段训练优化检索性能，在基准数据集上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统在粗排序优化中难以平衡领域特定知识学习和查询增强，导致检索性能不佳。

Method: 采用两阶段流程：1）使用混合损失(MoL)进行持续预训练，平衡领域知识和通用语言能力；2）使用组相对策略优化(GRPO)进行强化学习，优化查询和段落生成以最大化文档召回率。创新性地提出了多查询单段落后融合(MSLF)策略减少计算开销。

Result: 在基准数据集上的大量实验表明，MoLER实现了最先进的性能，显著优于基线方法。

Conclusion: MoLER填补了RAG系统中的知识空白，能够在专业领域中实现鲁棒且可扩展的检索。

Abstract: Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval
stage, particularly the coarse-ranking process. Existing coarse-ranking
optimization approaches often struggle to balance domain-specific knowledge
learning with query enhencement, resulting in suboptimal retrieval performance.
To address this challenge, we propose MoLER, a domain-aware RAG method that
uses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a
two-stage pipeline: a continual pre-training (CPT) phase using a Mixture of
Losses (MoL) to balance domain-specific knowledge with general language
capabilities, and a reinforcement learning (RL) phase leveraging Group Relative
Policy Optimization (GRPO) to optimize query and passage generation for
maximizing document recall. A key innovation is our Multi-query Single-passage
Late Fusion (MSLF) strategy, which reduces computational overhead during RL
training while maintaining scalable inference via Multi-query Multi-passage
Late Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER
achieves state-of-the-art performance, significantly outperforming baseline
methods. MoLER bridges the knowledge gap in RAG systems, enabling robust and
scalable retrieval in specialized domains.

</details>


### [52] [IntrEx: A Dataset for Modeling Engagement in Educational Conversations](https://arxiv.org/abs/2509.06652)
*Xingwei Tan,Mahathi Parvatham,Chiara Gambi,Gabriele Pergola*

Main category: cs.CL

TL;DR: IntrEx是首个针对教师-学生互动中趣味性和预期趣味性的大型标注数据集，用于研究教育对话中的参与度，发现微调后的LLM在预测趣味性方面优于GPT-4o等大型模型。


<details>
  <summary>Details</summary>
Motivation: 二语习得中参与度和动机至关重要，但维持学习者在教育对话中的兴趣仍具挑战性。现有研究主要关注教育文本的趣味性，而对对话中驱动参与度的语言特征了解甚少。

Method: 基于教师-学生聊天室语料库(TSCC)构建IntrEx数据集，采用序列级标注；使用基于比较的评分方法，借鉴人类反馈强化学习(RLHF)提高标注一致性；让100多名二语学习者参与标注；研究LLM预测人类趣味性判断的能力。

Result: 微调后的LLM(7B/8B参数)在预测趣味性评分方面优于GPT-4o等大型专有模型；分析了具体性、可理解性(可读性)和吸收度等语言和认知因素对教育对话参与度的影响。

Conclusion: 专业化数据集在教育环境中建模参与度具有潜力；语言和认知因素对教育对话的趣味性有重要影响；序列级标注能够捕捉兴趣在扩展对话中的演变过程。

Abstract: Engagement and motivation are crucial for second-language acquisition, yet
maintaining learner interest in educational conversations remains a challenge.
While prior research has explored what makes educational texts interesting,
still little is known about the linguistic features that drive engagement in
conversations. To address this gap, we introduce IntrEx, the first large
dataset annotated for interestingness and expected interestingness in
teacher-student interactions. Built upon the Teacher-Student Chatroom Corpus
(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,
allowing for the study of engagement beyond isolated turns to capture how
interest evolves over extended dialogues. We employ a rigorous annotation
process with over 100 second-language learners, using a comparison-based rating
approach inspired by reinforcement learning from human feedback (RLHF) to
improve agreement. We investigate whether large language models (LLMs) can
predict human interestingness judgments. We find that LLMs (7B/8B parameters)
fine-tuned on interestingness ratings outperform larger proprietary models like
GPT-4o, demonstrating the potential for specialised datasets to model
engagement in educational settings. Finally, we analyze how linguistic and
cognitive factors, such as concreteness, comprehensibility (readability), and
uptake, influence engagement in educational dialogues.

</details>


### [53] [ParCzech4Speech: A New Speech Corpus Derived from Czech Parliamentary Data](https://arxiv.org/abs/2509.06675)
*Vladislav Stankov,Matyáš Kopp,Ondřej Bojar*

Main category: cs.CL

TL;DR: ParCzech4Speech 1.0是基于ParCzech 4.0语料库处理后的版本，专门用于语音建模任务，最大变体包含2,695小时的捷克议会演讲音频和官方转录文本对齐数据。


<details>
  <summary>Details</summary>
Motivation: 为语音建模任务提供高质量的捷克语语音-文本对齐数据，改进ParCzech 3.0版本的数据质量和可靠性。

Method: 使用WhisperX和Wav2Vec 2.0对捷克议会演讲录音进行自动化音频-文本对齐处理，提取更高质量的对齐数据。

Result: 生成了三个灵活的数据变体：句子分割版本（用于ASR和语音合成）、未分割版本（保持原始语句流）和原始对齐版本（用于自定义精炼）。

Conclusion: 该数据集在LINDAT和Hugging Face平台发布，采用CC-BY许可，为捷克语语音处理研究提供了重要的资源支持。

Abstract: We introduce ParCzech4Speech 1.0, a processed version of the ParCzech 4.0
corpus, targeted at speech modeling tasks with the largest variant containing
2,695 hours. We combined the sound recordings of the Czech parliamentary
speeches with the official transcripts. The recordings were processed with
WhisperX and Wav2Vec 2.0 to extract automated audio-text alignment. Our
processing pipeline improves upon the ParCzech 3.0 speech recognition version
by extracting more data with higher alignment reliability. The dataset is
offered in three flexible variants: (1) sentence-segmented for automatic speech
recognition and speech synthesis tasks with clean boundaries, (2) unsegmented
preserving original utterance flow across sentences, and (3) a raw-alignment
for further custom refinement for other possible tasks. All variants maintain
the original metadata and are released under a permissive CC-BY license. The
dataset is available in the LINDAT repository, with the sentence-segmented and
unsegmented variants additionally available on Hugging Face.

</details>


### [54] [Will Annotators Disagree? Identifying Subjectivity in Value-Laden Arguments](https://arxiv.org/abs/2509.06704)
*Amir Homayounirad,Enrico Liscio,Tong Wang,Catholijn M. Jonker,Luciano C. Siebert*

Main category: cs.CL

TL;DR: 这篇论文探索了识别论证中主观性的方法，发现直接主观性识别方法显著提升了模型性能，而对比损失与二元交叉碑损失组合并未改善效果。


<details>
  <summary>Details</summary>
Motivation: 聚合多个标注为单一真实标签可能隐藏了标注者分歧的价值，特别是在主观性起关键作用的任务中。需要探索识别论证中人类价值观引发主观性的方法。

Method: 评估了两种主要方法：通过价值预测推断主观性与直接识别主观性。实验包括结合对比损失和二元交叉碑损失的方法。

Result: 直接主观性识别方法显著提升了标记主观论证的模型性能。对比损失与二元交叉碑损失组合并未改善性能，但减少了对每个标签主观性的依赖性。

Conclusion: 提出的方法能够帮助识别个人可能有不同解释的论证，促进更细致的标注过程，揭示了标注中被隐藏的主观性特征。

Abstract: Aggregating multiple annotations into a single ground truth label may hide
valuable insights into annotator disagreement, particularly in tasks where
subjectivity plays a crucial role. In this work, we explore methods for
identifying subjectivity in recognizing the human values that motivate
arguments. We evaluate two main approaches: inferring subjectivity through
value prediction vs. directly identifying subjectivity. Our experiments show
that direct subjectivity identification significantly improves the model
performance of flagging subjective arguments. Furthermore, combining
contrastive loss with binary cross-entropy loss does not improve performance
but reduces the dependency on per-label subjectivity. Our proposed methods can
help identify arguments that individuals may interpret differently, fostering a
more nuanced annotation process.

</details>


### [55] [Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint](https://arxiv.org/abs/2509.06795)
*Yanrui Du,Fenglei Fan,Sendong Zhao,Jiawei Cao,Qika Lin,Kai He,Ting Liu,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: IFT导致LLM安全性下降，研究发现拒绝方向偏移是关键原因。提出ProCon方法通过投影约束捕捉和温度策略，有效稳定拒绝方向并保持任务性能。


<details>
  <summary>Details</summary>
Motivation: IFT虽能增强LLM的能力，但严重損害其安全性，特别是对恶意指令的拒绝能力。研究发现隐藏状态中的"拒绝方向"在训练中偏移，是导致安全风险的关键原因。

Method: 提出ProCon方法：1)使用投影约束损失项来规范化隐藏状态在拒绝方向上的投影幅度；2)采用温度策略，在训练初期强约束后逐渐放宽；3)扩宽数据分布以强化约束信号。

Result: 在多种数据集、场景和LLM上的实验结果显示，ProCon能显著减少IFT带来的安全风险，同时保持任务性能提升。与强基准比较，方法一贯表现出更优秀的整体性能。

Conclusion: ProCon方法能够有效稳定训练过程中的拒绝方向，在保障安全性的同时维持任务性能。这种基于可解释性的LLM内部机制探索为未来安全研究奠定了基础。

Abstract: Instruction Fine-Tuning (IFT) has been widely adopted as an effective
post-training strategy to enhance various abilities of Large Language Models
(LLMs). However, prior studies have shown that IFT can significantly compromise
LLMs' safety, particularly their ability to refuse malicious instructions,
raising significant concerns. Recent research into the internal mechanisms of
LLMs has identified the refusal direction (r-direction) in the hidden states,
which plays a pivotal role in governing refusal behavior. Building on this
insight, our study reveals that the r-direction tends to drift during training,
which we identify as one of the causes of the associated safety risks. To
mitigate such drift, our proposed ProCon method introduces a
projection-constrained loss term that regularizes the projection magnitude of
each training sample's hidden state onto the r-direction. Our initial analysis
shows that applying an appropriate constraint can effectively mitigate the
refusal direction drift and associated safety risks, but remains limited by
overall performance barriers. To overcome this barrier, informed by our
observation of early-stage sharp drift and a data-driven perspective, we
introduce a warm-up strategy that emphasizes early-stage strong constraints and
broaden the data distribution to strengthen constraint signals, leading to an
enhanced ProCon method. Experimental results under various datasets, scenarios,
and LLMs demonstrate that our method can significantly mitigate safety risks
posed by IFT while preserving task performance gains. Even compared with strong
baselines, our method consistently delivers superior overall performance.
Crucially, our analysis indicates that ProCon can contribute to stabilizing the
r-direction during training, while such an interpretability-driven exploration
of LLMs' internal mechanisms lays a solid foundation for future safety
research.

</details>


### [56] [MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML](https://arxiv.org/abs/2509.06806)
*Haoyu Dong,Pengkun Zhang,Mingzhe Lu,Yanzhen Shen,Guolin Ke*

Main category: cs.CL

TL;DR: MachineLearningLM是一个持续预训练框架，通过在数百万个结构因果模型上合成ML任务进行训练，使通用LLM获得强大的上下文机器学习能力，同时保持其通用知识和推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然具有广泛的世界知识和强大的通用推理能力，但在标准机器学习任务中难以通过纯上下文学习从大量示例中学习，需要提升其上下文ML能力。

Method: 使用随机森林教师模型，将基于树的决策策略蒸馏到LLM中，增强数值建模的鲁棒性。通过高效的提示序列化，实现3-6倍的上下文窗口示例数量和50倍的批处理吞吐量。

Result: 在Qwen-2.5-7B-Instruct模型上，MachineLearningLM在金融、物理、生物和医疗等领域的表格分类任务中平均比GPT-5-mini等强基线模型提升约15%，表现出显著的多数示例缩放规律，准确率随示例数量从8增加到1024而单调提升。

Conclusion: 该框架成功地为通用LLM赋予了强大的上下文机器学习能力，在保持通用聊天能力（MMLU得分75.4%）的同时，实现了随机森林级别的准确率，为LLM在机器学习任务中的应用提供了有效解决方案。

Abstract: Large language models (LLMs) possess broad world knowledge and strong
general-purpose reasoning ability, yet they struggle to learn from many
in-context examples on standard machine learning (ML) tasks, that is, to
leverage many-shot demonstrations purely via in-context learning (ICL) without
gradient descent. We introduce MachineLearningLM, a portable
continued-pretraining framework that equips a general-purpose LLM with robust
in-context ML capability while preserving its general knowledge and reasoning
for broader chat workflows.
  Our pretraining procedure synthesizes ML tasks from millions of structural
causal models (SCMs), spanning shot counts up to 1,024. We begin with a
random-forest teacher, distilling tree-based decision strategies into the LLM
to strengthen robustness in numerical modeling. All tasks are serialized with a
token-efficient prompt, enabling 3x to 6x more examples per context window and
delivering up to 50x amortized throughput via batch inference.
  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),
MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an
average of about 15% on out-of-distribution tabular classification across
finance, physics, biology, and healthcare domains. It exhibits a striking
many-shot scaling law: accuracy increases monotonically as in-context
demonstrations grow from 8 to 1,024. Without any task-specific training, it
attains random-forest-level accuracy across hundreds of shots. General chat
capabilities, including knowledge and reasoning, are preserved: it achieves
75.4% on MMLU.

</details>


### [57] [MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security](https://arxiv.org/abs/2509.06807)
*Yanrui Du,Fenglei Fan,Sendong Zhao,Jiawei Cao,Ting Liu,Bing Qin*

Main category: cs.CL

TL;DR: MoGU_v2框架通过动态路由机制平衡LLM的安全性和可用性，在多个LLM系列中实现稳定改进，无需在安全性和实用性之间进行权衡。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全方法往往导致过度保守的拒绝响应，损害了实际可用性。需要找到一种方法能够同时提升安全性和可用性，而不是在两者之间进行权衡。

Method: 提出MoGU_v2框架：1）仅在编码高分类性安全特征的层中嵌入路由器；2）在路由器优化期间激活骨干模块实现双向适配；3）使用数据混合策略应对指令微调带来的风险

Result: MoGU_v2在各种LLM系列中表现出强大的适应性和稳定改进，包括主流LLM、设备端LLM和推理LLM。能够轻松恢复安全性而不影响任务性能增益

Conclusion: MoGU_v2是一个强大且多功能的解决方案，能够有效缓解现实应用中的安全风险，在安全性和可用性之间实现了更好的平衡

Abstract: As Large Language Models (LLMs) increasingly permeate human life, their
security has emerged as a critical concern, particularly their ability to
maintain harmless responses to malicious instructions. Although extensive
methods have improved LLMs' security, they often lead to conservative,
rejection-oriented responses that compromise practical usability. This presents
a key challenge: how to advance the Pareto frontier between LLMs' usability and
security, rather than necessitate a trade-off between them. To address this, we
propose the MoGU framework, in which the intra-layer router dynamically
allocates weights by sensing hidden states, thereby balancing the contributions
of security-optimized and usability-optimized variants. Despite its initial
potential, the MoGU framework faces limitations such as parameter redundancy
and performance bottlenecks. To overcome these, we further propose an improved
MoGU_v2 framework that establishes a tighter coupling between the routers and
hidden states. In MoGU_v2, routers are embedded only in layers encoding highly
classifiable security features, and backbone modules are activated during
router optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong
adaptability and stable improvements across various series of LLMs, including
mainstream LLMs serving as brains in various applications, on-device LLMs
optimized for resource-constrained scenarios, and reasoning LLMs tailored for
user interpretability. Meanwhile, even facing risks introduced by Instruction
Fine-tuning, MoGU_v2 can easily restore security without compromising the task
performance gains via a simple data-mix strategy. These comprehensive
improvements highlight MoGU_V2 as a robust and versatile solution for
mitigating security risks in real-world applications.

</details>


### [58] [Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem](https://arxiv.org/abs/2509.06809)
*Valentin Quesnel,Damien Sileo*

Main category: cs.CL

TL;DR: 通过利用E-prover对TPTP公理库进行饱和证明，生成大量高质量数学推理数据，构建了三种难度可控的数学推理挑战任务


<details>
  <summary>Details</summary>
Motivation: 解决高质量逻辑声数据稀缺问题，为提升大语言模型的数学推理能力提供可靠数据源

Method: 利用E-prover的饱和证明能力处理TPTP公理库，通过饱和公理、筛选有趣定理、生成任务的流水线构建数据

Result: 构建了保证正确的纯符号数据集，发现前沿模型在深层结构化推理任务上表现崩溃

Conclusion: 提供了评估模型推理空白的诊断工具和可扩展的符号训练数据来应对这一挑战

Abstract: The scarcity of high-quality, logically sound data is a critical bottleneck
for advancing the mathematical reasoning of Large Language Models (LLMs). Our
work confronts this challenge by turning decades of automated theorem proving
research into a scalable data engine. Rather than relying on error-prone LLMs
or complex proof-assistant syntax like Lean and Isabelle, our framework
leverages E-prover's saturation capabilities on the vast TPTP axiom library to
derive a massive, guaranteed-valid corpus of theorems. Our pipeline is
principled and simple: saturate axioms, filter for "interesting" theorems, and
generate tasks. With no LLMs in the loop, we eliminate factual errors by
construction. This purely symbolic data is then transformed into three
difficulty-controlled challenges: entailment verification, premise selection,
and proof reconstruction. Our zero-shot experiments on frontier models reveal a
clear weakness: performance collapses on tasks requiring deep, structural
reasoning. Our framework provides both the diagnostic tool to measure this gap
and a scalable source of symbolic training data to address it. We make the code
and data publicly available.
  https://github.com/sileod/reasoning_core
https://hf.co/datasets/reasoning-core/rc1

</details>


### [59] [A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs](https://arxiv.org/abs/2509.06813)
*Max Malyi,Jonathan Shek,Alasdair McDonald,Andre Biscaya*

Main category: cs.CL

TL;DR: 这篇论文提出了一个新题的可复现框架，用于对大语言模型在风电汽车维护日志分类任务上进行基准测试，并开源了这个框架以促进相关研究。


<details>
  <summary>Details</summary>
Motivation: 风电汽车维护日志的非结构化特性继续阻碍自动化分析，影响了风电运营维护的效率和成本效益。

Method: 系统性评估多种最新的专有和开源大语言模型，分析它们在可靠性、运营效率和模型检验方面的交易。

Result: 结果量化了清晰的性能层次，识别出了与基准标准高度一致且信心度评分可靠的顶级模型。分类性能依赖于任务的语义模糊性，所有模型在对象性组件识别上比在解释性维护行为上更一致。

Conclusion: 由于没有模型能达到完美准确性且检验结果差异显著，最有效和负责任的应用方式是人在循环系统，让LLMs作为强大助手加速和标准化数据标注，提升运维数据质量和下游可靠性分析。

Abstract: Effective Operation and Maintenance (O&M) is critical to reducing the
Levelised Cost of Energy (LCOE) from wind power, yet the unstructured,
free-text nature of turbine maintenance logs presents a significant barrier to
automated analysis. Our paper addresses this by presenting a novel and
reproducible framework for benchmarking Large Language Models (LLMs) on the
task of classifying these complex industrial records. To promote transparency
and encourage further research, this framework has been made publicly available
as an open-source tool. We systematically evaluate a diverse suite of
state-of-the-art proprietary and open-source LLMs, providing a foundational
assessment of their trade-offs in reliability, operational efficiency, and
model calibration. Our results quantify a clear performance hierarchy,
identifying top models that exhibit high alignment with a benchmark standard
and trustworthy, well-calibrated confidence scores. We also demonstrate that
classification performance is highly dependent on the task's semantic
ambiguity, with all models showing higher consensus on objective component
identification than on interpretive maintenance actions. Given that no model
achieves perfect accuracy and that calibration varies dramatically, we conclude
that the most effective and responsible near-term application is a
Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate
and standardise data labelling for human experts, thereby enhancing O&M data
quality and downstream reliability analysis.

</details>


### [60] [COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens](https://arxiv.org/abs/2509.06836)
*Eugene Kwek,Wenpeng Yin*

Main category: cs.CL

TL;DR: COMPACT是一种联合剪枝方法，通过剪枝稀有词汇表来缩小嵌入/解嵌入层，并使用常见词加权的激活值来剪枝FFN中间通道，在保持标准Transformer架构的同时实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 使LLM在内存、延迟和服务成本方面更高效，以适应边缘部署、交互式应用和大规模可持续推理的需求。现有剪枝方法存在局限性：宽度剪枝破坏标准架构或需要定制推理代码，深度剪枝移除整个层导致精度骤降。

Method: 联合剪枝：(1)剪枝稀有词汇表缩小嵌入/解嵌入层；(2)基于常见词加权的激活值剪枝FFN中间通道，使重要性评估与剪枝后的词分布对齐。保持标准Transformer架构，无需训练，支持词汇表与FFN剪枝的权衡。

Result: 在Qwen、LLaMA和Gemma系列模型(0.5B-70B)上实验显示，在相似或更高剪枝比例下达到最先进的下游任务性能，显著减少参数量、GPU内存使用和端到端延迟。

Conclusion: COMPACT结合了深度和宽度剪枝的优点，具有部署友好性、规模适应性、免训练操作和强内存节省及吞吐量提升，为LLM高效推理提供了有效解决方案。

Abstract: Making LLMs more efficient in memory, latency, and serving cost is crucial
for edge deployment, interactive applications, and sustainable inference at
scale. Pruning is a key technique toward this goal. However, prior pruning
methods are limited: width pruning often breaks the standard transformer layout
or requires custom inference code, while depth pruning removes entire layers
and can cause abrupt accuracy drops. In this work, we propose COMPACT, which
jointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)
prunes FFN intermediate channels using common-token-weighted activations,
aligning importance with the post-pruning token distribution. COMPACT enjoys
merits of both depth and width pruning, such as: deployment-friendliness (keeps
a standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN
pruning), training-free operation with competitive pruning time, and strong
memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and
Gemma families (0.5B-70B) show state-of-the-art downstream task performance at
similar or higher pruning ratios, with substantial reductions in parameters,
GPU memory, and end-to-end latency.

</details>


### [61] [EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models](https://arxiv.org/abs/2509.06838)
*Mohammad Reza Mirbagheri,Mohammad Mahdi Mirkamali,Zahra Motoshaker Arani,Ali Javeri,Amir Mahdi Sadeghzadeh,Rasool Jalili*

Main category: cs.CL

TL;DR: 提出了EPT（波斯语可信度评估）指标，这是一个专门针对波斯文化设计的基准测试，用于评估大语言模型在六个关键维度的可信度：真实性、安全性、公平性、鲁棒性、隐私性和伦理对齐。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种语言任务上表现出色，但确保其可信度仍然是一个关键挑战。特别是在波斯文化背景下，需要文化基础的评估标准来开发负责任的AI系统。

Method: 构建了带标签的数据集，使用自动化的基于LLM的评估和人工评估相结合的方法，评估了包括ChatGPT、Claude、DeepSeek、Gemini、Grok、LLaMA、Mistral和Qwen在内的多个领先模型。

Result: 研究结果显示在安全性维度存在显著缺陷，强调了需要重点关注模型行为的这一关键方面。同时揭示了这些模型与波斯伦理文化价值观对齐的情况。

Conclusion: 该研究为推进可信赖和文化负责任的AI提供了重要见解，指出了关键差距和发展机会。数据集已公开可用，为后续研究提供了基础。

Abstract: Large Language Models (LLMs), trained on extensive datasets using advanced
deep learning architectures, have demonstrated remarkable performance across a
wide range of language tasks, becoming a cornerstone of modern AI technologies.
However, ensuring their trustworthiness remains a critical challenge, as
reliability is essential not only for accurate performance but also for
upholding ethical, cultural, and social values. Careful alignment of training
data and culturally grounded evaluation criteria are vital for developing
responsible AI systems. In this study, we introduce the EPT (Evaluation of
Persian Trustworthiness) metric, a culturally informed benchmark specifically
designed to assess the trustworthiness of LLMs across six key aspects:
truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We
curated a labeled dataset and evaluated the performance of several leading
models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and
Qwen - using both automated LLM-based and human assessments. Our results reveal
significant deficiencies in the safety dimension, underscoring the urgent need
for focused attention on this critical aspect of model behavior. Furthermore,
our findings offer valuable insights into the alignment of these models with
Persian ethical-cultural values and highlight critical gaps and opportunities
for advancing trustworthy and culturally responsible AI. The dataset is
publicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.

</details>


### [62] [The Majority is not always right: RL training for solution aggregation](https://arxiv.org/abs/2509.06870)
*Wenting Zhao,Pranjal Aggarwal,Swarnadeep Saha,Asli Celikyilmaz,Jason Weston,Ilia Kulikov*

Main category: cs.CL

TL;DR: 提出AggLM方法，通过强化学习训练聚合模型来整合多个候选解决方案，相比简单投票或奖励模型排序能更有效地提升LLM推理性能


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖简单多数投票或奖励模型排序来聚合解决方案，但这些方法收益有限，需要更智能的聚合策略来提升大语言模型在复杂推理任务中的表现

Method: 使用强化学习训练聚合模型，让模型学习审查、协调和综合多个候选解决方案来生成最终正确答案。关键是通过平衡难易训练样本来学习恢复少数但正确的答案以及容易的多数正确答案

Result: AggLM方法在多个基准测试中优于基于规则和奖励模型的基线方法，能够有效泛化到不同模型的解决方案，包括比训练数据中更强的模型，同时比多数投票方法需要更少的token

Conclusion: 将聚合作为显式推理技能进行学习是有效的，通过精心设计的训练策略可以显著提升解决方案聚合的效果，为LLM推理任务提供了新的优化途径

Abstract: Scaling up test-time compute, by generating multiple independent solutions
and selecting or aggregating among them, has become a central paradigm for
improving large language models (LLMs) on challenging reasoning tasks. While
most prior work relies on simple majority voting or reward model ranking to
aggregate solutions, these approaches may only yield limited benefits. In this
work, we propose to learn aggregation as an explicit reasoning skill: given a
set of candidate solutions, we train an aggregator model to review, reconcile,
and synthesize a final, correct answer using reinforcement learning from
verifiable rewards. A key ingredient is careful balancing of easy and hard
training examples, allowing the model to learn both to recover
minority-but-correct answers as well as easy majority-correct answers.
Empirically, we find our method, AggLM, outperforms both strong rule-based and
reward-model baselines, across multiple benchmarks. Furthermore, it generalizes
effectively to solutions from differing models, including stronger ones than
contained in the training data, all while requiring substantially fewer tokens
than majority voting with larger numbers of solutions.

</details>


### [63] [UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction](https://arxiv.org/abs/2509.06883)
*Joe Wilder,Nikhil Kadapala,Benji Xu,Mohammed Alsaadi,Aiden Parsons,Mitchell Rogers,Palash Agarwal,Adam Hassick,Laura Dietz*

Main category: cs.CL

TL;DR: 本文探索了多种提示学习和上下文学习方法，包括少样本提示和不同LLM家族的微调，用于从社交媒体文本中提取值得核查的声明。FLAN-T5微调获得了最佳METEOR分数，但其他方法有时能提取更高质量的声明。


<details>
  <summary>Details</summary>
Motivation: 参与CheckThat! Task 2英语任务，研究如何从社交媒体段落中有效提取值得核查的声明，比较不同提示学习和微调方法的性能。

Method: 使用了少样本提示、上下文学习以及不同LLM家族（包括FLAN-T5）的微调方法，通过METEOR分数评估提取质量。

Result: FLAN-T5微调模型获得了最佳的METEOR分数，但研究发现其他方法有时能提取出更高质量的声明，尽管其METEOR分数较低。

Conclusion: 虽然FLAN-T5微调在自动化评估指标上表现最佳，但其他提示学习方法在实际声明质量方面可能更有优势，表明自动化评估指标与人工质量评估之间存在差异。

Abstract: We participate in CheckThat! Task 2 English and explore various methods of
prompting and in-context learning, including few-shot prompting and fine-tuning
with different LLM families, with the goal of extracting check-worthy claims
from social media passages. Our best METEOR score is achieved by fine-tuning a
FLAN-T5 model. However, we observe that higher-quality claims can sometimes be
extracted using other methods, even when their METEOR scores are lower.

</details>


### [64] [mmBERT: A Modern Multilingual Encoder with Annealed Language Learning](https://arxiv.org/abs/2509.06888)
*Marc Marone,Orion Weller,William Fleshman,Eugene Yang,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: mmBERT是一个基于3T多语言文本预训练的编码器模型，通过创新的逆掩码比率调度和逆温度采样策略，在1800多种语言上显著提升了分类和检索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对编码器模型特别是多语言模型的最新研究，需要开发一个能够处理高资源和低资源语言的强大编码器模型。

Method: 使用3T多语言文本预训练，引入逆掩码比率调度和逆温度采样策略，在衰减阶段添加1700多种低资源语言数据。

Result: 在分类和检索任务上显著超越前代模型，性能与OpenAI o3和Google Gemini 2.5 Pro相当，特别是在低资源语言上表现突出。

Conclusion: mmBERT证明了通过创新的训练策略和数据调度方法，编码器模型在多语言任务中可以达到与先进模型相当的性能水平。

Abstract: Encoder-only languages models are frequently used for a variety of standard
machine learning tasks, including classification and retrieval. However, there
has been a lack of recent research for encoder models, especially with respect
to multilingual models. We introduce mmBERT, an encoder-only language model
pretrained on 3T tokens of multilingual text in over 1800 languages. To build
mmBERT we introduce several novel elements, including an inverse mask ratio
schedule and an inverse temperature sampling ratio. We add over 1700
low-resource languages to the data mix only during the decay phase, showing
that it boosts performance dramatically and maximizes the gains from the
relatively small amount of training data. Despite only including these
low-resource languages in the short decay phase we achieve similar
classification performance to models like OpenAI's o3 and Google's Gemini 2.5
Pro. Overall, we show that mmBERT significantly outperforms the previous
generation of models on classification and retrieval tasks -- on both high and
low-resource languages.

</details>


### [65] [Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification](https://arxiv.org/abs/2509.06902)
*Aivin V. Solatorio*

Main category: cs.CL

TL;DR: Proof-Carrying Numbers (PCN) 是一种展示层协议，通过机械验证来强制保证大语言模型生成数字的保真度，防止数字幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型作为随机系统可能生成与可用数据不符的数字（数字幻觉），现有的安全措施如检索增强生成、引用和不确定性估计虽然提高了透明度，但无法保证保真度。

Method: PCN 将数字跨度作为与结构化声明绑定的声明绑定令牌发出，验证器根据声明的策略（如精确相等、舍入、别名或带限定符的容差）检查每个令牌。验证放在渲染器中而非模型中，只有经过声明检查的数字才被标记为已验证。

Result: PCN 被形式化并证明了其正确性、诚实令牌下的完备性、故障关闭行为和策略细化下的单调性。PCN 轻量级、模型无关，可无缝集成到现有应用中，并可扩展加密承诺。

Conclusion: 通过在显示前强制验证，PCN 为数字敏感环境建立了一个简单契约：信任只能通过证明获得，而缺少标记则传达不确定性。

Abstract: Large Language Models (LLMs) as stochastic systems may generate numbers that
deviate from available data, a failure known as \emph{numeric hallucination}.
Existing safeguards -- retrieval-augmented generation, citations, and
uncertainty estimation -- improve transparency but cannot guarantee fidelity:
fabricated or misquoted values may still be displayed as if correct. We propose
\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that
enforces numeric fidelity through mechanical verification. Under PCN, numeric
spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a
verifier checks each token under a declared policy (e.g., exact equality,
rounding, aliases, or tolerance with qualifiers). Crucially, PCN places
verification in the \emph{renderer}, not the model: only claim-checked numbers
are marked as verified, and all others default to unverified. This separation
prevents spoofing and guarantees fail-closed behavior. We formalize PCN and
prove soundness, completeness under honest tokens, fail-closed behavior, and
monotonicity under policy refinement. PCN is lightweight and model-agnostic,
integrates seamlessly into existing applications, and can be extended with
cryptographic commitments. By enforcing verification as a mandatory step before
display, PCN establishes a simple contract for numerically sensitive settings:
\emph{trust is earned only by proof}, while the absence of a mark communicates
uncertainty.

</details>


### [66] [Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning](https://arxiv.org/abs/2509.06948)
*Liang Chen,Xueting Han,Li Shen,Jing Bai,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 本文提出了一种双层优化方法，通过让监督微调(SFT)元学习如何指导强化学习(RL)的优化过程，实现SFT和RL的更好协同训练，在五个推理基准测试中取得了更好的效果和效率平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的两阶段方法（先SFT后RL）限制了SFT和RL之间的交互，限制了整体效果。RL虽然能激励大语言模型的推理能力，但由于试错性质导致效率低下。

Method: 采用双层优化方法，将SFT目标条件化于最优RL策略，使SFT能够元学习指导RL的优化过程。下层执行RL更新同时接收SFT监督，上层显式最大化协同增益（联合训练相比单独RL的性能优势）。

Result: 在五个推理基准测试上的实证评估表明，该方法始终优于基线方法，并在效果和效率之间实现了更好的平衡。

Conclusion: 通过双层优化实现SFT和RL的协同训练，能够有效提升大语言模型推理能力的学习效果和效率。

Abstract: Reinforcement learning (RL) has proven effective in incentivizing the
reasoning abilities of large language models (LLMs), but suffers from severe
efficiency challenges due to its trial-and-error nature. While the common
practice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this
decoupled two-stage approach limits interaction between SFT and RL, thereby
constraining overall effectiveness. This study introduces a novel method for
learning reasoning models that employs bilevel optimization to facilitate
better cooperation between these training paradigms. By conditioning the SFT
objective on the optimal RL policy, our approach enables SFT to meta-learn how
to guide RL's optimization process. During training, the lower level performs
RL updates while simultaneously receiving SFT supervision, and the upper level
explicitly maximizes the cooperative gain-the performance advantage of joint
SFT-RL training over RL alone. Empirical evaluations on five reasoning
benchmarks demonstrate that our method consistently outperforms baselines and
achieves a better balance between effectiveness and efficiency.

</details>


### [67] [Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models](https://arxiv.org/abs/2509.06949)
*Yinjie Wang,Ling Yang,Bowen Li,Ye Tian,Ke Shen,Mengdi Wang*

Main category: cs.CL

TL;DR: TraceRL是一个轨迹感知的强化学习框架，用于扩散语言模型的后训练，通过扩散价值模型提升训练稳定性，在数学推理和编程任务上表现优异，并发布了开源框架。


<details>
  <summary>Details</summary>
Motivation: 为了解决扩散语言模型在复杂推理任务中的性能问题，并提高采样灵活性，需要一种能够整合偏好推理轨迹的后训练方法。

Method: 提出TraceRL框架，使用扩散价值模型增强训练稳定性，支持课程学习，可适配不同架构的模型，并应用加速KV缓存技术。

Result: TraDo-4B-Instruct在数学推理任务上持续超越7B规模的自回归模型；TraDo-8B-Instruct在数学推理基准上相对Qwen2.5-7B-Instruct提升6.1%，相对Llama3.1-8B-Instruct提升51.3%；通过课程学习获得首个长链推理扩散模型，在MATH500上相对Qwen2.5-7B-Instruct提升18.1%。

Conclusion: TraceRL框架有效提升了扩散语言模型的推理性能，证明了轨迹感知强化学习在后训练中的价值，并提供了可复现的开源实现。

Abstract: We propose TraceRL, a trajectory-aware reinforcement learning framework for
diffusion language models (DLMs) that incorporates preferred inference
trajectory into post-training, and is applicable across different
architectures. Equipped with a diffusion-based value model that enhances
training stability, we demonstrate improved reasoning performance on complex
math and coding tasks. Besides, it can also be applied to adapt block-specific
models to larger blocks, which improves sampling flexibility. Employing
TraceRL, we derive a series of state-of-the-art diffusion language models,
namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still
consistently outperforms them across complex math reasoning tasks.
TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over
Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical
reasoning benchmarks. Through curriculum learning, we also derive the first
long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%
relative accuracy gain. To facilitate reproducible research and practical
applications, we release a comprehensive open-source framework for building,
training, and deploying diffusion LLMs across diverse architectures. The
framework integrates accelerated KV-cache techniques and inference engines for
both inference and reinforcement learning, and includes implementations of
various supervised fine-tuning and RL methods for mathematics, coding, and
general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL

</details>


### [68] [On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts](https://arxiv.org/abs/2509.06952)
*Linlu Qiu,Cedegao E. Zhang,Joshua B. Tenenbaum,Yoon Kim,Roger P. Levy*

Main category: cs.CL

TL;DR: 这篇论文通过Wavelength游戏框架评估语言模型的语用理解能力，发现大型模型在语言理解上接近人类水平，而CoT提示和RSA方法能显著提升语言生成能力。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型作为会话助手的广泛使用，需要理解其语用理解能力，即如何在上下文中进行通信目标和规范的理性推理。

Method: 采用Wavelength通信游戏框架，研究多种语言模型在语言理解和生成上的表现，测试了直接提示、思维链(CoT)提示以及理性语言行为(RSA)方法。

Result: 现有最先进的大型语言模型在语言理解任务上达到了类似于人类的准确率，且与人类判断呈现高相关性。在语言生成任务上，CoT提示比直接提示效果更好，而RSA方法在两种提示方式上都带来了显著改善。

Conclusion: 研究帮助识别了语言模型在语用理解方面的优势与局限性，并证明了RSA方法在提升模型理解能力方面的潜力，为了解概念表征、语言理解和社会理解提供了新的研究方向。

Abstract: Language use is shaped by pragmatics -- i.e., reasoning about communicative
goals and norms in context. As language models (LMs) are increasingly used as
conversational agents, it becomes ever more important to understand their
pragmatic reasoning abilities. We propose an evaluation framework derived from
Wavelength, a popular communication game where a speaker and a listener
communicate about a broad range of concepts in a granular manner. We study a
range of LMs on both language comprehension and language production using
direct and Chain-of-Thought (CoT) prompting, and further explore a Rational
Speech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM
inference. We find that state-of-the-art LMs, but not smaller ones, achieve
strong performance on language comprehension, obtaining similar-to-human
accuracy and exhibiting high correlations with human judgments even without CoT
prompting or RSA. On language production, CoT can outperform direct prompting,
and using RSA provides significant improvements over both approaches. Our study
helps identify the strengths and limitations in LMs' pragmatic reasoning
abilities and demonstrates the potential for improving them with RSA, opening
up future avenues for understanding conceptual representation, language
understanding, and social reasoning in LMs and humans.

</details>


### [69] [Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval](https://arxiv.org/abs/2311.01870)
*Jinrui Yang,Timothy Baldwin,Trevor Cohn*

Main category: cs.CL

TL;DR: Multi-EuP是一个新的多语言基准数据集，包含来自欧洲议会的22K多语言文档，涵盖24种语言，用于研究多语言信息检索中的公平性和偏见问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决多语言信息检索(IR)环境中的公平性问题，分析排名上下文中的语言偏见和人口统计偏见，需要一个包含真实多语言语料库、跨语言相关性判断和丰富人口统计信息的基准数据集。

Method: 收集欧洲议会的22K多语言文档，涵盖24种语言，构建包含多语言主题翻译和跨语言相关性判断的数据集，并提供文档相关的人口统计信息。

Result: 创建了Multi-EuP数据集，可用于单语和多语言IR的基准测试，并通过初步实验展示了分词策略选择导致的语言偏见。

Conclusion: Multi-EuP为研究多语言信息检索中的公平性和偏见问题提供了有价值的基准资源，有助于分析语言和人口统计偏见。

Abstract: We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K
multi-lingual documents collected from the European Parliament, spanning 24
languages. This dataset is designed to investigate fairness in a multilingual
information retrieval (IR) context to analyze both language and demographic
bias in a ranking context. It boasts an authentic multilingual corpus,
featuring topics translated into all 24 languages, as well as cross-lingual
relevance judgments. Furthermore, it offers rich demographic information
associated with its documents, facilitating the study of demographic bias. We
report the effectiveness of Multi-EuP for benchmarking both monolingual and
multilingual IR. We also conduct a preliminary experiment on language bias
caused by the choice of tokenization strategy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [70] [Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning](https://arxiv.org/abs/2509.05316)
*Praveen Bushipaka,Lucia Passaro,Tommaso Cucinotta*

Main category: cs.LG

TL;DR: 本文系统评估了LLM遗忘学习中的常见实践，发现单一邻居集和标准采样方法存在不足，提出了包含多样邻居集和模块化实体级遗忘策略的最佳实践方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM遗忘学习基准通常只使用单一邻居集和通用知识，未能反映真实世界数据的复杂性，且标准采样方法的有效性和稳定性未经严格检验。

Method: 系统评估现有常见实践，提出并验证最佳实践：包含多样邻居集、改进采样方法、提出模块化实体级遗忘(MELU)策略替代循环采样。

Result: 研究发现单一邻居集效果不佳，标准采样方法效率低下且结果差，提出的模块化方法结合鲁棒算法提供了清晰稳定的有效遗忘路径。

Conclusion: 需要采用多样邻居集来平衡遗忘效果和模型效用，标准1:1采样方法需要改进，模块化实体级遗忘策略是循环采样的有效替代方案。

Abstract: A conventional LLM Unlearning setting consists of two subsets -"forget" and
"retain", with the objectives of removing the undesired knowledge from the
forget set while preserving the remaining knowledge from the retain. In
privacy-focused unlearning research, a retain set is often further divided into
neighbor sets, containing either directly or indirectly connected to the forget
targets; and augmented by a general-knowledge set. A common practice in
existing benchmarks is to employ only a single neighbor set, with general
knowledge which fails to reflect the real-world data complexities and
relationships. LLM Unlearning typically involves 1:1 sampling or cyclic
iteration sampling. However, the efficacy and stability of these de facto
standards have not been critically examined. In this study, we systematically
evaluate these common practices. Our findings reveal that relying on a single
neighbor set is suboptimal and that a standard sampling approach can obscure
performance trade-offs. Based on this analysis, we propose and validate an
initial set of best practices: (1) Incorporation of diverse neighbor sets to
balance forget efficacy and model utility, (2) Standard 1:1 sampling methods
are inefficient and yield poor results, (3) Our proposed Modular Entity-Level
Unlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate
that this modular approach, combined with robust algorithms, provides a clear
and stable path towards effective unlearning.

</details>


### [71] [MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs](https://arxiv.org/abs/2509.05488)
*Hongjun Xu,Junxi Xia,Weisi Yang,Yueyuan Sui,Stephen Xia*

Main category: cs.LG

TL;DR: 首个在资源受限微控制器上部署Mamba模型的方案MambaLite-Micro，通过C语言实现、算子融合和内存优化，减少83%峰值内存，保持与PyTorch一致的精度。


<details>
  <summary>Details</summary>
Motivation: 由于微控制器内存有限、缺乏原生算子支持和嵌入式友好工具链，部署Mamba模型面临挑战，需要开发轻量级部署方案。

Method: 提出MambaLite-Micro：1) 导出模型权重为轻量格式；2) 用C语言手工实现Mamba层和支持算子，进行算子融合和内存布局优化；3) 消除大型中间张量。

Result: 峰值内存减少83.0%，平均数值误差仅1.7x10^-5，在关键词检测和人体活动识别任务上达到100%一致性，保持分类精度，在ESP32S3和STM32H7上验证可移植性。

Conclusion: MambaLite-Micro成功将先进序列模型Mamba部署到资源受限嵌入式平台，为实际应用铺平道路。

Abstract: Deploying Mamba models on microcontrollers (MCUs) remains challenging due to
limited memory, the lack of native operator support, and the absence of
embedded-friendly toolchains. We present, to our knowledge, the first
deployment of a Mamba-based neural architecture on a resource-constrained MCU,
a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline
maps a trained PyTorch Mamba model to on-device execution by (1) exporting
model weights into a lightweight format, and (2) implementing a handcrafted
Mamba layer and supporting operators in C with operator fusion and memory
layout optimization. MambaLite-Micro eliminates large intermediate tensors,
reducing 83.0% peak memory, while maintaining an average numerical error of
only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on
keyword spotting(KWS) and human activity recognition (HAR) tasks,
MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully
preserving classification accuracy. We further validated portability by
deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating
consistent operation across heterogeneous embedded platforms and paving the way
for bringing advanced sequence models like Mamba to real-world
resource-constrained applications.

</details>


### [72] [Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance](https://arxiv.org/abs/2509.05328)
*Xiang Yuan,Jun Shu,Deyu meng,Zongben Xu*

Main category: cs.LG

TL;DR: 本文提出了一种新的正则化方法，通过在函数空间约束微调模型与预训练模型的距离，并使用模拟OOD样本来保持预训练模型的OOD稳健性，同时提升下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的稳健微调方法通过保留预训练权重、特征或输出来维持OOD稳健性，但这些方法并不能永远改善不同模型架构的OOD稳健性，因为它们可能是函数空间优化的代理指标。

Method: 提出了两种正则化：1)在函数空间约束微调模型与预训练模型的距离，使用模拟OOD样本来保持OOD稳健性；2)一个一致性正则化来促进干扰样本的稳定预测。

Result: 大量实验证明，该方法能够在各种CLIP背榜模型上一致地改善下游任务的ID微调性能和OOD稳健性，超越现有的基于正则化的稳健微调方法。

Conclusion: 通过直接在函数空间中约束微调过程，而不仅仅是保留权重或特征，可以更有效地保持预训练模型的OOD稳健性，同时提升下游任务的性能。

Abstract: Robust fine-tuning aims to achieve competitive in-distribution (ID)
performance while maintaining the out-of-distribution (OOD) robustness of a
pre-trained model when transferring it to a downstream task. To remedy this,
most robust fine-tuning methods aim to preserve the pretrained weights,
features, or logits. However, we find that these methods cannot always improve
OOD robustness for different model architectures. This is due to the OOD
robustness requiring the model function to produce stable prediction for input
information of downstream tasks, while existing methods might serve as a poor
proxy for the optimization in the function space. Based on this finding, we
propose a novel regularization that constrains the distance of fine-tuning and
pre-trained model in the function space with the simulated OOD samples, aiming
to preserve the OOD robustness of the pre-trained model. Besides, to further
enhance the OOD robustness capability of the fine-tuning model, we introduce an
additional consistency regularization to promote stable predictions of
perturbed samples. Extensive experiments demonstrate our approach could
consistently improve both downstream task ID fine-tuning performance and OOD
robustness across a variety of CLIP backbones, outperforming existing
regularization-based robust fine-tuning methods.

</details>


### [73] [Safeguarding Graph Neural Networks against Topology Inference Attacks](https://arxiv.org/abs/2509.05429)
*Jie Fu,Hong Yuan,Zhili Chen,Wendy Hui Wang*

Main category: cs.LG

TL;DR: 本文研究了GNN中的拓扑隐私风险，提出了拓扑推理攻击(TIAs)并发现现有边缘级差分隐私机制不足，进而提出了Private Graph Reconstruction (PGR)防御框架来保护拓扑隐私同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: GNN在图形结构数据学习方面表现出强大能力，但其广泛应用引发了严重的隐私问题。现有研究主要关注边缘级隐私，而拓扑隐私（图整体结构的机密性）这一关键威胁尚未得到充分探索。

Method: 提出了拓扑推理攻击(TIAs)套件，能够仅通过黑盒访问GNN模型来重建目标训练图的结构。为防御此类攻击，提出了Private Graph Reconstruction (PGR)框架，将其表述为一个双层优化问题，使用元梯度迭代生成合成训练图，并基于演化图同时更新GNN模型。

Result: 研究发现GNN对这些攻击高度易感，现有边缘级差分隐私机制要么无法缓解风险，要么严重损害模型准确性。PGR框架显著减少了拓扑泄漏，同时对模型准确性的影响最小。

Conclusion: 拓扑隐私是GNN中一个关键但被忽视的威胁，需要专门的防御机制。PGR框架有效解决了这一问题，在保护拓扑隐私的同时保持了模型性能。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful models for learning
from graph-structured data. However, their widespread adoption has raised
serious privacy concerns. While prior research has primarily focused on
edge-level privacy, a critical yet underexplored threat lies in topology
privacy - the confidentiality of the graph's overall structure. In this work,
we present a comprehensive study on topology privacy risks in GNNs, revealing
their vulnerability to graph-level inference attacks. To this end, we propose a
suite of Topology Inference Attacks (TIAs) that can reconstruct the structure
of a target training graph using only black-box access to a GNN model. Our
findings show that GNNs are highly susceptible to these attacks, and that
existing edge-level differential privacy mechanisms are insufficient as they
either fail to mitigate the risk or severely compromise model accuracy. To
address this challenge, we introduce Private Graph Reconstruction (PGR), a
novel defense framework designed to protect topology privacy while maintaining
model accuracy. PGR is formulated as a bi-level optimization problem, where a
synthetic training graph is iteratively generated using meta-gradients, and the
GNN model is concurrently updated based on the evolving graph. Extensive
experiments demonstrate that PGR significantly reduces topology leakage with
minimal impact on model accuracy. Our code is anonymously available at
https://github.com/JeffffffFu/PGR.

</details>


### [74] [Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis](https://arxiv.org/abs/2509.05449)
*Disha Makhija,Manoj Ghuhan Arivazhagan,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.LG

TL;DR: 本文提出了memTrace框架，通过分析LLM内部表示（隐藏状态和注意力模式）而非仅输出，来检测成员推理攻击，在多个模型上达到平均0.85 AUC分数，表明即使输出信号看似安全，内部行为仍可能泄露训练数据信息。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明成员推理攻击对大型语言模型效果仅略优于随机猜测，认为现代预训练方法可能免于隐私泄露风险。本文旨在探索通过分析模型内部表示而非仅输出来发现潜在的成员推理信号。

Method: 提出memTrace框架，通过分析transformer隐藏状态和注意力模式的"神经痕迹"，包括层级表示动态、注意力分布特征和跨层转换模式，检测传统基于损失的方法可能无法捕获的记忆指纹。

Result: 在多个模型系列上实现了强大的成员检测能力，在流行的MIA基准测试中达到平均0.85的AUC分数，表明内部模型行为可以揭示训练数据暴露的方面。

Conclusion: 即使基于输出的信号看似受到保护，内部模型行为仍可能泄露隐私信息，强调需要进一步研究成员隐私问题，并开发更强大的隐私保护训练技术。

Abstract: Membership inference attacks (MIAs) reveal whether specific data was used to
train machine learning models, serving as important tools for privacy auditing
and compliance assessment. Recent studies have reported that MIAs perform only
marginally better than random guessing against large language models,
suggesting that modern pre-training approaches with massive datasets may be
free from privacy leakage risks. Our work offers a complementary perspective to
these findings by exploring how examining LLMs' internal representations,
rather than just their outputs, may provide additional insights into potential
membership inference signals. Our framework, \emph{memTrace}, follows what we
call \enquote{neural breadcrumbs} extracting informative signals from
transformer hidden states and attention patterns as they process candidate
sequences. By analyzing layer-wise representation dynamics, attention
distribution characteristics, and cross-layer transition patterns, we detect
potential memorization fingerprints that traditional loss-based approaches may
not capture. This approach yields strong membership detection across several
model families achieving average AUC scores of 0.85 on popular MIA benchmarks.
Our findings suggest that internal model behaviors can reveal aspects of
training data exposure even when output-based signals appear protected,
highlighting the need for further research into membership privacy and the
development of more robust privacy-preserving training techniques for large
language models.

</details>


### [75] [Calibrated Recommendations with Contextual Bandits](https://arxiv.org/abs/2509.05460)
*Diego Feijer,Himan Abdollahpouri,Sanket Gupta,Alexander Clare,Yuxiao Wen,Todd Wasson,Maria Dimakopoulou,Zahra Nazari,Kyle Kretschman,Mounia Lalmas*

Main category: cs.LG

TL;DR: Spotify提出基于上下文多臂老虎机的校准方法，动态学习用户在不同情境下的最优内容类型分布，解决历史数据偏向音乐内容的问题，提升用户参与度和非音乐内容的曝光。


<details>
  <summary>Details</summary>
Motivation: Spotify首页包含多种内容类型（音乐、播客、有声书），但历史数据严重偏向音乐内容，难以提供平衡且个性化的内容组合。用户对不同内容类型的偏好会随时间、星期和设备等情境因素变化。

Method: 使用上下文多臂老虎机（contextual bandits）方法，动态学习每个用户在特定情境下的最优内容类型分布，而非依赖历史平均值。

Result: 离线和在线实验结果均显示，该方法提高了Spotify首页的推荐精度和用户参与度，特别是对播客等代表性不足的内容类型有显著改善。

Conclusion: 基于上下文多臂老虎机的校准方法能够有效适应用户兴趣在不同情境下的变化，提升内容推荐的平衡性和个性化效果，特别有助于非音乐内容的推广。

Abstract: Spotify's Home page features a variety of content types, including music,
podcasts, and audiobooks. However, historical data is heavily skewed toward
music, making it challenging to deliver a balanced and personalized content
mix. Moreover, users' preference towards different content types may vary
depending on the time of day, the day of week, or even the device they use. We
propose a calibration method that leverages contextual bandits to dynamically
learn each user's optimal content type distribution based on their context and
preferences. Unlike traditional calibration methods that rely on historical
averages, our approach boosts engagement by adapting to how users interests in
different content types varies across contexts. Both offline and online results
demonstrate improved precision and user engagement with the Spotify Home page,
in particular with under-represented content types such as podcasts.

</details>


### [76] [PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series](https://arxiv.org/abs/2509.05478)
*Jia Wang,Xiao Wang,Chi Zhang*

Main category: cs.LG

TL;DR: PLanTS是一个周期性感知的自监督学习框架，通过多粒度分块机制和状态转换预测任务，有效解决了多元时间序列的高维度和标签稀缺问题，在多个下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列具有高维度、标签稀缺和非平稳性等挑战，现有自监督学习方法忽略了内在周期性结构，无法捕捉潜在状态的动态演化。

Method: 提出周期性感知的多粒度分块机制和广义对比损失，设计下一状态转换预测的预训练任务，以保持实例级和状态级相似性并捕捉时间动态。

Result: PLanTS在多元分类、预测、轨迹跟踪和异常检测等任务中持续优于现有自监督学习方法，且比基于DTW的方法具有更好的运行时效率。

Conclusion: PLanTS通过显式建模周期性结构和状态转换，显著提升了多元时间序列的表征质量，为处理复杂时间序列数据提供了有效解决方案。

Abstract: Multivariate time series (MTS) are ubiquitous in domains such as healthcare,
climate science, and industrial monitoring, but their high dimensionality,
limited labeled data, and non-stationary nature pose significant challenges for
conventional machine learning methods. While recent self-supervised learning
(SSL) approaches mitigate label scarcity by data augmentations or time
point-based contrastive strategy, they neglect the intrinsic periodic structure
of MTS and fail to capture the dynamic evolution of latent states. We propose
PLanTS, a periodicity-aware self-supervised learning framework that explicitly
models irregular latent states and their transitions. We first designed a
period-aware multi-granularity patching mechanism and a generalized contrastive
loss to preserve both instance-level and state-level similarities across
multiple temporal resolutions. To further capture temporal dynamics, we design
a next-transition prediction pretext task that encourages representations to
encode predictive information about future state evolution. We evaluate PLanTS
across a wide range of downstream tasks-including multi-class and multi-label
classification, forecasting, trajectory tracking and anomaly detection. PLanTS
consistently improves the representation quality over existing SSL methods and
demonstrates superior runtime efficiency compared to DTW-based methods.

</details>


### [77] [STL-based Optimization of Biomolecular Neural Networks for Regression and Control](https://arxiv.org/abs/2509.05481)
*Eric Palanques-Tost,Hanna Krasowski,Murat Arcak,Ron Weiss,Calin Belta*

Main category: cs.LG

TL;DR: 利用信号时序逻辑(STL)规范来训练生物分子神经网络(BNNs)，解决了BNNs因缺乏目标数据而难以训练的问题，实现了在生物系统中的回归和控制任务。


<details>
  <summary>Details</summary>
Motivation: 生物分子神经网络具有超越简单生物电路的通用函数逼近能力，但由于缺乏目标数据，训练BNNs仍然具有挑战性。

Method: 基于STL的定量语义，开发了梯度优化的BNNs权重学习算法，使BNNs能够在生物系统中执行回归和控制任务。

Result: 数值实验表明，基于STL的学习方法能够高效解决所研究的回归和控制任务，包括作为失调状态报告器和慢性疾病模型的反馈控制。

Conclusion: STL规范为BNNs提供了有效的训练目标，使得在没有传统目标数据的情况下也能成功训练BNNs执行复杂的生物功能。

Abstract: Biomolecular Neural Networks (BNNs), artificial neural networks with
biologically synthesizable architectures, achieve universal function
approximation capabilities beyond simple biological circuits. However, training
BNNs remains challenging due to the lack of target data. To address this, we
propose leveraging Signal Temporal Logic (STL) specifications to define
training objectives for BNNs. We build on the quantitative semantics of STL,
enabling gradient-based optimization of the BNN weights, and introduce a
learning algorithm that enables BNNs to perform regression and control tasks in
biological systems. Specifically, we investigate two regression problems in
which we train BNNs to act as reporters of dysregulated states, and a feedback
control problem in which we train the BNN in closed-loop with a chronic disease
model, learning to reduce inflammation while avoiding adverse responses to
external infections. Our numerical experiments demonstrate that STL-based
learning can solve the investigated regression and control tasks efficiently.

</details>


### [78] [Prior Distribution and Model Confidence](https://arxiv.org/abs/2509.05485)
*Maksim Kazanskii,Artem Kasianov*

Main category: cs.LG

TL;DR: 通过分析训练数据在嵌入空间中的分布，提出了一种无需重新训练的方法来评估模型对未见数据的预测信心度，通过过滤低信心度预测显著提高分类准确性。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据分布对图像分类模型性能的影响，并开发一种方法来在不需重新训练的情况下估计模型对未见数据的预测信心度。

Method: 分析训练集在嵌入空间中的表示，基于嵌入空间中与训练分布的距离过滤低信心度的预测。使用多个嵌入模型来表示训练数据，以获得更稳健的信心度估计。

Result: 在多个分类模型上都展示了一致的性能提升，显著提高了分类准确性。结合多个嵌入模型能够更好地检测和排除分布外样本，带来进一步的准确性提升。

Conclusion: 该方法不依赖于特定模型结构，具有良好的可扩展性，不仅适用于计算机视觉领域，还有潜在应用于自然语言处理等需要预测可靠性的领域。

Abstract: This paper investigates the impact of training data distribution on the
performance of image classification models. By analyzing the embeddings of the
training set, we propose a framework to understand the confidence of model
predictions on unseen data without the need for retraining. Our approach
filters out low-confidence predictions based on their distance from the
training distribution in the embedding space, significantly improving
classification accuracy. We demonstrate this on the example of several
classification models, showing consistent performance gains across
architectures. Furthermore, we show that using multiple embedding models to
represent the training data enables a more robust estimation of confidence, as
different embeddings capture complementary aspects of the data. Combining these
embeddings allows for better detection and exclusion of out-of-distribution
samples, resulting in further accuracy improvements. The proposed method is
model-agnostic and generalizable, with potential applications beyond computer
vision, including domains such as Natural Language Processing where prediction
reliability is critical.

</details>


### [79] [Self-Aligned Reward: Towards Effective and Efficient Reasoners](https://arxiv.org/abs/2509.05489)
*Peixuan Han,Adit Krishnan,Gerald Friedland,Jiaxuan You,Chris Kong*

Main category: cs.LG

TL;DR: 本文提出自对齐奖励(SAR)，通过相对困惑度差异来补充可验证奖励，在提高推理准确性的同时减少30%的推理成本，实现了正确性和效率的帕累托最优权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的可验证奖励信号过于粗糙，只提供二元正确性反馈，导致推理过程冗长、计算成本高，且现有解决方案往往牺牲准确性。

Method: 引入自对齐奖励(SAR)，定义为基于查询的答案与独立答案之间的相对困惑度差异，偏好简洁且查询特定的响应。

Result: 在7个基准测试的4个模型上，SAR与PPO和GRPO等RL算法结合使用，准确率提高4%，推理成本降低30%，实现了正确性和效率的帕累托最优权衡。

Conclusion: SAR作为可验证奖励的细粒度补充，能够在不丢失关键推理的情况下抑制不必要的阐述，为更高效有效的LLM训练铺平了道路。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced
reasoning in large language models (LLMs), but such signals remain coarse,
offering only binary correctness feedback. This limitation often results in
inefficiencies, including overly verbose reasoning and high computational cost,
while existing solutions often compromise accuracy. To address this, we
introduce self-aligned reward (SAR), a self-guided signal that complements
verifiable rewards to encourage both reasoning accuracy and efficiency. SAR is
defined as the relative perplexity difference between an answer conditioned on
the query and the standalone answer, thereby favoring responses that are
concise and query-specific. Quantitative analysis reveals that SAR reliably
distinguishes answer quality: concise, correct answers score higher than
redundant ones, and partially correct answers score higher than entirely
incorrect ones. Evaluation on 4 models across 7 benchmarks shows that
integrating SAR with prevalent RL algorithms like PPO and GRPO improves
accuracy by 4%, while reducing inference cost by 30%. Further analysis
demonstrates that SAR achieves a Pareto-optimal trade-off between correctness
and efficiency compared to reward signals based on length or self-confidence.
We also show that SAR shortens responses while preserving advanced reasoning
behaviors, demonstrating its ability to suppress unnecessary elaboration
without losing critical reasoning. These results highlight the promise of
self-aligned reward as a fine-grained complement to verifiable rewards, paving
the way for more efficient and effective LLM training.

</details>


### [80] [Distributed Deep Learning using Stochastic Gradient Staleness](https://arxiv.org/abs/2509.05679)
*Viet Hoang Pham,Hyo-Sung Ahn*

Main category: cs.LG

TL;DR: 这篇论文提出了一种集成数据并行和完全解耦并行反向传播算法的分布式训练方法，以加速深度神经网络的训练过程。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练需要大量时间，特别是当网络深度增加和数据集扩大时。传统的反向传播算法存在锁定问题，限制了训练效率。

Method: 采用多个计算单元并行运行，结合数据并行和完全解耦并行反向传播算法。通过这种方式增加每迭代处理的训练数据量，同时减少锁定问题。

Result: 计算理论证明该方法在某些条件下能够收敛到关键点。在CIFAR-10数据集上的分类任务中进行了实验验证，证明了方法的有效性。

Conclusion: 该分布式训练方法能够显著提高深度神经网络的训练效率，解决了传统训练方法中的锁定问题和训练速度缺陷。

Abstract: Despite the notable success of deep neural networks (DNNs) in solving complex
tasks, the training process still remains considerable challenges. A primary
obstacle is the substantial time required for training, particularly as high
performing DNNs tend to become increasingly deep (characterized by a larger
number of hidden layers) and require extensive training datasets. To address
these challenges, this paper introduces a distributed training method that
integrates two prominent strategies for accelerating deep learning: data
parallelism and fully decoupled parallel backpropagation algorithm. By
utilizing multiple computational units operating in parallel, the proposed
approach enhances the amount of training data processed in each iteration while
mitigating locking issues commonly associated with the backpropagation
algorithm. These features collectively contribute to significant improvements
in training efficiency. The proposed distributed training method is rigorously
proven to converge to critical points under certain conditions. Its
effectiveness is further demonstrated through empirical evaluations, wherein an
DNN is trained to perform classification tasks on the CIFAR-10 dataset.

</details>


### [81] [DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training](https://arxiv.org/abs/2509.05542)
*Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DreamPRM-1.5是一个实例重加权框架，通过双层优化自适应调整训练样本重要性，解决了多模态过程奖励模型训练中的分布偏移和噪声数据问题


<details>
  <summary>Details</summary>
Motivation: 多模态过程奖励模型(PRMs)训练面临分布偏移和噪声数据的挑战，需要开发能够自适应处理这些问题的框架

Method: 提出实例重加权框架，采用双层优化自适应调整训练样本重要性；设计了两种互补策略：Instance Table（适用于小数据集）和Instance Net（适用于大数据集）；集成到测试时缩放中

Result: 在MMMU基准测试上达到84.6%的准确率，超越了GPT-5的表现

Conclusion: DreamPRM-1.5通过创新的实例重加权方法有效解决了多模态PRM训练中的关键挑战，在性能上实现了显著提升

Abstract: Training multimodal process reward models (PRMs) is challenged by
distribution shifts and noisy data. We introduce DreamPRM-1.5, an
instance-reweighted framework that adaptively adjusts the importance of each
training example via bi-level optimization. We design two complementary
strategies: Instance Table, effective for smaller datasets, and Instance Net,
scalable to larger ones. Integrated into test-time scaling, DreamPRM-1.5
achieves 84.6 accuracy on the MMMU benchmark, surpassing GPT-5.

</details>


### [82] [Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing](https://arxiv.org/abs/2509.06552)
*Zheqi Lv,Wenqiao Zhang,Kairui Fu,Qi Tian,Shengyu Zhang,Jiajie Su,Jingyuan Chen,Kun Kuang,Fei Wu*

Main category: cs.LG

TL;DR: Persona是一个无需反向传播的参数编辑框架，通过云端神经适配器生成参数编辑矩阵，动态调整设备端模型以适应实时数据分布变化，无需部署后重新训练。


<details>
  <summary>Details</summary>
Motivation: 设备端实时数据分布变化挑战轻量级模型的泛化能力，当前研究主要依赖数据密集和计算昂贵的微调方法，忽视了这一关键问题。

Method: 使用基于原型的参数编辑框架，通过云端神经适配器根据实时设备数据生成参数编辑矩阵，将设备端模型聚类为原型模型，并通过跨层知识转移实现多层参数一致性调整。

Result: 在多个数据集的视觉任务和推荐任务上的广泛实验证实了Persona的有效性和通用性。

Conclusion: Persona提供了一种高效的后部署模型个性化方法，能够有效应对设备端实时数据分布变化，提升模型泛化能力。

Abstract: The on-device real-time data distribution shift on devices challenges the
generalization of lightweight on-device models. This critical issue is often
overlooked in current research, which predominantly relies on data-intensive
and computationally expensive fine-tuning approaches. To tackle this, we
introduce Persona, a novel personalized method using a prototype-based,
backpropagation-free parameter editing framework to enhance model
generalization without post-deployment retraining. Persona employs a neural
adapter in the cloud to generate a parameter editing matrix based on real-time
device data. This matrix adeptly adapts on-device models to the prevailing data
distributions, efficiently clustering them into prototype models. The
prototypes are dynamically refined via the parameter editing matrix,
facilitating efficient evolution. Furthermore, the integration of cross-layer
knowledge transfer ensures consistent and context-aware multi-layer parameter
changes and prototype assignment. Extensive experiments on vision task and
recommendation task on multiple datasets confirm Persona's effectiveness and
generality.

</details>


### [83] [Reinforcement Learning with Anticipation: A Hierarchical Approach for Long-Horizon Tasks](https://arxiv.org/abs/2509.05545)
*Yang Yu*

Main category: cs.LG

TL;DR: RLA（强化学习与预期）框架通过分层策略和预期模型解决长时域目标条件任务，使用价值几何一致性原则训练，具有理论保证和收敛性


<details>
  <summary>Details</summary>
Motivation: 解决分层强化学习在长时域目标条件任务中自动发现层次结构和多级策略联合训练的不稳定性问题，以及缺乏理论保证的局限性

Method: 学习两个协同模型：低层目标条件策略（学习到达指定子目标）和高层预期模型（作为规划器提出中间子目标），通过价值几何一致性原则训练预期模型

Result: RLA在各种条件下能够逼近全局最优策略，为长时域目标条件任务中的分层规划和执行提供了原则性和收敛性方法

Conclusion: RLA是一个有原则且可扩展的框架，通过预期模型和价值几何一致性正则化，有效解决了分层强化学习在长时域任务中的挑战

Abstract: Solving long-horizon goal-conditioned tasks remains a significant challenge
in reinforcement learning (RL). Hierarchical reinforcement learning (HRL)
addresses this by decomposing tasks into more manageable sub-tasks, but the
automatic discovery of the hierarchy and the joint training of multi-level
policies often suffer from instability and can lack theoretical guarantees. In
this paper, we introduce Reinforcement Learning with Anticipation (RLA), a
principled and potentially scalable framework designed to address these
limitations. The RLA agent learns two synergistic models: a low-level,
goal-conditioned policy that learns to reach specified subgoals, and a
high-level anticipation model that functions as a planner, proposing
intermediate subgoals on the optimal path to a final goal. The key feature of
RLA is the training of the anticipation model, which is guided by a principle
of value geometric consistency, regularized to prevent degenerate solutions. We
present proofs that RLA approaches the globally optimal policy under various
conditions, establishing a principled and convergent method for hierarchical
planning and execution in long-horizon goal-conditioned tasks.

</details>


### [84] [ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization](https://arxiv.org/abs/2509.05584)
*Sadegh Jafari,Aishwarya Sarkar,Mohiuddin Bilwal,Ali Jannesari*

Main category: cs.LG

TL;DR: ProfilingAgent是一个基于LLM的多智能体系统，通过分析静态指标和动态性能信号，为不同架构的模型自动设计剪枝和量化策略，在保持精度的同时显著减少内存使用和提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 基础模型面临计算和内存瓶颈，传统压缩技术使用统一启发式方法，忽略了架构和运行时异质性，需要更智能的自动化压缩方案。

Method: 提出ProfilingAgent系统，利用LLM分析静态指标(MACs、参数量)和动态信号(延迟、内存)，为不同层设计定制化的结构化剪枝和动态量化策略。

Result: 在ImageNet-1K等数据集上，剪枝保持或提升精度(ImageNet-1K精度下降约1%，ViT-B/16在小数据集上提升2%)，量化实现74%内存节省和<0.5%精度损失，推理速度提升1.74倍。

Conclusion: 智能体系统是profiling引导模型优化的可扩展解决方案，LLM推理质量对迭代剪枝至关重要。

Abstract: Foundation models face growing compute and memory bottlenecks, hindering
deployment on resource-limited platforms. While compression techniques such as
pruning and quantization are widely used, most rely on uniform heuristics that
ignore architectural and runtime heterogeneity. Profiling tools expose
per-layer latency, memory, and compute cost, yet are rarely integrated into
automated pipelines. We propose ProfilingAgent, a profiling-guided, agentic
approach that uses large language models (LLMs) to automate compression via
structured pruning and post-training dynamic quantization. Our modular
multi-agent system reasons over static metrics (MACs, parameter counts) and
dynamic signals (latency, memory) to design architecture-specific strategies.
Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to
bottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with
ResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive
or improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on
smaller datasets), while quantization achieves up to 74% memory savings with
<0.5% accuracy loss. Our quantization also yields consistent inference speedups
of up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo
highlight the importance of LLM reasoning quality for iterative pruning. These
results establish agentic systems as scalable solutions for profiling-guided
model optimization.

</details>


### [85] [Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2509.05615)
*Xiaoguang Zhu,Lianlong Sun,Yang Liu,Pengyi Jiang,Uma Srivatsa,Nipavan Chiamvimonvat,Vladimir Filkov*

Main category: cs.LG

TL;DR: 提出一个因果分析框架来解决医学多模态学习中的缺失模态偏差问题，通过反事实干预和双重分支网络来消除数据获取过程中的偏差。


<details>
  <summary>Details</summary>
Motivation: 现实医学数据集中常存在模态缺失问题，现有方法忽略了数据获取过程引入的偏差，包括缺失性偏差和分布偏差，这阻碍了模型的泛化能力。

Method: 基于结构因果分析，提出包含缺失性去混淆模块（基于后门调整近似因果干预）和双重分支神经网络（显式分离因果特征与伪相关）的统一框架。

Result: 在真实世界公共数据集和院内数据集上验证了方法的有效性，证明了其因果洞察力。

Conclusion: 该框架能够有效解决医学多模态表示学习中的偏差问题，提升模型的泛化性能，为医学数据挖掘提供了新的因果分析视角。

Abstract: Medical multimodal representation learning aims to integrate heterogeneous
clinical data into unified patient representations to support predictive
modeling, which remains an essential yet challenging task in the medical data
mining community. However, real-world medical datasets often suffer from
missing modalities due to cost, protocol, or patient-specific constraints.
Existing methods primarily address this issue by learning from the available
observations in either the raw data space or feature space, but typically
neglect the underlying bias introduced by the data acquisition process itself.
In this work, we identify two types of biases that hinder model generalization:
missingness bias, which results from non-random patterns in modality
availability, and distribution bias, which arises from latent confounders that
influence both observed features and outcomes. To address these challenges, we
perform a structural causal analysis of the data-generating process and propose
a unified framework that is compatible with existing direct prediction-based
multimodal learning methods. Our method consists of two key components: (1) a
missingness deconfounding module that approximates causal intervention based on
backdoor adjustment and (2) a dual-branch neural network that explicitly
disentangles causal features from spurious correlations. We evaluated our
method in real-world public and in-hospital datasets, demonstrating its
effectiveness and causal insights.

</details>


### [86] [OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search](https://arxiv.org/abs/2509.05656)
*Bo Lyu,Yu Cui,Tuo Shi,Ke Li*

Main category: cs.LG

TL;DR: OptiProxy-NAS是一个端到端的神经架构搜索框架，通过代理表示将离散的NAS空间转化为连续、可微且平滑的优化问题，从而可以使用梯度优化方法进行高效搜索。


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索(NAS)是一个计算昂贵的离散优化问题，现有方法如基于预测器的方法和可微分架构搜索存在局限性。作者希望开发一种更高效的端到端优化框架来加速NAS过程。

Method: 提出OptiProxy-NAS框架，使用代理表示将离散的NAS空间重新表述为连续、可微且平滑的优化问题，从而可以直接应用梯度优化方法搜索架构参数。

Result: 在12个NAS任务、4个搜索空间和三个不同领域（计算机视觉、自然语言处理、资源受限NAS）的全面实验中，该方法展现了优越的搜索结果和效率，在低保真度场景下也验证了其灵活性。

Conclusion: OptiProxy-NAS通过将NAS问题转化为连续优化问题，提供了一种高效且灵活的神经架构搜索解决方案，在多个领域和场景下都表现出色。

Abstract: Neural architecture search (NAS) is a hard computationally expensive
optimization problem with a discrete, vast, and spiky search space. One of the
key research efforts dedicated to this space focuses on accelerating NAS via
certain proxy evaluations of neural architectures. Different from the prevalent
predictor-based methods using surrogate models and differentiable architecture
search via supernetworks, we propose an optimization proxy to streamline the
NAS as an end-to-end optimization framework, named OptiProxy-NAS. In
particular, using a proxy representation, the NAS space is reformulated to be
continuous, differentiable, and smooth. Thereby, any differentiable
optimization method can be applied to the gradient-based search of the relaxed
architecture parameters. Our comprehensive experiments on $12$ NAS tasks of $4$
search spaces across three different domains including computer vision, natural
language processing, and resource-constrained NAS fully demonstrate the
superior search results and efficiency. Further experiments on low-fidelity
scenarios verify the flexibility.

</details>


### [87] [DQS: A Low-Budget Query Strategy for Enhancing Unsupervised Data-driven Anomaly Detection Approaches](https://arxiv.org/abs/2509.05663)
*Lucas Correia,Jan-Christoph Goos,Thomas Bäck,Anna V. Kononova*

Main category: cs.LG

TL;DR: 提出了一种基于主动学习的无监督时间序列异常检测方法，通过DQS查询策略选择多样化样本进行标签查询，优化阈值选择，在小样本预算下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有无监督时间序列异常检测方法存在阈值设置不佳的问题，而声称无监督的方法实际上需要标注数据进行校准，这在现实世界中往往不可得。

Method: 将主动学习集成到现有无监督异常检测方法中，通过新颖的DQS（基于差异的查询策略）选择多变量时间序列标签进行查询，使用动态时间规整评估异常分数相似性来最大化查询样本的多样性。

Result: DQS在小预算场景下表现最佳，但其他策略在面对错误标注时更鲁棒。所有查询策略即使在存在错误标注的情况下都优于无监督阈值。

Conclusion: 在实际应用中，查询策略的选择取决于标注专家的专业水平和愿意标注的样本数量。只要能够查询专家，推荐使用基于主动学习的阈值方法。

Abstract: Truly unsupervised approaches for time series anomaly detection are rare in
the literature. Those that exist suffer from a poorly set threshold, which
hampers detection performance, while others, despite claiming to be
unsupervised, need to be calibrated using a labelled data subset, which is
often not available in the real world. This work integrates active learning
with an existing unsupervised anomaly detection method by selectively querying
the labels of multivariate time series, which are then used to refine the
threshold selection process. To achieve this, we introduce a novel query
strategy called the dissimilarity-based query strategy (DQS). DQS aims to
maximise the diversity of queried samples by evaluating the similarity between
anomaly scores using dynamic time warping. We assess the detection performance
of DQS in comparison to other query strategies and explore the impact of
mislabelling, a topic that is underexplored in the literature. Our findings
indicate that DQS performs best in small-budget scenarios, though the others
appear to be more robust when faced with mislabelling. Therefore, in the real
world, the choice of query strategy depends on the expertise of the oracle and
the number of samples they are willing to label. Regardless, all query
strategies outperform the unsupervised threshold even in the presence of
mislabelling. Thus, whenever it is feasible to query an oracle, employing an
active learning-based threshold is recommended.

</details>


### [88] [GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR](https://arxiv.org/abs/2509.05671)
*Labani Halder,Tanmay Sen,Sarbani Palit*

Main category: cs.LG

TL;DR: 提出基于图神经网络的多模态联邦学习框架GraMFedDHAR，用于解决多模态传感器数据的人体活动识别问题，在差分隐私条件下显著提升性能


<details>
  <summary>Details</summary>
Motivation: 解决多模态传感器数据的噪声/不完整性、标签数据稀缺、隐私问题，以及传统联邦学习在异构多模态数据和差分隐私要求下的挑战

Method: 将多种传感器数据模型化为模态特异性图表示，通过殊差图卷积神经网络处理，采用注意力机制融合而非简单拼接，结合差分隐私保护

Result: 在非DP设置下比基准模型准确率提升2%，在差分隐私条件下性能优势更显著（提升7-13%），显示图神经网络对DP噪声具有更好耐受性

Conclusion: 图基形式模型在多模态学习中显示出良好的稳健性，尤其适用于差分隐私保护的联邦学习场景

Abstract: Human Activity Recognition (HAR) using multimodal sensor data remains
challenging due to noisy or incomplete measurements, scarcity of labeled
examples, and privacy concerns. Traditional centralized deep learning
approaches are often constrained by infrastructure availability, network
latency, and data sharing restrictions. While federated learning (FL) addresses
privacy by training models locally and sharing only model parameters, it still
has to tackle issues arising from the use of heterogeneous multimodal data and
differential privacy requirements. In this article, a Graph-based Multimodal
Federated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse
sensor streams such as a pressure mat, depth camera, and multiple
accelerometers are modeled as modality-specific graphs, processed through
residual Graph Convolutional Neural Networks (GCNs), and fused via
attention-based weighting rather than simple concatenation. The fused
embeddings enable robust activity classification, while differential privacy
safeguards data during federated aggregation. Experimental results show that
the proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with
up to 2 percent higher accuracy in non-DP settings in both centralized and
federated paradigms. More importantly, significant improvements are observed
under differential privacy constraints: MultiModalGCN consistently surpasses
MultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on
the privacy budget and setting. These results highlight the robustness of
graph-based modeling in multimodal learning, where GNNs prove more resilient to
the performance degradation introduced by DP noise.

</details>


### [89] [Morphological Perceptron with Competitive Layer: Training Using Convex-Concave Procedure](https://arxiv.org/abs/2509.05697)
*Iara Cunha,Marcos Eduardo Valle*

Main category: cs.LG

TL;DR: 本文提出使用凸凹过程(CCP)来训练具有竞争层的形态感知器(MPCL)网络，解决了形态操作不可微分导致的梯度优化方法不适用的问题。


<details>
  <summary>Details</summary>
Motivation: 形态感知器中的形态操作具有不可微分性，使得基于梯度的优化方法无法用于训练这类网络，需要寻找不依赖梯度信息的替代策略。

Method: 将训练问题表述为凸函数之差(DC)形式，使用凸凹过程(CCP)进行迭代求解，产生一系列线性规划子问题。

Result: 计算实验证明了所提出的训练方法在处理MPCL网络分类任务方面的有效性。

Conclusion: CCP方法为训练不可微分的形态感知器网络提供了一种有效的替代方案，能够成功解决多类分类任务。

Abstract: A morphological perceptron is a multilayer feedforward neural network in
which neurons perform elementary operations from mathematical morphology. For
multiclass classification tasks, a morphological perceptron with a competitive
layer (MPCL) is obtained by integrating a winner-take-all output layer into the
standard morphological architecture. The non-differentiability of morphological
operators renders gradient-based optimization methods unsuitable for training
such networks. Consequently, alternative strategies that do not depend on
gradient information are commonly adopted. This paper proposes the use of the
convex-concave procedure (CCP) for training MPCL networks. The training problem
is formulated as a difference of convex (DC) functions and solved iteratively
using CCP, resulting in a sequence of linear programming subproblems.
Computational experiments demonstrate the effectiveness of the proposed
training method in addressing classification tasks with MPCL networks.

</details>


### [90] [Simulation Priors for Data-Efficient Deep Learning](https://arxiv.org/abs/2509.05732)
*Lenart Treven,Bhavya Sukhija,Jonas Rothfuss,Stelian Coros,Florian Dörfler,Andreas Krause*

Main category: cs.LG

TL;DR: SimPEL是一种将第一性原理模型与贝叶斯深度学习相结合的方法，使用低保真模拟器作为先验，在数据稀缺时利用模拟器知识，在数据充足时发挥深度学习的灵活性，同时量化认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统在现实世界中高效学习的问题。第一性原理模型因简化假设而无法捕捉现实复杂性，而深度学习方法需要大量代表性数据。

Method: 提出SimPEL方法，将低保真模拟器作为贝叶斯深度学习的先验，结合第一性原理模型和数据驱动学习，同时量化认知不确定性。

Result: 在生物、农业和机器人等多个领域展示出优越性能，在高速RC汽车任务中以显著少于现有基线方法的数据量学会了高度动态的漂移停车动作。

Conclusion: SimPEL在复杂现实环境中具有数据高效学习和控制的潜力，能够弥合基于模型的强化学习中的模拟到现实差距。

Abstract: How do we enable AI systems to efficiently learn in the real-world?
First-principles models are widely used to simulate natural systems, but often
fail to capture real-world complexity due to simplifying assumptions. In
contrast, deep learning approaches can estimate complex dynamics with minimal
assumptions but require large, representative datasets. We propose SimPEL, a
method that efficiently combines first-principles models with data-driven
learning by using low-fidelity simulators as priors in Bayesian deep learning.
This enables SimPEL to benefit from simulator knowledge in low-data regimes and
leverage deep learning's flexibility when more data is available, all the while
carefully quantifying epistemic uncertainty. We evaluate SimPEL on diverse
systems, including biological, agricultural, and robotic domains, showing
superior performance in learning complex dynamics. For decision-making, we
demonstrate that SimPEL bridges the sim-to-real gap in model-based
reinforcement learning. On a high-speed RC car task, SimPEL learns a highly
dynamic parking maneuver involving drifting with substantially less data than
state-of-the-art baselines. These results highlight the potential of SimPEL for
data-efficient learning and control in complex real-world environments.

</details>


### [91] [Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies](https://arxiv.org/abs/2509.05735)
*Jiaqi Chen,Ji Shi,Cansu Sancaktar,Jonas Frey,Georg Martius*

Main category: cs.LG

TL;DR: 模型基于强化学习中，在线训练比离线训练表现更好。离线模型遇到分布外状态问题，通过添加在线交互或探索数据可以缓解。建议收集更多探索数据而非仅专家数据。


<details>
  <summary>Details</summary>
Motivation: 研究模型基于强化学习中在线与离线数据收集策略对世界模型和任务性能的影响，这在现有文献中缺乏深入研究。

Method: 在31个不同环境中进行实验对比，分析在线和离线训练的性能差异，识别离线模型的分布外状态问题，并测试通过添加在线交互和探索数据来缓解这些问题。

Result: 在线模型表现优于离线模型。离线模型遇到分布外状态问题，导致想象与实际执行的不匹配。添加在线交互或探索数据可以有效恢复性能。

Conclusion: 建议在收集大规模数据集时添加探索数据，而非仅依赖专家数据，以提高模型在实际应用中的性能和稳定性。

Abstract: Data collection is crucial for learning robust world models in model-based
reinforcement learning. The most prevalent strategies are to actively collect
trajectories by interacting with the environment during online training or
training on offline datasets. At first glance, the nature of learning
task-agnostic environment dynamics makes world models a good candidate for
effective offline training. However, the effects of online vs. offline data on
world models and thus on the resulting task performance have not been
thoroughly studied in the literature. In this work, we investigate both
paradigms in model-based settings, conducting experiments on 31 different
environments. First, we showcase that online agents outperform their offline
counterparts. We identify a key challenge behind performance degradation of
offline agents: encountering Out-Of-Distribution states at test time. This
issue arises because, without the self-correction mechanism in online agents,
offline datasets with limited state space coverage induce a mismatch between
the agent's imagination and real rollouts, compromising policy training. We
demonstrate that this issue can be mitigated by allowing for additional online
interactions in a fixed or adaptive schedule, restoring the performance of
online training with limited interaction data. We also showcase that
incorporating exploration data helps mitigate the performance degradation of
offline agents. Based on our insights, we recommend adding exploration data
when collecting large datasets, as current efforts predominantly focus on
expert data alone.

</details>


### [92] [Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders](https://arxiv.org/abs/2509.05766)
*Jiaju Miao,Wei Zhu*

Main category: cs.LG

TL;DR: 基于PRC随机森林与自编码器的混合框架，通过学习缩结潜表征来同时解决簇簇类不平衡和维度灾难问题，在异常检测任务中实现了更高的准确性、可扩展性和可解释性


<details>
  <summary>Details</summary>
Motivation: 异常检测在网络安全、入侵检测和欺诈防范等关键应用中至关重要，但经常遇到极端簇簇不平衡和维度灾难两大挑战

Method: 提出一种混合框架，将PRC随机森林(PRC-RF)与自编码器结合，利用自编码器学习缩结潜表征来应对维度灾难

Result: 在多个标准数据集上进行了涉及范围广泛的实验，结果显示Autoencoder-PRC-RF模型相比之前方法具有更优异的准确性、可扩展性和可解释性

Conclusion: 该混合框架在高风险异常检测任务中展现出强大潜力，能够同时有效解决簇簇不平衡和维度灾难两大挑战

Abstract: Anomaly detection underpins critical applications from network security and
intrusion detection to fraud prevention, where recognizing aberrant patterns
rapidly is indispensable. Progress in this area is routinely impeded by two
obstacles: extreme class imbalance and the curse of dimensionality. To combat
the former, we previously introduced Precision-Recall Curve (PRC)
classification trees and their ensemble extension, the PRC Random Forest
(PRC-RF). Building on that foundation, we now propose a hybrid framework that
integrates PRC-RF with autoencoders, unsupervised machine learning methods that
learn compact latent representations, to confront both challenges
simultaneously. Extensive experiments across diverse benchmark datasets
demonstrate that the resulting Autoencoder-PRC-RF model achieves superior
accuracy, scalability, and interpretability relative to prior methods,
affirming its potential for high-stakes anomaly-detection tasks.

</details>


### [93] [Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting](https://arxiv.org/abs/2509.05768)
*Chen Shao,Yue Wang,Zhenyi Zhu,Zhanbo Huang,Sebastian Pütz,Benjamin Schäfer,Tobais Käfer,Michael Färber*

Main category: cs.LG

TL;DR: 提出了Real-E数据集，涵盖30+欧洲国家74个发电站10年数据，包含丰富元数据。基准测试显示现有方法在该复杂非平稳数据集上表现不佳，提出了新指标量化相关性结构变化。


<details>
  <summary>Details</summary>
Motivation: 现有能源预测基准在时空范围和多能源特征方面有限，影响其在真实世界部署的可靠性和适用性。

Method: 构建Real-E数据集，进行广泛数据分析，基准测试20多种基线模型，引入新指标量化相关性结构变化。

Result: 现有方法在Real-E数据集上表现不佳，该数据集展现出更复杂和非平稳的相关性动态。

Conclusion: 研究揭示了当前方法的关键局限性，为构建更鲁棒的预测模型提供了强有力的实证基础。

Abstract: Energy forecasting is vital for grid reliability and operational efficiency.
Although recent advances in time series forecasting have led to progress,
existing benchmarks remain limited in spatial and temporal scope and lack
multi-energy features. This raises concerns about their reliability and
applicability in real-world deployment. To address this, we present the Real-E
dataset, covering over 74 power stations across 30+ European countries over a
10-year span with rich metadata. Using Real- E, we conduct an extensive data
analysis and benchmark over 20 baselines across various model types. We
introduce a new metric to quantify shifts in correlation structures and show
that existing methods struggle on our dataset, which exhibits more complex and
non-stationary correlation dynamics. Our findings highlight key limitations of
current methods and offer a strong empirical basis for building more robust
forecasting models

</details>


### [94] [DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection](https://arxiv.org/abs/2509.05778)
*Arantxa Urrea-Castaño,Nicolás Segura-Kunsagi,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.LG

TL;DR: 本文提出了一种双重交叉验证框架DCV-ROOD，用于更稳健地评估分布外检测模型的性能，通过分别处理分布内和分布外数据的特性，并考虑类别层次结构来实现更公平的数据分割。


<details>
  <summary>Details</summary>
Motivation: 分布外(OOD)检测对于提高AI系统的稳健性至关重要，但当前缺乏有效的评估框架来验证这些检测方法的性能。虽然交叉验证在预测学习算法性能方面很有效，但需要适应急需修改才能适用于OOD检测场景。

Method: 提出双重交叉验证框架DCV-ROOD，对分布内(ID)数据采用标准分割方式，对分布外(OOD)数据则按类别分组进行分割。还考虑类别层次结构，提出了考虑全层次结构的数据分割方法，以获得公平的ID-OOD分区。

Result: 通过对多个独立数据集进行实验，验证了DCV-ROOD框架的有效性。结果显示，该方法能够很快收敛到真实性能值，证明了其在评估OOD检测模型时的可靠性。

Conclusion: DCV-ROOD框架为OOD检测方法提供了一种稳健、可靠的评估方法，能够更准确地估计模型的真实性能。这个框架考虑了ID和OOD数据的不同特性，并通过类别层次数据分割确保了评估的公平性，为进一步研究和实践提供了有力的评估工具。

Abstract: Out-of-distribution (OOD) detection plays a key role in enhancing the
robustness of artificial intelligence systems by identifying inputs that differ
significantly from the training distribution, thereby preventing unreliable
predictions and enabling appropriate fallback mechanisms. Developing reliable
OOD detection methods is a significant challenge, and rigorous evaluation of
these techniques is essential for ensuring their effectiveness, as it allows
researchers to assess their performance under diverse conditions and to
identify potential limitations or failure modes. Cross-validation (CV) has
proven to be a highly effective tool for providing a reasonable estimate of the
performance of a learning algorithm. Although OOD scenarios exhibit particular
characteristics, an appropriate adaptation of CV can lead to a suitable
evaluation framework for this setting. This work proposes a dual CV framework
for robust evaluation of OOD detection models, aimed at improving the
reliability of their assessment. The proposed evaluation framework aims to
effectively integrate in-distribution (ID) and OOD data while accounting for
their differing characteristics. To achieve this, ID data are partitioned using
a conventional approach, whereas OOD data are divided by grouping samples based
on their classes. Furthermore, we analyze the context of data with class
hierarchy to propose a data splitting that considers the entire class hierarchy
to obtain fair ID-OOD partitions to apply the proposed evaluation framework.
This framework is called Dual Cross-Validation for Robust Out-of-Distribution
Detection (DCV-ROOD). To test the validity of the evaluation framework, we
selected a set of state-of-the-art OOD detection methods, both with and without
outlier exposure. The results show that the method achieves very fast
convergence to the true performance.

</details>


### [95] [Select, then Balance: A Plug-and-Play Framework for Exogenous-Aware Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.05779)
*Wei Chen,Yuqian Wu,Yuanshao Zhu,Xixuan Hao,Shiyu Wang,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出ExoST框架，通过"选择-平衡"范式建模外生变量在时空预测中的作用，解决外生变量影响不一致和历史/未来变量不平衡问题


<details>
  <summary>Details</summary>
Motivation: 现有时空预测方法仅使用有限观测目标变量，而现实中外生变量可作为额外输入特征提升预测精度，但面临不同外生变量影响不一致以及历史与未来变量影响不平衡的挑战

Method: 构建潜在空间门控专家模块，将融合的外生信息投影到潜在空间动态选择和重组显著信号；设计孪生网络架构，将重组后的过去和未来外生变量表示输入双分支时空骨干网络捕获动态模式；通过上下文感知加权机制实现建模过程中的动态平衡

Result: 在真实世界数据集上的大量实验证明了所提框架的有效性、通用性、鲁棒性和效率

Conclusion: ExoST框架通过创新的"选择-平衡"范式成功解决了外生变量建模中的关键挑战，为时空预测提供了有效的解决方案

Abstract: Spatio-temporal forecasting aims to predict the future state of dynamic
systems and plays an important role in multiple fields. However, existing
solutions only focus on modeling using a limited number of observed target
variables. In real-world scenarios, exogenous variables can be integrated into
the model as additional input features and associated with the target signal to
promote forecast accuracy. Although promising, this still encounters two
challenges: the inconsistent effects of different exogenous variables to the
target system, and the imbalance effects between historical variables and
future variables. To address these challenges, this paper introduces \model, a
novel framework for modeling \underline{exo}genous variables in
\underline{s}patio-\underline{t}emporal forecasting, which follows a ``select,
then balance'' paradigm. Specifically, we first construct a latent space gated
expert module, where fused exogenous information is projected into a latent
space to dynamically select and recompose salient signals via specialized
sub-experts. Furthermore, we design a siamese network architecture in which
recomposed representations of past and future exogenous variables are fed into
dual-branch spatio-temporal backbones to capture dynamic patterns. The outputs
are integrated through a context-aware weighting mechanism to achieve dynamic
balance during the modeling process. Extensive experiments on real-world
datasets demonstrate the effectiveness, generality, robustness, and efficiency
of our proposed framework.

</details>


### [96] [time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models](https://arxiv.org/abs/2509.05801)
*Debdeep Sanyal,Aaryan Nagpal,Dhruv Kumar,Murari Mandal,Saurabh Deshpande*

Main category: cs.LG

TL;DR: 这篇论文通过激活移植技术证明了基础模型内部存在可控制的语义表征空间，能够模拟市场崩溃等突发事件，推动时间序列预测从后解释向直接因果干预转变。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型是否真正内部化了语义概念（如市场治理），以及能否利用其内部表征来模拟突发急震事件。

Method: 提出激活移植技术，通过在模型前向传播过程中将某事件（如历史崩溃）的统计矩征强制注入到另一个事件（如平静期）的隐藏状态中。

Result: 注入崩溃语义会导致下跌预测，注入平静语义则可压制崩溃并恢复稳定性。模型编码了事件严重程度的等级概念，潜在向量的模长与系统性震荡的强度直接相关。

Conclusion: 可控制的、语义基础的表征是大型时间序列Transformer模型的稳健特性，为战略性压力测试提供了语义"假如"分析能力。

Abstract: While transformer-based foundation models excel at forecasting routine
patterns, two questions remain: do they internalize semantic concepts such as
market regimes, or merely fit curves? And can their internal representations be
leveraged to simulate rare, high-stakes events such as market crashes? To
investigate this, we introduce activation transplantation, a causal
intervention that manipulates hidden states by imposing the statistical moments
of one event (e.g., a historical crash) onto another (e.g., a calm period)
during the forward pass. This procedure deterministically steers forecasts:
injecting crash semantics induces downturn predictions, while injecting calm
semantics suppresses crashes and restores stability. Beyond binary control, we
find that models encode a graded notion of event severity, with the latent
vector norm directly correlating with the magnitude of systemic shocks.
Validated across two architecturally distinct TSFMs, Toto (decoder only) and
Chronos (encoder-decoder), our results demonstrate that steerable, semantically
grounded representations are a robust property of large time series
transformers. Our findings provide evidence for a latent concept space that
governs model predictions, shifting interpretability from post-hoc attribution
to direct causal intervention, and enabling semantic "what-if" analysis for
strategic stress-testing.

</details>


### [97] [Simple Optimizers for Convex Aligned Multi-Objective Optimization](https://arxiv.org/abs/2509.05811)
*Ben Kretzu,Karen Ullrich,Yonathan Efroni*

Main category: cs.LG

TL;DR: 本文放宽了对齐多目标优化(AMOO)中的强凸性假设，在更符合深度学习实践的平滑性或Lipschitz连续性条件下，提出了可扩展的梯度下降算法并建立了收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现有AMOO框架的分析依赖于强凸性假设，这与深度学习实践中的标准假设不符。本文旨在在更一般的凸性条件下研究AMOO问题。

Method: 开发新的分析工具和度量标准，提出可扩展的梯度下降算法用于凸AMOO问题，并建立收敛理论保证。

Result: 证明了算法在标准平滑性或Lipschitz连续性条件下的收敛性，并证明了朴素等权重方法的次优性下界。

Conclusion: 本文为凸AMOO问题提供了更实用的理论框架和算法，扩展了AMOO在深度学习等实际应用中的适用性。

Abstract: It is widely recognized in modern machine learning practice that access to a
diverse set of tasks can enhance performance across those tasks. This
observation suggests that, unlike in general multi-objective optimization, the
objectives in many real-world settings may not be inherently conflicting. To
address this, prior work introduced the Aligned Multi-Objective Optimization
(AMOO) framework and proposed gradient-based algorithms with provable
convergence guarantees. However, existing analysis relies on strong
assumptions, particularly strong convexity, which implies the existence of a
unique optimal solution. In this work, we relax this assumption and study
gradient-descent algorithms for convex AMOO under standard smoothness or
Lipschitz continuity conditions-assumptions more consistent with those used in
deep learning practice. This generalization requires new analytical tools and
metrics to characterize convergence in the convex AMOO setting. We develop such
tools, propose scalable algorithms for convex AMOO, and establish their
convergence guarantees. Additionally, we prove a novel lower bound that
demonstrates the suboptimality of naive equal-weight approaches compared to our
methods.

</details>


### [98] [Performance of Conformal Prediction in Capturing Aleatoric Uncertainty](https://arxiv.org/abs/2509.05826)
*Misgina Tsighe Hagos,Claes Lundström*

Main category: cs.LG

TL;DR: 本文研究发现共形预测方法在捕捉数据固有歧义（偶然不确定性）方面效果有限，预测集大小与人类标注者标注的多样性之间相关性很弱


<details>
  <summary>Details</summary>
Motivation: 共形预测被认为能够通过预测集大小来量化偶然不确定性，但缺乏实证证据验证其有效性。本文旨在研究共形预测器是否能有效量化由类别重叠引起的数据固有歧义

Method: 使用三种共形预测方法为八个深度学习模型生成预测集，在四个包含多人标注（5-50人）的数据集上进行实验，测量预测集大小与人类标注多样性之间的相关性

Result: 绝大多数共形预测输出与人类标注之间仅呈现非常弱到弱的相关性，只有少数呈现中等相关性

Conclusion: 虽然共形预测能提供更高的真实类别覆盖率，但其捕捉偶然不确定性的能力有限，需要重新评估共形预测器生成的预测集

Abstract: Conformal prediction is a model-agnostic approach to generating prediction
sets that cover the true class with a high probability. Although its prediction
set size is expected to capture aleatoric uncertainty, there is a lack of
evidence regarding its effectiveness. The literature presents that prediction
set size can upper-bound aleatoric uncertainty or that prediction sets are
larger for difficult instances and smaller for easy ones, but a validation of
this attribute of conformal predictors is missing. This work investigates how
effectively conformal predictors quantify aleatoric uncertainty, specifically
the inherent ambiguity in datasets caused by overlapping classes. We perform
this by measuring the correlation between prediction set sizes and the number
of distinct labels assigned by human annotators per instance. We further assess
the similarity between prediction sets and human-provided annotations. We use
three conformal prediction approaches to generate prediction sets for eight
deep learning models trained on four datasets. The datasets contain annotations
from multiple human annotators (ranging from five to fifty participants) per
instance, enabling the identification of class overlap. We show that the vast
majority of the conformal prediction outputs show a very weak to weak
correlation with human annotations, with only a few showing moderate
correlation. These findings underscore the necessity of critically reassessing
the prediction sets generated using conformal predictors. While they can
provide a higher coverage of the true classes, their capability in capturing
aleatoric uncertainty remains limited.

</details>


### [99] [Finetuning LLMs for Human Behavior Prediction in Social Science Experiments](https://arxiv.org/abs/2509.05830)
*Akaash Kolluri,Shengguang Wu,Joon Sung Park,Michael S. Bernstein*

Main category: cs.LG

TL;DR: 通过在社会科学实验数据上微调大语言模型，显著提高了社会科学实验结果的模拟准确性，在未见实验中预测与人类响应分布的对齐度提升26%，超过GPT-4o 13%，同时减少了人口学偏见。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型模拟社会科学实验结果的潜力，通过在历史实验数据上微调提高模拟的准确性和可靠性。

Method: 构建SocSci210数据集（包含210个实验的2.9百万个响应），在Qwen2.5-14B基础模型上进行微调，得到Socrates-Qwen-14B模型，测试在未见实验和条件下的演绎性能。

Result: 在完全未见实验中，预测与人类响应分布的对齐度提升26%，超过GPT-4o 13%；在未见条件下提升71%；人口学偏见减少10.6%。

Conclusion: 社会科学领域丰富的主题数据可以通过微调大语言模型实现更准确的实验偏偏识别，为实验设计提供有效的预测工具。

Abstract: Large language models (LLMs) offer a powerful opportunity to simulate the
results of social science experiments. In this work, we demonstrate that
finetuning LLMs directly on individual-level responses from past experiments
meaningfully improves the accuracy of such simulations across diverse social
science domains. We construct SocSci210 via an automatic pipeline, a dataset
comprising 2.9 million responses from 400,491 participants in 210 open-source
social science experiments. Through finetuning, we achieve multiple levels of
generalization. In completely unseen studies, our strongest model,
Socrates-Qwen-14B, produces predictions that are 26% more aligned with
distributions of human responses to diverse outcome questions under varying
conditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by
13%. By finetuning on a subset of conditions in a study, generalization to new
unseen conditions is particularly robust, improving by 71%. Since SocSci210
contains rich demographic information, we reduce demographic parity, a measure
of bias, by 10.6% through finetuning. Because social sciences routinely
generate rich, topic-specific datasets, our findings indicate that finetuning
on such data could enable more accurate simulations for experimental hypothesis
screening. We release our data, models and finetuning code at
stanfordhci.github.io/socrates.

</details>


### [100] [Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces](https://arxiv.org/abs/2509.05833)
*Zeyu Song,Sainyam Galhotra,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: 这是一个用于评估去中心化梯度市场中精强聚合方法的综合性基准框架，重点考虑了经济效益、公平性和市场稳定性等因素。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习基准测试忽视了去中心化梯度市场中的关键经济和系统因素，特别是当买家依赖私有基准数据集进行评估时的成本效益、卖家公平性和市场稳定性。

Method: 提出了一个综合性基准框架，包括：(1)模拟市场动态的环境；(2)在标准FL指标基础上增加了市场相关评估维度；(3)对MartFL框架进行深入实证分析；(4)提供可操作的洞察。

Result: 该基准框架能够在多样化数据集、本地攻击和Sybil攻击等场景下进行评测，并提供了模型性能、稳健性、成本、公平性和稳定性之间的权衡分析。

Conclusion: 这个基准框架为社区提供了重要的工具和实证数据，以评估和设计更精强、更公平且经济可行的去中心化梯度市场。

Abstract: The rise of distributed and privacy-preserving machine learning has sparked
interest in decentralized gradient marketplaces, where participants trade
intermediate artifacts like gradients. However, existing Federated Learning
(FL) benchmarks overlook critical economic and systemic factors unique to such
marketplaces-cost-effectiveness, fairness to sellers, and market
stability-especially when a buyer relies on a private baseline dataset for
evaluation.
  We introduce a comprehensive benchmark framework to holistically evaluate
robust gradient aggregation methods within these buyer-baseline-reliant
marketplaces. Our contributions include: (1) a simulation environment modeling
marketplace dynamics with a variable buyer baseline and diverse seller
distributions; (2) an evaluation methodology augmenting standard FL metrics
with marketplace-centric dimensions such as Economic Efficiency, Fairness, and
Selection Dynamics; (3) an in-depth empirical analysis of the existing
Distributed Gradient Marketplace framework, MartFL, including the integration
and comparative evaluation of adapted FLTrust and SkyMask as alternative
aggregation strategies within it. This benchmark spans diverse datasets, local
attacks, and Sybil attacks targeting the marketplace selection process; and (4)
actionable insights into the trade-offs between model performance, robustness,
cost, fairness, and stability.
  This benchmark equips the community with essential tools and empirical
evidence to evaluate and design more robust, equitable, and economically viable
decentralized gradient marketplaces.

</details>


### [101] [Data-Driven Stochastic Modeling Using Autoregressive Sequence Models: Translating Event Tables to Queueing Dynamics](https://arxiv.org/abs/2509.05839)
*Daksh Mittal,Shunri Zheng,Jing Dong,Hongseok Namkoong*

Main category: cs.LG

TL;DR: 使用自回归序列模型从事件流数据中学习条件分布，自动化构建队列网络模型和模拟器


<details>
  <summary>Details</summary>
Motivation: 传统队列网络模型需要大量人工努力和领域专业知识，为了让模型方法更可扩展和更易访问

Method: 提出基于自回归序列模型的数据驱动框架，使用Transformer类架构参数化事件类型和事件时间的条件分布

Result: 在多样化队列网络生成的事件表上验证框架有效，能够用于模拟、不确定性量化和反事实评估

Conclusion: 利用AI进步和数据可用性，向更自动化的数据驱动模型流程进行了探索，支持队列网络模型在服务领域的更广泛采用

Abstract: While queueing network models are powerful tools for analyzing service
systems, they traditionally require substantial human effort and domain
expertise to construct. To make this modeling approach more scalable and
accessible, we propose a data-driven framework for queueing network modeling
and simulation based on autoregressive sequence models trained on event-stream
data. Instead of explicitly specifying arrival processes, service mechanisms,
or routing logic, our approach learns the conditional distributions of event
types and event times, recasting the modeling task as a problem of sequence
distribution learning. We show that Transformer-style architectures can
effectively parameterize these distributions, enabling automated construction
of high-fidelity simulators. As a proof of concept, we validate our framework
on event tables generated from diverse queueing networks, showcasing its
utility in simulation, uncertainty quantification, and counterfactual
evaluation. Leveraging advances in artificial intelligence and the growing
availability of data, our framework takes a step toward more automated,
data-driven modeling pipelines to support broader adoption of queueing network
models across service domains.

</details>


### [102] [The Measure of Deception: An Analysis of Data Forging in Machine Unlearning](https://arxiv.org/abs/2509.05865)
*Rishabh Dixit,Yuan Hui,Rayan Saab*

Main category: cs.LG

TL;DR: 该论文研究了机器学习遗忘中的对抗伪造问题，分析了ε-伪造集合的测度特性，证明了在多种模型和条件下伪造点的概率极小，为检测虚假遗忘声明提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 受隐私法规和有害数据处理的驱动，机器学习遗忘需要修改训练模型以有效"忘记"指定数据。验证遗忘的关键挑战是对抗伪造——恶意制作模拟目标点梯度的数据，造成已遗忘的假象而实际未移除信息。

Method: 提出ε-伪造集合的分析框架，针对线性回归和单层神经网络，计算该集合的Lebesgue测度。在一般正则性假设下，证明伪造集合测度以ε^{(d-r)/2}速率衰减，其中d为数据维度，r为模型梯度变分矩阵的零度。扩展到批量SGD和几乎处处光滑损失函数。

Result: 研究表明ε-伪造集合的测度很小，按ε的阶数缩放，当ε足够小时为ε^d。在非退化数据分布下，随机采样伪造点的概率极小。对抗伪造在根本上受到限制，虚假遗忘声明原则上可以被检测。

Conclusion: 对抗伪造存在根本性限制，通过分析ε-伪造集合的测度特性，为检测虚假的机器学习遗忘声明提供了理论依据和可行性证明。

Abstract: Motivated by privacy regulations and the need to mitigate the effects of
harmful data, machine unlearning seeks to modify trained models so that they
effectively ``forget'' designated data. A key challenge in verifying unlearning
is forging -- adversarially crafting data that mimics the gradient of a target
point, thereby creating the appearance of unlearning without actually removing
information. To capture this phenomenon, we consider the collection of data
points whose gradients approximate a target gradient within tolerance
$\epsilon$ -- which we call an $\epsilon$-forging set -- and develop a
framework for its analysis. For linear regression and one-layer neural
networks, we show that the Lebesgue measure of this set is small. It scales on
the order of $\epsilon$, and when $\epsilon$ is small enough, $\epsilon^d$.
More generally, under mild regularity assumptions, we prove that the forging
set measure decays as $\epsilon^{(d-r)/2}$, where $d$ is the data dimension and
$r<d$ is the nullity of a variation matrix defined by the model gradients.
Extensions to batch SGD and almost-everywhere smooth loss functions yield the
same asymptotic scaling. In addition, we establish probability bounds showing
that, under non-degenerate data distributions, the likelihood of randomly
sampling a forging point is vanishingly small. These results provide evidence
that adversarial forging is fundamentally limited and that false unlearning
claims can, in principle, be detected.

</details>


### [103] [Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement Learning](https://arxiv.org/abs/2509.05874)
*Shao-An Yin*

Main category: cs.LG

TL;DR: 提出基于深度强化学习的稀疏参考文献选择框架，模拟人类知识构建过程，在有限时间和成本下优先选择阅读哪些论文


<details>
  <summary>Details</summary>
Motivation: 科学文献的快速扩张使得获取新知识变得困难，特别是在专业领域中推理复杂、全文访问受限且目标参考文献在大量候选文献中稀疏分布的情况下

Method: 使用深度强化学习框架进行稀疏参考文献选择，模仿人类知识构建过程，在仅能访问标题和摘要的情况下优先选择阅读论文

Result: 在药物-基因关系发现任务上的评估表明，人类和机器都能从部分信息中有效构建知识

Conclusion: 该深度强化学习方法能够在信息受限的情况下有效支持知识构建和文献选择决策

Abstract: The rapid expansion of scientific literature makes it increasingly difficult
to acquire new knowledge, particularly in specialized domains where reasoning
is complex, full-text access is restricted, and target references are sparse
among a large set of candidates. We present a Deep Reinforcement Learning
framework for sparse reference selection that emulates human knowledge
construction, prioritizing which papers to read under limited time and cost.
Evaluated on drug--gene relation discovery with access restricted to titles and
abstracts, our approach demonstrates that both humans and machines can
construct knowledge effectively from partial information.

</details>


### [104] [SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework](https://arxiv.org/abs/2509.05886)
*Reza Pirayeshshirazinezhad*

Main category: cs.LG

TL;DR: 基于机器学习和物理信息神经网络的液态钠对流热传道系数预测模型，达到约8%的预测误差


<details>
  <summary>Details</summary>
Motivation: 高保真度CFD模拟液态金属强式对流热传递耗时计算费，需要更高效的模型工具

Method: 采用核机器学习、浅层神经网络、自监督物理信息神经网络和迁移学习方法，根据数据和物理不确定性调整损失函数权重

Result: 自监督物理信息神经网络成功预测钠的热传道率，误差约8%，单独使用物理回归误差4%到10%

Conclusion: 机器学习模型为液态金属冷却微型热池设计和优化提供了高效的替代方案

Abstract: A surrogate model is developed to predict the convective heat transfer
coefficient of liquid sodium (Na) flow within rectangular miniature heat sinks.
Initially, kernel-based machine learning techniques and shallow neural network
are applied to a dataset with 87 Nusselt numbers for liquid sodium in
rectangular miniature heat sinks. Subsequently, a self-supervised
physics-informed neural network and transfer learning approach are used to
increase the estimation performance. In the self-supervised physics-informed
neural network, an additional layer determines the weight the of physics in the
loss function to balance data and physics based on their uncertainty for a
better estimation. For transfer learning, a shallow neural network trained on
water is adapted for use with Na. Validation results show that the
self-supervised physics-informed neural network successfully estimate the heat
transfer rates of Na with an error margin of approximately +8%. Using only
physics for regression, the error remains between 5% to 10%. Other machine
learning methods specify the prediction mostly within +8%. High-fidelity
modeling of turbulent forced convection of liquid metals using computational
fluid dynamics (CFD) is both time-consuming and computationally expensive.
Therefore, machine learning based models offer a powerful alternative tool for
the design and optimization of liquid-metal-cooled miniature heat sinks.

</details>


### [105] [X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs](https://arxiv.org/abs/2509.05899)
*Dazhi Peng*

Main category: cs.LG

TL;DR: X-SQL是一个基于开源大语言模型的Text-to-SQL框架，通过创新的数据库模式专家组件（X-Linking和X-Admin）显著提升模式信息利用，在Spider数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based Text-to-SQL方法往往忽视数据库模式信息的重要性，而研究发现模式信息对生成高质量SQL查询起着关键甚至主导作用。

Method: 提出包含两个组件的数据库模式专家：1）X-Linking：基于LLM监督微调的Schema Linking方法；2）X-Admin：通过桥接抽象模式信息和用户自然语言问题来实现Schema Understanding。还采用多LLM策略为不同组件选择最优模型。

Result: 在Spider-Dev数据集上达到84.9%的执行准确率，在Spider-Test数据集上达到82.5%的执行准确率，成为基于开源模型的最佳Text-to-SQL框架。

Conclusion: X-SQL通过有效利用数据库模式信息和多LLM策略，显著提升了Text-to-SQL任务的性能，证明了模式信息在该任务中的关键作用。

Abstract: With Large Language Models' (LLMs) emergent abilities on code generation
tasks, Text-to-SQL has become one of the most popular downstream applications.
Despite the strong results of multiple recent LLM-based Text-to-SQL frameworks,
the research community often overlooks the importance of database schema
information for generating high-quality SQL queries. We find that such schema
information plays a significant or even dominant role in the Text-to-SQL task.
To tackle this challenge, we propose a novel database schema expert with two
components. We first introduce X-Linking, an LLM Supervised Finetuning
(SFT)-based method that achieves superior Schema Linking results compared to
existing open-source Text-to-SQL methods. In addition, we innovatively propose
an X-Admin component that focuses on Schema Understanding by bridging the gap
between abstract schema information and the user's natural language question.
Aside from better learning with schema information, we experiment with
Multi-LLMs for different components within the system to further boost its
performance. By incorporating these techniques into our end-to-end framework,
X-SQL, we have achieved Execution Accuracies of 84.9% on the Spider-Dev dataset
and 82.5% on the Spider-Test dataset. This outstanding performance establishes
X-SQL as the leading Text-to-SQL framework based on open-source models.

</details>


### [106] [Smoothed Online Optimization for Target Tracking: Robust and Learning-Augmented Algorithms](https://arxiv.org/abs/2509.05930)
*Ali Zeynali,Mahsa Sahebdel,Qingsong Liu,Mohammad Hajiesmaili,Ramesh K. Sitaraman*

Main category: cs.LG

TL;DR: 提出了SOOTT框架，整合目标跟踪、对抗扰动和切换成本三个在线决策目标，并开发了BEST算法和CoRT学习增强算法


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中需要同时处理动态目标跟踪、不可预测扰动和决策平滑性的在线决策问题，如AI集群中的工作负载调度

Method: 首先提出具有竞争性保证的BEST鲁棒算法，然后引入CoRT学习增强算法，利用不可信的ML预测来改进决策

Result: 理论分析表明CoRT在预测准确时优于BEST，同时在任意预测错误下保持鲁棒性。案例研究验证了算法在平衡轨迹跟踪、决策平滑性和抗扰动方面的有效性

Conclusion: SOOTT框架为不确定环境下的在线决策提供了新视角，BEST和CoRT算法在实际应用中展现出良好的性能平衡

Abstract: We introduce the Smoothed Online Optimization for Target Tracking (SOOTT)
problem, a new framework that integrates three key objectives in online
decision-making under uncertainty: (1) tracking cost for following a
dynamically moving target, (2) adversarial perturbation cost for withstanding
unpredictable disturbances, and (3) switching cost for penalizing abrupt
changes in decisions. This formulation captures real-world scenarios such as
elastic and inelastic workload scheduling in AI clusters, where operators must
balance long-term service-level agreements (e.g., LLM training) against sudden
demand spikes (e.g., real-time inference). We first present BEST, a robust
algorithm with provable competitive guarantees for SOOTT. To enhance practical
performance, we introduce CoRT, a learning-augmented variant that incorporates
untrusted black-box predictions (e.g., from ML models) into its decision
process. Our theoretical analysis shows that CoRT strictly improves over BEST
when predictions are accurate, while maintaining robustness under arbitrary
prediction errors. We validate our approach through a case study on workload
scheduling, demonstrating that both algorithms effectively balance trajectory
tracking, decision smoothness, and resilience to external disturbances.

</details>


### [107] [Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior](https://arxiv.org/abs/2509.06025)
*Vignesh Ethiraj,Subhash Talluri*

Main category: cs.LG

TL;DR: 提出了统一交互基础模型(UIFM)，采用复合标记化方法将多属性事件作为语义连贯单元处理，解决了传统模型在序列化事件时丢失关键上下文的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于自然语言的基础模型无法理解电信、电商和金融等领域中结构化交互的整体性，将事件序列化为文本会导致语义碎片化和关键上下文丢失。

Method: 采用复合标记化原则，将每个多属性事件视为单个语义连贯单元，学习用户行为的底层"语法"，感知完整的交互而非离散的数据流。

Result: 该架构不仅更准确，而且代表了创建更适应性和智能预测系统的根本性进步。

Conclusion: UIFM模型是朝着真正行为理解和构建更智能预测系统的重要一步，能够更好地处理复杂演化的事件序列。

Abstract: A central goal of artificial intelligence is to build systems that can
understand and predict complex, evolving sequences of events. However, current
foundation models, designed for natural language, fail to grasp the holistic
nature of structured interactions found in domains like telecommunications,
e-commerce and finance. By serializing events into text, they disassemble them
into semantically fragmented parts, losing critical context. In this work, we
introduce the Unified Interaction Foundation Model (UIFM), a foundation model
engineered for genuine behavioral understanding. At its core is the principle
of composite tokenization, where each multi-attribute event is treated as a
single, semantically coherent unit. This allows UIFM to learn the underlying
"grammar" of user behavior, perceiving entire interactions rather than a
disconnected stream of data points. We demonstrate that this architecture is
not just more accurate, but represents a fundamental step towards creating more
adaptable and intelligent predictive systems.

</details>


### [108] [PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training](https://arxiv.org/abs/2509.06053)
*Mingrui Lv,Hangzhi Liu,Zhi Luo,Hongjie Zhang,Jie Ou*

Main category: cs.LG

TL;DR: PolicyEvolve是一个基于LLM的多智能体对抗游戏程序化策略生成框架，通过迭代进化的方式减少对人工编码和环境交互的依赖，生成高性能且可解释的策略代码。


<details>
  <summary>Details</summary>
Motivation: 传统MARL方法需要大量样本和计算资源，且缺乏可解释性。LLM在单智能体任务中成功生成程序化策略的启发下，希望将其扩展到多玩家游戏中。

Method: 包含四个模块：全局池保存精英策略，局部池存储临时策略，策略规划器基于环境信息和反馈生成策略，轨迹评判器分析交互数据并提出改进方向。通过迭代进化过程优化策略。

Result: 显著减少对人工编码策略的依赖，以最少的环境交互实现高性能策略，提高策略的可解释性和执行效率。

Conclusion: PolicyEvolve框架为多玩家游戏提供了一种高效、可解释的程序化策略生成方法，通过LLM和进化算法的结合解决了传统MARL方法的局限性。

Abstract: Multi-agent reinforcement learning (MARL) has achieved significant progress
in solving complex multi-player games through self-play. However, training
effective adversarial policies requires millions of experience samples and
substantial computational resources. Moreover, these policies lack
interpretability, hindering their practical deployment. Recently, researchers
have successfully leveraged Large Language Models (LLMs) to generate
programmatic policies for single-agent tasks, transforming neural network-based
policies into interpretable rule-based code with high execution efficiency.
Inspired by this, we propose PolicyEvolve, a general framework for generating
programmatic policies in multi-player games. PolicyEvolve significantly reduces
reliance on manually crafted policy code, achieving high-performance policies
with minimal environmental interactions. The framework comprises four modules:
Global Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool
preserves elite policies accumulated during iterative training. The Local Pool
stores temporary policies for the current iteration; only sufficiently
high-performing policies from this pool are promoted to the Global Pool. The
Policy Planner serves as the core policy generation module. It samples the top
three policies from the Global Pool, generates an initial policy for the
current iteration based on environmental information, and refines this policy
using feedback from the Trajectory Critic. Refined policies are then deposited
into the Local Pool. This iterative process continues until the policy achieves
a sufficiently high average win rate against the Global Pool, at which point it
is integrated into the Global Pool. The Trajectory Critic analyzes interaction
data from the current policy, identifies vulnerabilities, and proposes
directional improvements to guide the Policy Planner

</details>


### [109] [A novel biomass fluidized bed gasification model coupled with machine learning and CFD simulation](https://arxiv.org/abs/2509.06056)
*Chun Wang*

Main category: cs.LG

TL;DR: 提出基于机器学习和计算流体力学的生物质流化床气化耦合模型，提高复杂热化学反应过程的预测精度和计算效率


<details>
  <summary>Details</summary>
Motivation: 提高生物质流化床气化过程中复杂热化学反应预测的准确性和计算效率

Method: 基于实验数据和高保真模拟结果构建高质量数据集，训练描述反应动力学特性的代理模型，并将其嵌入CFD框架实现反应速率和组分演化的实时更新

Result: 建立了机器学习和CFD相结合的耦合模型

Conclusion: 该方法能够有效提升生物质流化床气化过程的模拟精度和计算性能

Abstract: A coupling model of biomass fluidized bed gasification based on machine
learning and computational fluid dynamics is proposed to improve the prediction
accuracy and computational efficiency of complex thermochemical reaction
process. By constructing a high-quality data set based on experimental data and
high fidelity simulation results, the agent model used to describe the
characteristics of reaction kinetics was trained and embedded into the
computational fluid dynamics (CFD) framework to realize the real-time update of
reaction rate and composition evolution.

</details>


### [110] [ARIES: Relation Assessment and Model Recommendation for Deep Time Series Forecasting](https://arxiv.org/abs/2509.06060)
*Fei Wang,Yujie Li,Zezhi Shao,Chengqing Yu,Yisong Fu,Zhulin An,Yongjun Xu,Xueqi Cheng*

Main category: cs.LG

TL;DR: ARIES是一个评估时间序列属性与建模策略关系的框架，包含合成数据集构建、50多个预测模型基准测试，以及首个深度预测模型推荐系统


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集缺乏多样化的时间模式，无法系统评估模型性能与数据属性的关系，且没有有效的模型推荐方法导致测试成本高昂

Method: 构建具有多种明确时间模式的合成数据集，设计全面的时间序列属性计算系统，对50多个预测模型进行广泛基准测试，建立时间序列属性与建模策略的关系

Result: 实验结果显示时间序列属性与建模策略存在明显相关性，基于此开发了首个深度预测模型推荐器

Conclusion: ARIES是首个建立时间序列数据属性与建模策略关系的研究，并实现了模型推荐系统，为现实世界时间序列提供可解释的建议

Abstract: Recent advancements in deep learning models for time series forecasting have
been significant. These models often leverage fundamental time series
properties such as seasonality and non-stationarity, which may suggest an
intrinsic link between model performance and data properties. However, existing
benchmark datasets fail to offer diverse and well-defined temporal patterns,
restricting the systematic evaluation of such connections. Additionally, there
is no effective model recommendation approach, leading to high time and cost
expenditures when testing different architectures across different downstream
applications. For those reasons, we propose ARIES, a framework for assessing
relation between time series properties and modeling strategies, and for
recommending deep forcasting models for realistic time series. First, we
construct a synthetic dataset with multiple distinct patterns, and design a
comprehensive system to compute the properties of time series. Next, we conduct
an extensive benchmarking of over 50 forecasting models, and establish the
relationship between time series properties and modeling strategies. Our
experimental results reveal a clear correlation. Based on these findings, we
propose the first deep forecasting model recommender, capable of providing
interpretable suggestions for real-world time series. In summary, ARIES is the
first study to establish the relations between the properties of time series
data and modeling strategies, while also implementing a model recommendation
system. The code is available at: https://github.com/blisky-li/ARIES.

</details>


### [111] [A Surrogate model for High Temperature Superconducting Magnets to Predict Current Distribution with Neural Network](https://arxiv.org/abs/2509.06067)
*Mianjun Xiao,Peng Song,Yulong Liu,Cedric Korte,Ziyang Xu,Jiale Gao,Jiaqi Lu,Haoyang Nie,Qiantong Deng,Timing Qu*

Main category: cs.LG

TL;DR: 开发基于全连接残差神经网络(FCRN)的代理模型，用于快速预测REBCO超导磁体的时空电流密度分布，相比传统有限元方法计算速度提升数个数量级。


<details>
  <summary>Details</summary>
Motivation: 有限元方法在高温超导磁体设计中计算成本高，特别是对于米级尺寸磁体和多物理场耦合情况，限制了大型REBCO磁体系统的快速设计。

Method: 使用全连接残差神经网络构建代理模型，训练数据来自不同匝数和饼数的有限元模拟，最佳配置为12个残差块和每层256个神经元。

Result: FCRN架构比传统全连接网络收敛更好，可可靠预测超出训练范围50%的磁化损耗，最大误差低于10%，计算速度比有限元快数个数量级。

Conclusion: 提出的FCRN代理模型在精度和效率方面都表现出色，为大型高温超导磁体的快速分析提供了有前景的工具。

Abstract: Finite element method (FEM) is widely used in high-temperature
superconducting (HTS) magnets, but its computational cost increases with magnet
size and becomes time-consuming for meter-scale magnets, especially when
multi-physics couplings are considered, which limits the fast design of
large-scale REBCO magnet systems. In this work, a surrogate model based on a
fully connected residual neural network (FCRN) is developed to predict the
space-time current density distribution in REBCO solenoids. Training datasets
were generated from FEM simulations with varying numbers of turns and pancakes.
The results demonstrate that, for deeper networks, the FCRN architecture
achieves better convergence than conventional fully connected network (FCN),
with the configuration of 12 residual blocks and 256 neurons per layer
providing the most favorable balance between training accuracy and
generalization capability. Extrapolation studies show that the model can
reliably predict magnetization losses for up to 50% beyond the training range,
with maximum errors below 10%. The surrogate model achieves predictions several
orders of magnitude faster than FEM and still remains advantageous when
training costs are included. These results indicate that the proposed
FCRN-based surrogate model provides both accuracy and efficiency, offering a
promising tool for the rapid analysis of large-scale HTS magnets.

</details>


### [112] [Teaching Precommitted Agents: Model-Free Policy Evaluation and Control in Quasi-Hyperbolic Discounted MDPs](https://arxiv.org/abs/2509.06094)
*S. R. Eshwar*

Main category: cs.LG

TL;DR: 本文为具有准双曲线折扣偏好的预承诺智能体填补了强化学习理论空白，证明了最优策略可简化为简单的一步非平稳形式，并设计了首个实用的无模型算法。


<details>
  <summary>Details</summary>
Motivation: 准双曲线折扣是模拟人类和动物时间不一致偏好的重要模型，但其在强化学习框架中的整合一直有限，需要解决理论和算法上的关键空白。

Method: 通过理论分析形式化描述最优策略结构，证明其可简化为一步非平稳形式；设计首个实用的无模型策略评估和Q学习算法，并提供可证明的收敛保证。

Result: 成功证明了最优策略的简化结构，开发了具有收敛保证的实用算法，为准双曲线偏好在强化学习中的应用提供了理论基础。

Conclusion: 该研究为准双曲线折扣偏好融入强化学习奠定了重要基础，为处理时间不一致决策问题提供了新的理论见解和算法工具。

Abstract: Time-inconsistent preferences, where agents favor smaller-sooner over
larger-later rewards, are a key feature of human and animal decision-making.
Quasi-Hyperbolic (QH) discounting provides a simple yet powerful model for this
behavior, but its integration into the reinforcement learning (RL) framework
has been limited. This paper addresses key theoretical and algorithmic gaps for
precommitted agents with QH preferences. We make two primary contributions: (i)
we formally characterize the structure of the optimal policy, proving for the
first time that it reduces to a simple one-step non-stationary form; and (ii)
we design the first practical, model-free algorithms for both policy evaluation
and Q-learning in this setting, both with provable convergence guarantees. Our
results provide foundational insights for incorporating QH preferences in RL.

</details>


### [113] [If generative AI is the answer, what is the question?](https://arxiv.org/abs/2509.06120)
*Ambuj Tewari*

Main category: cs.LG

TL;DR: 生成式AI的理论基础与应用研究，包括五大生成模型家族、概率框架、游戏理论分析以及社会责任问题


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI作为一种特定的机器学习任务的理论基础，并了解其与预测、压缩和决策制定等领域的联系

Method: 调查五大生成模型家族（自回归模型、变分自动编码器、正规化流、生成对抗网络、滴流模型），提出概率框架区分密度估计与生成，使用游戏理论框架分析两方对抗学习设置

Result: 建立了一个系统的生成式AI理论框架，包含模型分类、数学基础、训练方法和部署准备，并识别了关键的社会责任问题

Conclusion: 采用任务为主的视角来理解生成式AI，重点关注生成的本质问题而不仅仅是模型实现方式，为该领域的研究和应用提供了理论指导

Abstract: Beginning with text and images, generative AI has expanded to audio, video,
computer code, and molecules. Yet, if generative AI is the answer, what is the
question? We explore the foundations of generation as a distinct machine
learning task with connections to prediction, compression, and decision-making.
We survey five major generative model families: autoregressive models,
variational autoencoders, normalizing flows, generative adversarial networks,
and diffusion models. We then introduce a probabilistic framework that
emphasizes the distinction between density estimation and generation. We review
a game-theoretic framework with a two-player adversary-learner setup to study
generation. We discuss post-training modifications that prepare generative
models for deployment. We end by highlighting some important topics in socially
responsible generation such as privacy, detection of AI-generated content, and
copyright and IP. We adopt a task-first framing of generation, focusing on what
generation is as a machine learning problem, rather than only on how models
implement it.

</details>


### [114] [Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs Neural Operators](https://arxiv.org/abs/2509.06154)
*Dibyajyoti Nayak,Somdatta Goswami*

Main category: cs.LG

TL;DR: 通过结合图神经网模拟器和显式数值时间步进方案，构建了高效的偏微方程求解模型，在训练数据极少的情况下仍能达到高精度预测效果


<details>
  <summary>Details</summary>
Motivation: 解决神经运算符需要大量训练数据且在数据稀缺时表现差的问题，同时保持物理过程的因果性和时间本地性结构

Method: 采用图神经网模拟器(GNS)框架，结合显式数值时间步进方案，通过学习瞬时时间导数来建立偏微方程求解模型，并使用PCA+KMeans轨迹选择策略

Result: 在仅使用3%训练数据(30个样本)的情况下，在三个标准PDE系统上都达到了相对L2误差小于1%的高精度，相比于FNO AR和DON AR自回归模型分别减少82.48%和99.86%的误差累积

Conclusion: 结合图基本地归纳偏置与传统时间积分器，可以构建准确、物理一致且可扩展的代理模型用于时间依存偏微方程

Abstract: Neural operators (NOs) approximate mappings between infinite-dimensional
function spaces but require large datasets and struggle with scarce training
data. Many NO formulations don't explicitly encode causal, local-in-time
structure of physical evolution. While autoregressive models preserve causality
by predicting next time-steps, they suffer from rapid error accumulation. We
employ Graph Neural Simulators (GNS) - a message-passing graph neural network
framework - with explicit numerical time-stepping schemes to construct accurate
forward models that learn PDE solutions by modeling instantaneous time
derivatives. We evaluate our framework on three canonical PDE systems: (1) 2D
Burgers' scalar equation, (2) 2D coupled Burgers' vector equation, and (3) 2D
Allen-Cahn equation. Rigorous evaluations demonstrate GNS significantly
improves data efficiency, achieving higher generalization accuracy with
substantially fewer training trajectories compared to neural operator baselines
like DeepONet and FNO. GNS consistently achieves under 1% relative L2 errors
with only 30 training samples out of 1000 (3% of available data) across all
three PDE systems. It substantially reduces error accumulation over extended
temporal horizons: averaged across all cases, GNS reduces autoregressive error
by 82.48% relative to FNO AR and 99.86% relative to DON AR. We introduce a
PCA+KMeans trajectory selection strategy enhancing low-data performance.
Results indicate combining graph-based local inductive biases with conventional
time integrators yields accurate, physically consistent, and scalable surrogate
models for time-dependent PDEs.

</details>


### [115] [Tracking daily paths in home contexts with RSSI fingerprinting based on UWB through deep learning models](https://arxiv.org/abs/2509.06161)
*Aurora Polo-Rodríguez,Juan Carlos Valera,Jesús Peral,David Gil,Javier Medina-Quero*

Main category: cs.LG

TL;DR: 这篇论文研究了基于超宽带(UWB)技术咆深度学习模型的家庭环境中居民路径追踪方法，通过RSSI指纹技术充分利用信号强度数据，实现了约50厘米的平均绝对误差。


<details>
  <summary>Details</summary>
Motivation: 解决UWB技术在实际家庭环境中因墙壁咆障碍物影响而导致定位精度下降的问题，提高人体活动识别的准确性。

Method: 采用指纹技术基于RSSI数据，在两个不同面积宿舍中收集日常活动数据，比较CNN、LSTM咆混合CNN+LSTM模型的性能，并评估不同时间窗口类型的影响。

Result: 混合CNN+LSTM模型表现最优，实现了约50厘米的平均绝对误差，显著提高了定位精度。

Conclusion: 研究证明了UWB指纹技术结合深度学习模型在家庭环境中的有效性，为日常人类活动识别提供了准确的位置估计。

Abstract: The field of human activity recognition has evolved significantly, driven
largely by advancements in Internet of Things (IoT) device technology,
particularly in personal devices. This study investigates the use of
ultra-wideband (UWB) technology for tracking inhabitant paths in home
environments using deep learning models. UWB technology estimates user
locations via time-of-flight and time-difference-of-arrival methods, which are
significantly affected by the presence of walls and obstacles in real
environments, reducing their precision. To address these challenges, we propose
a fingerprinting-based approach utilizing received signal strength indicator
(RSSI) data collected from inhabitants in two flats (60 m2 and 100 m2) while
performing daily activities. We compare the performance of convolutional neural
network (CNN), long short-term memory (LSTM), and hybrid CNN+LSTM models, as
well as the use of Bluetooth technology. Additionally, we evaluate the impact
of the type and duration of the temporal window (future, past, or a combination
of both). Our results demonstrate a mean absolute error close to 50 cm,
highlighting the superiority of the hybrid model in providing accurate location
estimates, thus facilitating its application in daily human activity
recognition in residential settings.

</details>


### [116] [An Improved Template for Approximate Computing](https://arxiv.org/abs/2509.06162)
*M. Rezaalipour,F. Costa,M. Biasion,R. Otoni,G. A. Constantinides,L. Pozzi*

Main category: cs.LG

TL;DR: 通过提出一种基于可参数化乘积共享模板的新方法，在神经网络中实现更好的面积节省和近似计算效果


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署神经网络时，需要在能耗与准确性之间取得平衡，近似计算是降低能耗的有效手段

Method: 改进XPAT布尔重写技术，使用可参数化乘积共享模板作为面积代理指标，生成更小的算术运算器

Result: 方法在相同准确性损失下获得更好的面积节省，超越了原始XPAT和其他两种现有方法

Conclusion: 提出的模板参数化方法能够更有效地寻找低面积解决方案，为边缘设备的神经网络部署提供了更优的近似计算技术

Abstract: Deploying neural networks on edge devices entails a careful balance between
the energy required for inference and the accuracy of the resulting
classification. One technique for navigating this tradeoff is approximate
computing: the process of reducing energy consumption by slightly reducing the
accuracy of arithmetic operators. In this context, we propose a methodology to
reduce the area of the small arithmetic operators used in neural networks -
i.e., adders and multipliers - via a small loss in accuracy, and show that we
improve area savings for the same accuracy loss w.r.t. the state of the art. To
achieve our goal, we improve on a boolean rewriting technique recently
proposed, called XPAT, where the use of a parametrisable template to rewrite
circuits has proved to be highly beneficial. In particular, XPAT was able to
produce smaller circuits than comparable approaches while utilising a naive sum
of products template structure. In this work, we show that template parameters
can act as proxies for chosen metrics and we propose a novel template based on
parametrisable product sharing that acts as a close proxy to synthesised area.
We demonstrate experimentally that our methodology converges better to low-area
solutions and that it can find better approximations than both the original
XPAT and two other state-of-the-art approaches.

</details>


### [117] [Exploring Urban Factors with Autoencoders: Relationship Between Static and Dynamic Features](https://arxiv.org/abs/2509.06167)
*Ximena Pocco,Waqar Hassan,Karelia Salinas,Vladimir Molchanov,Luis G. Nonato*

Main category: cs.LG

TL;DR: 开发可视化辅助框架分析融合潜在数据表示是否比单独表示更能有效发现动态和静态城市数据中的模式，发现融合表示产生更结构化模式，而单独表示在特定情况下有用


<details>
  <summary>Details</summary>
Motivation: 解决城市分析中多源异构数据的融合挑战，探索融合数据表示是否能比单独数据源提供更深入的洞察

Method: 开发可视化辅助框架来比较融合潜在数据表示与单独表示在发现城市数据模式方面的效果

Result: 融合潜在表示产生更结构化的模式，而单独表示在特定情况下更有用

Conclusion: 融合数据表示在城市分析中能产生更好的结构化模式，但单独数据源在特定场景下仍具有价值

Abstract: Urban analytics utilizes extensive datasets with diverse urban information to
simulate, predict trends, and uncover complex patterns within cities. While
these data enables advanced analysis, it also presents challenges due to its
granularity, heterogeneity, and multimodality. To address these challenges,
visual analytics tools have been developed to support the exploration of latent
representations of fused heterogeneous and multimodal data, discretized at a
street-level of detail. However, visualization-assisted tools seldom explore
the extent to which fused data can offer deeper insights than examining each
data source independently within an integrated visualization framework. In this
work, we developed a visualization-assisted framework to analyze whether fused
latent data representations are more effective than separate representations in
uncovering patterns from dynamic and static urban data. The analysis reveals
that combined latent representations produce more structured patterns, while
separate ones are useful in particular cases.

</details>


### [118] [Reasoning Language Model for Personalized Lung Cancer Screening](https://arxiv.org/abs/2509.06169)
*Chuang Niu,Ge Wang*

Main category: cs.LG

TL;DR: 提出了一种推理语言模型(RLM)，通过整合放射学发现和纵向医疗记录进行个性化肺癌风险评估，显著提升了风险预测性能。


<details>
  <summary>Details</summary>
Motivation: Lung-RADS系统在敏感性和特异性之间存在权衡，仅基于肺结节特征进行风险分层，未纳入多种风险因素，需要更准确的肺癌筛查风险评估方法。

Method: 构建推理语言模型(RLM)，通过数据集构建和蒸馏、监督微调、强化学习等方法，将风险评估任务分解为子组件，分析不同风险因素的贡献，并使用数据驱动的系统方程合成最终风险评分。

Result: 在国家肺癌筛查试验数据集上，模型在风险预测性能方面取得了显著改进，提高了预测准确性和可监测性。

Conclusion: 该方法通过思维链推理过程改善了预测性能，有助于肺癌筛查的临床转化应用。

Abstract: Accurate risk assessment in lung cancer screening is critical for enabling
early cancer detection and minimizing unnecessary invasive procedures. The Lung
CT Screening Reporting and Data System (Lung-RADS) has been widely used as the
standard framework for patient management and follow-up. Nevertheless,
Lung-RADS faces trade-offs between sensitivity and specificity, as it
stratifies risk solely based on lung nodule characteristics without
incorporating various risk factors. Here we propose a reasoning language model
(RLM) to integrate radiology findings with longitudinal medical records for
individualized lung cancer risk assessment. Through a systematic study
including dataset construction and distillation, supervised fine-tuning,
reinforcement learning, and comprehensive evaluation, our model makes
significant improvements in risk prediction performance on datasets in the
national lung screening trial. Notably, RLM can decompose the risk evaluation
task into sub-components, analyze the contributions of diverse risk factors,
and synthesize them into a final risk score computed using our data-driven
system equation. Our approach improves both predictive accuracy and
monitorability through the chain of thought reasoning process, thereby
facilitating clinical translation into lung cancer screening.

</details>


### [119] [Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning](https://arxiv.org/abs/2509.06213)
*Christo Mathew,Wentian Wang,Lazaros Gallos,Paul Kantor,Vladimir Menkov,Hao Wang*

Main category: cs.LG

TL;DR: 研究在GOHR环境中使用Transformer-A2C算法，比较了特征个体和对象个体两种状态表示策略在部分观测下的强化学习效果


<details>
  <summary>Details</summary>
Motivation: 解决GOHR环境中需要同时推断隐藏规则和学习最优策略的复杂问题，探索不同状态表示方法对学习效率的影响

Method: 采用Transformer基础的Advantage Actor-Critic (A2C)算法，比较了特征中心(FC)和对象个体(OC)两种状态表示策略，在部分观测条件下进行训练

Result: 在多种规则基础和试验列表实验设置中评估模型，分析了迁移效应和表示方式对学习效率的影响

Conclusion: 该研究为复杂隐藏规则推理问题提供了有效的强化学习方案，两种状态表示策略在不同情况下各有优势

Abstract: We investigate reinforcement learning in the Game Of Hidden Rules (GOHR)
environment, a complex puzzle in which an agent must infer and execute hidden
rules to clear a 6$\times$6 board by placing game pieces into buckets. We
explore two state representation strategies, namely Feature-Centric (FC) and
Object-Centric (OC), and employ a Transformer-based Advantage Actor-Critic
(A2C) algorithm for training. The agent has access only to partial observations
and must simultaneously infer the governing rule and learn the optimal policy
through experience. We evaluate our models across multiple rule-based and
trial-list-based experimental setups, analyzing transfer effects and the impact
of representation on learning efficiency.

</details>


### [120] [Metric Embedding Initialization-Based Differentially Private and Explainable Graph Clustering](https://arxiv.org/abs/2509.06214)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: 提出了一种基于度量嵌入初始化的差分隐私可解释图聚类方法，通过SDP优化和HST初始化提供良好聚类配置，使用k-median聚类策略获得结果，并在保证隐私的同时提供可解释性


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私图聚类研究面临高噪声、低效率和可解释性差等挑战，严重制约了该领域的发展

Method: 构建SDP优化问题，提取关键集并使用基于HST的初始化方法提供良好初始聚类配置，然后应用k-median聚类策略获得聚类结果，通过聚类中心差异为查询集提供比较解释

Result: 在公开数据集上的大量实验表明，该框架在各种聚类指标上优于现有方法，同时严格确保隐私保护

Conclusion: 该方法成功解决了差分隐私图聚类中的噪声、效率和可解释性问题，为隐私保护的图数据分析提供了有效解决方案

Abstract: Graph clustering under the framework of differential privacy, which aims to
process graph-structured data while protecting individual privacy, has been
receiving increasing attention. Despite significant achievements in current
research, challenges such as high noise, low efficiency and poor
interpretability continue to severely constrain the development of this field.
In this paper, we construct a differentially private and interpretable graph
clustering approach based on metric embedding initialization. Specifically, we
construct an SDP optimization, extract the key set and provide a
well-initialized clustering configuration using an HST-based initialization
method. Subsequently, we apply an established k-median clustering strategy to
derive the cluster results and offer comparative explanations for the query set
through differences from the cluster centers. Extensive experiments on public
datasets demonstrate that our proposed framework outperforms existing methods
in various clustering metrics while strictly ensuring privacy.

</details>


### [121] [MCIGLE: Multimodal Exemplar-Free Class-Incremental Graph Learning](https://arxiv.org/abs/2509.06219)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: MCIGLE是一个无需样本的类增量学习框架，专门处理多模态图结构数据，通过特征提取对齐和递归最小二乘法解决灾难性遗忘等问题。


<details>
  <summary>Details</summary>
Motivation: 随着多模态图结构数据的普及，现有方法在处理灾难性遗忘、分布偏差、内存限制和泛化能力弱等挑战时表现不佳。

Method: 提出MCIGLE框架，通过提取和对齐多模态图特征，应用串联递归最小二乘法进行知识保留，采用多通道处理平衡准确性和内存保护。

Result: 在公共数据集上的实验验证了该方法的有效性和泛化能力。

Conclusion: MCIGLE框架成功解决了多模态图结构数据类增量学习中的关键问题，为相关领域提供了有效的解决方案。

Abstract: Exemplar-free class-incremental learning enables models to learn new classes
over time without storing data from old ones. As multimodal graph-structured
data becomes increasingly prevalent, existing methods struggle with challenges
like catastrophic forgetting, distribution bias, memory limits, and weak
generalization. We propose MCIGLE, a novel framework that addresses these
issues by extracting and aligning multimodal graph features and applying
Concatenated Recursive Least Squares for effective knowledge retention. Through
multi-channel processing, MCIGLE balances accuracy and memory preservation.
Experiments on public datasets validate its effectiveness and generalizability.

</details>


### [122] [UrbanMIMOMap: A Ray-Traced MIMO CSI Dataset with Precoding-Aware Maps and Benchmarks](https://arxiv.org/abs/2509.06270)
*Honggang Jia,Xiucheng Wang,Nan Cheng,Ruijin Sun,Changle Li*

Main category: cs.LG

TL;DR: 这篇论文提供了UrbanMIMOMap，一个通过高精度光线追踪生成的大规模城市MIMO CSI数据集，以支持6G环境感知和隧道地图生成的研究。


<details>
  <summary>Details</summary>
Motivation: 6G系统需要环境感知通信，而无线电地图是关键使能技术。但现有公开数据集多为SISO系统和路径损耗数据，不能满足MIMO系统对详细隧道状态信息的需求。

Method: 使用高精度光线追踪技术生成大规模城市环境下的MIMO隧道状态信息数据集，包含密集空间格点上的复杂CSI矩阵。

Result: 构建了UrbanMIMOMap数据集，提供了比传统路径损耗更丰富的CSI信息，并通过代表性机器学习方法进行了基准性能评估。

Conclusion: 该工作为高精度无线电地图生成、MIMO空间性能和6G环境感知的机器学习研究提供了关键数据集和参考基准，代码和数据已开源。

Abstract: Sixth generation (6G) systems require environment-aware communication, driven
by native artificial intelligence (AI) and integrated sensing and communication
(ISAC). Radio maps (RMs), providing spatially continuous channel information,
are key enablers. However, generating high-fidelity RM ground truth via
electromagnetic (EM) simulations is computationally intensive, motivating
machine learning (ML)-based RM construction. The effectiveness of these
data-driven methods depends on large-scale, high-quality training data. Current
public datasets often focus on single-input single-output (SISO) and limited
information, such as path loss, which is insufficient for advanced multi-input
multi-output (MIMO) systems requiring detailed channel state information (CSI).
To address this gap, this paper presents UrbanMIMOMap, a novel large-scale
urban MIMO CSI dataset generated using high-precision ray tracing. UrbanMIMOMap
offers comprehensive complex CSI matrices across a dense spatial grid, going
beyond traditional path loss data. This rich CSI is vital for constructing
high-fidelity RMs and serves as a fundamental resource for data-driven RM
generation, including deep learning. We demonstrate the dataset's utility
through baseline performance evaluations of representative ML methods for RM
construction. This work provides a crucial dataset and reference for research
in high-precision RM generation, MIMO spatial performance, and ML for 6G
environment awareness. The code and data for this work are available at:
https://github.com/UNIC-Lab/UrbanMIMOMap.

</details>


### [123] [IPR: Intelligent Prompt Routing with User-Controlled Quality-Cost Trade-offs](https://arxiv.org/abs/2509.06274)
*Aosong Feng,Zhichao Xu,Xian Wu,Kang Zhou,Sheng Guan,Yueyan Chen,Ninad Kulkarni,Yun Zhou,Balasubramaniam Srinivasan,Haibo Ding,Lin Lee Cheong*

Main category: cs.LG

TL;DR: IPR是一个智能提示路由框架，通过预测响应质量和用户容忍度动态选择最优LLM，在保持质量的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决在大规模商业系统中路由查询到最具成本效益的LLM同时保持响应质量的基本挑战，优化性能-成本权衡。

Method: 采用模块化架构，包含轻量级质量估计器（在150万标注提示上训练）、用户控制的容忍度参数路由机制，以及使用冻结编码器和模型特定适配器的可扩展设计。

Result: 在主要云平台上部署后，IPR实现了43.9%的成本降低，同时保持与Claude家族最强模型的质量相当，处理延迟低于150ms。

Conclusion: IPR框架有效解决了LLM路由的成本-质量权衡问题，提供了用户可控的解决方案，并显著简化了新模型集成过程。

Abstract: Routing incoming queries to the most cost-effective LLM while maintaining
response quality poses a fundamental challenge in optimizing performance-cost
trade-offs for large-scale commercial systems. We present IPR\, a
quality-constrained Intelligent Prompt Routing framework that dynamically
selects optimal models based on predicted response quality and user-specified
tolerance levels. IPR introduces three key innovations: (1) a modular
architecture with lightweight quality estimators trained on 1.5M prompts
annotated with calibrated quality scores, enabling fine-grained quality
prediction across model families; (2) a user-controlled routing mechanism with
tolerance parameter $\tau \in [0,1]$ that provides explicit control over
quality-cost trade-offs; and (3) an extensible design using frozen encoders
with model-specific adapters, reducing new model integration from days to
hours. To rigorously train and evaluate IPR, we curate an industrial-level
dataset IPRBench\footnote{IPRBench will be released upon legal approval.}, a
comprehensive benchmark containing 1.5 million examples with response quality
annotations across 11 LLM candidates. Deployed on a major cloud platform, IPR
achieves 43.9\% cost reduction while maintaining quality parity with the
strongest model in the Claude family and processes requests with sub-150ms
latency.

</details>


### [124] [RecMind: LLM-Enhanced Graph Neural Networks for Personalized Consumer Recommendations](https://arxiv.org/abs/2509.06286)
*Chang Xue,Youwei Lu,Chen Yang,Jinming Xing*

Main category: cs.LG

TL;DR: RecMind是一个LLM增强的图推荐系统，将语言模型作为偏好先验而非单一排序器，通过对比学习和门控融合实现文本与图结构的协同优化


<details>
  <summary>Details</summary>
Motivation: 个性化推荐面临稀疏交互、快速内容更新和异构文本信号的挑战，需要有效结合语言模型和图结构信息

Method: 使用冻结LLM加轻量适配器生成文本条件嵌入，LightGCN学习协同嵌入，通过对称对比目标对齐两种视图，采用层内门控融合机制

Result: 在Yelp和Amazon-Electronics数据集上，所有8个指标均达到最佳结果，Recall@40和NDCG@40相对提升分别达4.53%和4.01%

Conclusion: 跨视图对齐的必要性和门控融合相比后期融合及纯LLM变体的优势得到验证，语言模型在冷启动/长尾场景中主导，图结构在其他场景稳定排序

Abstract: Personalization is a core capability across consumer technologies, streaming,
shopping, wearables, and voice, yet it remains challenged by sparse
interactions, fast content churn, and heterogeneous textual signals. We present
RecMind, an LLM-enhanced graph recommender that treats the language model as a
preference prior rather than a monolithic ranker. A frozen LLM equipped with
lightweight adapters produces text-conditioned user/item embeddings from
titles, attributes, and reviews; a LightGCN backbone learns collaborative
embeddings from the user-item graph. We align the two views with a symmetric
contrastive objective and fuse them via intra-layer gating, allowing language
to dominate in cold/long-tail regimes and graph structure to stabilize rankings
elsewhere. On Yelp and Amazon-Electronics, RecMind attains the best results on
all eight reported metrics, with relative improvements up to +4.53\%
(Recall@40) and +4.01\% (NDCG@40) over strong baselines. Ablations confirm both
the necessity of cross-view alignment and the advantage of gating over late
fusion and LLM-only variants.

</details>


### [125] [A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults](https://arxiv.org/abs/2509.06289)
*Shaoqi Wei,Senling Wang,Hiroshi Kai,Yoshinobu Higami,Ruijun Ma,Tianming Ni,Xiaoqing Wen,Hiroshi Takahashi*

Main category: cs.LG

TL;DR: 提出基于时空图卷积网络的统一框架，用于快速准确预测大规模时序电路中的长周期故障影响概率，显著减少仿真时间同时保持高精度


<details>
  <summary>Details</summary>
Motivation: 静默数据错误(SDEs)会降低安全关键系统的可靠性，功能测试检测SDE相关故障但仿真成本高昂，需要高效的故障影响预测方法

Method: 将门级网表建模为时空图以捕捉拓扑和信号时序，使用专用的空间和时间编码器来高效预测多周期故障影响概率

Result: 在ISCAS-89基准测试中，方法减少仿真时间10倍以上，同时保持高精度(5周期预测的平均绝对误差为0.024)，测试点选择研究表明基于预测FIPs选择观测点能更好检测长周期难检测故障

Conclusion: 该框架可扩展到SoC级测试策略优化，适合下游电子设计自动化流程，支持定量风险评估和效率-精度权衡

Abstract: Silent Data Errors (SDEs) from time-zero defects and aging degrade
safety-critical systems. Functional testing detects SDE-related faults but is
expensive to simulate. We present a unified spatio-temporal graph convolutional
network (ST-GCN) for fast, accurate prediction of long-cycle fault impact
probabilities (FIPs) in large sequential circuits, supporting quantitative risk
assessment. Gate-level netlists are modeled as spatio-temporal graphs to
capture topology and signal timing; dedicated spatial and temporal encoders
predict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the method
reduces simulation time by more than 10x while maintaining high accuracy (mean
absolute error 0.024 for 5-cycle predictions). The framework accepts features
from testability metrics or fault simulation, allowing efficiency-accuracy
trade-offs. A test-point selection study shows that choosing observation points
by predicted FIPs improves detection of long-cycle, hard-to-detect faults. The
approach scales to SoC-level test strategy optimization and fits downstream
electronic design automation flows.

</details>


### [126] [LoaQ: Layer-wise Output Approximation Quantization](https://arxiv.org/abs/2509.06297)
*Li Lin,Xiaojun Wan*

Main category: cs.LG

TL;DR: LoaQ是一种新的层后训练量化方法，通过输出级一致性优化，在LLaMA和Qwen模型上有效提升了权重量化和权重激活联合量化的性能。


<details>
  <summary>Details</summary>
Motivation: 现有层后训练量化方法采用局部视角，只能实现权重激活感知近似，导致近似不充分且与原始输出存在偏差，需要更准确的输出级一致性方法。

Method: 基于主流LLM结构特性的深入理解，提出LoaQ输出近似方法，明确针对输出级一致性，具有简单闭式解，可与现有量化技术正交集成。

Result: 在LLaMA和Qwen模型家族上的实验表明，LoaQ在权重量化和权重激活联合量化中均表现有效，能进一步提升整体量化质量。

Conclusion: LoaQ与现有量化策略无缝集成，具有推动后训练量化前沿发展的强大潜力，更好地实现了输出一致性直觉指导。

Abstract: A natural and intuitive idea in model quantization is to approximate each
component's quantized output to match its original. Layer-wise post-training
quantization (PTQ), though based on this idea, adopts a strictly local view and
can achieve, at best, only activation-aware approximations of weights. As a
result, it often leads to insufficient approximations and practical deviations
from this guiding intuition. Recent work has achieved a more accurate
approximation of linear-layer outputs within the framework of layer-wise PTQ,
but such refinements remain inadequate for achieving alignment with the full
model output. Based on a deeper understanding of the structural characteristics
of mainstream LLMs, we propose $LoaQ$, an output-approximation method for
layer-wise PTQ that explicitly targets output-level consistency. It better
aligns with this intuition and can feature a simple closed-form solution,
making it orthogonal to existing techniques and readily integrable into
existing quantization pipelines. Experiments on the LLaMA and Qwen model
families demonstrate that LoaQ performs effectively in both weight-only and
weight-activation joint quantization. By integrating seamlessly with existing
quantization strategies, it further enhances overall quantization quality and
shows strong potential to advance the frontier of post-training quantization.

</details>


### [127] [WindFM: An Open-Source Foundation Model for Zero-Shot Wind Power Forecasting](https://arxiv.org/abs/2509.06311)
*Hang Fan,Yu Shi,Zongliang Fu,Shuo Chen,Wei Wei,Wei Xu,Jian Li*

Main category: cs.LG

TL;DR: WindFM是一个轻量级生成式基础模型，专门用于概率性风电功率预测，通过离散化-生成框架和Transformer架构，在零样本情况下实现最先进的预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据驱动方法无法泛化到其他站点或难以融入能源领域特定数据的问题，需要开发专门针对风电预测的基础模型。

Method: 采用离散化-生成框架：专用时间序列分词器将连续多变量观测转换为离散分层标记，然后使用仅解码器Transformer在126,000多个站点的1500亿时间步数据上进行自回归预训练。

Result: 仅810万参数的紧凑模型在确定性和概率性任务上实现零样本最先进性能，优于专用模型和更大基础模型，在不同大陆的分布外数据下表现出强适应性。

Conclusion: WindFM展示了学习表示的鲁棒性和可迁移性，为风电预测提供了有效的轻量级基础模型解决方案，模型已公开可用。

Abstract: High-quality wind power forecasting is crucial for the operation of modern
power grids. However, prevailing data-driven paradigms either train a
site-specific model which cannot generalize to other locations or rely on
fine-tuning of general-purpose time series foundation models which are
difficult to incorporate domain-specific data in the energy sector. This paper
introduces WindFM, a lightweight and generative Foundation Model designed
specifically for probabilistic wind power forecasting. WindFM employs a
discretize-and-generate framework. A specialized time-series tokenizer first
converts continuous multivariate observations into discrete, hierarchical
tokens. Subsequently, a decoder-only Transformer learns a universal
representation of wind generation dynamics by autoregressively pre-training on
these token sequences. Using the comprehensive WIND Toolkit dataset comprising
approximately 150 billion time steps from more than 126,000 sites, WindFM
develops a foundational understanding of the complex interplay between
atmospheric conditions and power output. Extensive experiments demonstrate that
our compact 8.1M parameter model achieves state-of-the-art zero-shot
performance on both deterministic and probabilistic tasks, outperforming
specialized models and larger foundation models without any fine-tuning. In
particular, WindFM exhibits strong adaptiveness under out-of-distribution data
from a different continent, demonstrating the robustness and transferability of
its learned representations. Our pre-trained model is publicly available at
https://github.com/shiyu-coder/WindFM.

</details>


### [128] [Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix](https://arxiv.org/abs/2509.06314)
*Mehmet Can Yavuz,Berrin Yanikoglu*

Main category: cs.LG

TL;DR: 本文提出了一个冗余指数rho(C)来直接量化潜在表示中的维度间依赖性，通过分析耦合矩阵并与正态分布比较来提供紧凑、可解释的表示质量度量。


<details>
  <summary>Details</summary>
Motivation: 深度网络经常产生冗余的潜在空间，多个坐标编码重叠信息，降低了有效容量并阻碍泛化。标准指标如准确率或重建损失只能提供间接证据，无法将冗余作为失败模式进行隔离。

Method: 引入冗余指数rho(C)，通过分析从潜在表示导出的耦合矩阵，并通过能量距离将其非对角线统计量与正态分布进行比较，来量化维度间依赖性。

Result: 在MNIST变体、Fashion-MNIST、CIFAR-10和CIFAR-100上的实验验证表明，低rho(C)可靠地预测高分类准确率或低重建误差，而高冗余与性能崩溃相关。TPE优先探索低rho区域，表明rho(C)可以指导神经架构搜索。

Conclusion: rho(C)通过揭示跨模型和任务的冗余作为通用瓶颈，为评估和改进学习表示的效率提供了理论视角和实用工具。

Abstract: A central challenge in representation learning is constructing latent
embeddings that are both expressive and efficient. In practice, deep networks
often produce redundant latent spaces where multiple coordinates encode
overlapping information, reducing effective capacity and hindering
generalization. Standard metrics such as accuracy or reconstruction loss
provide only indirect evidence of such redundancy and cannot isolate it as a
failure mode. We introduce a redundancy index, denoted rho(C), that directly
quantifies inter-dimensional dependencies by analyzing coupling matrices
derived from latent representations and comparing their off-diagonal statistics
against a normal distribution via energy distance. The result is a compact,
interpretable, and statistically grounded measure of representational quality.
We validate rho(C) across discriminative and generative settings on MNIST
variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiple
architectures and hyperparameter optimization strategies. Empirically, low
rho(C) reliably predicts high classification accuracy or low reconstruction
error, while elevated redundancy is associated with performance collapse.
Estimator reliability grows with latent dimension, yielding natural lower
bounds for reliable analysis. We further show that Tree-structured Parzen
Estimators (TPE) preferentially explore low-rho regions, suggesting that rho(C)
can guide neural architecture search and serve as a redundancy-aware
regularization target. By exposing redundancy as a universal bottleneck across
models and tasks, rho(C) offers both a theoretical lens and a practical tool
for evaluating and improving the efficiency of learned representations.

</details>


### [129] [Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics](https://arxiv.org/abs/2509.06322)
*Jiajun Bao,Nicolas Boullé,Toni J. B. Liu,Raphaël Sarfati,Christopher J. Earls*

Main category: cs.LG

TL;DR: LLMs能够通过上下文学习准确预测偏微分方程的时空动力学，无需微调或自然语言提示，预测精度随上下文长度增加而提高，但随空间离散化变细而下降


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在零样本时间序列预测中表现出的新兴上下文学习能力，特别是对偏微分方程解的时空动力学外推能力

Method: 使用文本训练的基础模型处理离散化偏微分方程解，分析多步滚动预测中的误差积累，研究token级输出分布以理解模型内部处理机制

Result: 预测精度随时间上下文长度增加而提高，但在更细空间离散化时下降；多步预测误差随时间范围代数增长；发现一致的ICL进展模式：从语法模式模仿到高熵探索阶段，最终形成数值基础预测

Conclusion: LLMs能够通过上下文学习有效处理偏微分方程解的时空外推，表现出可预测的神经缩放规律，为理解模型内部处理机制提供了新见解

Abstract: Large language models (LLMs) have demonstrated emergent in-context learning
(ICL) capabilities across a range of tasks, including zero-shot time-series
forecasting. We show that text-trained foundation models can accurately
extrapolate spatiotemporal dynamics from discretized partial differential
equation (PDE) solutions without fine-tuning or natural language prompting.
Predictive accuracy improves with longer temporal contexts but degrades at
finer spatial discretizations. In multi-step rollouts, where the model
recursively predicts future spatial states over multiple time steps, errors
grow algebraically with the time horizon, reminiscent of global error
accumulation in classical finite-difference solvers. We interpret these trends
as in-context neural scaling laws, where prediction quality varies predictably
with both context length and output length. To better understand how LLMs are
able to internally process PDE solutions so as to accurately roll them out, we
analyze token-level output distributions and uncover a consistent ICL
progression: beginning with syntactic pattern imitation, transitioning through
an exploratory high-entropy phase, and culminating in confident, numerically
grounded predictions.

</details>


### [130] [Exploring approaches to computational representation and classification of user-generated meal logs](https://arxiv.org/abs/2509.06330)
*Guanlan Hu,Adit Anand,Pooja M. Desai,Iñigo Urteaga,Lena Mamykina*

Main category: cs.LG

TL;DR: 机器学习结合域特定增强技术能够准确分类自由文本餐记录是否符合营养目标，性能超过自我评估


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用机器学习和域特定增强技术来分析患者生成的健康数据（自由文本餐记录），以支持精准医疗中的个性化营养指导

Method: 使用TFIDF和BERT文本嵌入，统计学习和多层感知器分类器，结合本体论、成分解析器和营养素内容等域特定增强信息

Result: 即使没有增强，机器学习也超过了自我评估；最佳组合达到更高准确度；成分解析、食物实体和营养素信息增强在多个营养目标上表现良好

Conclusion: 机器学习能够利用非结构化自由文本餐记录可靠地分类餐饮是否符合特定营养目标，特别是结合营养域知识时效果更佳，为精准健康领域的患者中心营养指导提供了潜力

Abstract: This study examined the use of machine learning and domain specific
enrichment on patient generated health data, in the form of free text meal
logs, to classify meals on alignment with different nutritional goals. We used
a dataset of over 3000 meal records collected by 114 individuals from a
diverse, low income community in a major US city using a mobile app. Registered
dietitians provided expert judgement for meal to goal alignment, used as gold
standard for evaluation. Using text embeddings, including TFIDF and BERT, and
domain specific enrichment information, including ontologies, ingredient
parsers, and macronutrient contents as inputs, we evaluated the performance of
logistic regression and multilayer perceptron classifiers using accuracy,
precision, recall, and F1 score against the gold standard and self assessment.
Even without enrichment, ML outperformed self assessments of individuals who
logged meals, and the best performing combination of ML classifier with
enrichment achieved even higher accuracies. In general, ML classifiers with
enrichment of Parsed Ingredients, Food Entities, and Macronutrients information
performed well across multiple nutritional goals, but there was variability in
the impact of enrichment and classification algorithm on accuracy of
classification for different nutritional goals. In conclusion, ML can utilize
unstructured free text meal logs and reliably classify whether meals align with
specific nutritional goals, exceeding self assessments, especially when
incorporating nutrition domain knowledge. Our findings highlight the potential
of ML analysis of patient generated health data to support patient centered
nutrition guidance in precision healthcare.

</details>


### [131] [A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning in LLMs](https://arxiv.org/abs/2509.06332)
*Roussel Rahman,Aashwin Ananda Mishra*

Main category: cs.LG

TL;DR: LLM在数值推理方面表现出算法执行能力强但启发式搜索能力弱的特点，在确定性算法任务上表现优异但在需要组合搜索的数学谜题上表现不佳


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型数值推理的鲁棒性，探究其在从基础运算到复杂组合谜题等不同复杂度数学问题上的表现

Method: 测试多个先进LLM智能体在100个问题挑战上的表现，问题分为四类：基础算术、高级运算、素数检查、24点数字谜题

Result: 智能体在前三类需要确定性算法执行的任务上准确率高，但在需要启发式搜索的24点谜题上持续失败

Conclusion: LLM的数值推理能力更类似于复杂的模式匹配而非灵活的分析思维，主要局限于回忆和执行已知算法，缺乏生成性解决问题的能力

Abstract: Large Language Models (LLMs) have demonstrated remarkable emergent
capabilities, yet the robustness of their numerical reasoning remains an open
question. While standard benchmarks evaluate LLM reasoning on complex problem
sets using aggregated metrics, they often obscure foundational weaknesses. In
this work, we probe LLM mathematical numeracy by evaluating performance on
problems of escalating complexity, from constituent operations to combinatorial
puzzles. We test several state-of-the-art LLM-based agents on a 100-problem
challenge comprising four categories: (1) basic arithmetic, (2) advanced
operations, (3) primality checking, and (4) the Game of 24 number puzzle. Our
results show that while the agents achieved high accuracy on the first three
categories, which require deterministic algorithmic execution, they
consistently failed at the number puzzle, underlining its demand for a
heuristic search over a large combinatorial space to be a significant
bottleneck. These findings reveal that the agents' proficiency is largely
confined to recalling and executing known algorithms, rather than performing
generative problem-solving. This suggests their apparent numerical reasoning is
more akin to sophisticated pattern-matching than flexible, analytical thought,
limiting their potential for tasks that require novel or creative numerical
insights.

</details>


### [132] [Ban&Pick: Achieving Free Performance Gains and Inference Speedup via Smarter Routing in MoE-LLMs](https://arxiv.org/abs/2509.06346)
*Yuanteng Chen,Peisong Wang,Yuantian Shao,Jian Cheng*

Main category: cs.LG

TL;DR: Ban&Pick是一种无需重新训练的后处理策略，通过识别关键专家并动态剪枝冗余专家，提升稀疏混合专家模型的性能和推理速度


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型的路由器在预训练中过早收敛并强制平衡使用，导致关键专家利用不足和冗余问题，限制了模型性能和效率的潜力

Method: 提出Ban&Pick后训练策略：Pick组件发现并强化对性能影响大的关键专家；Ban组件根据层和token敏感性动态剪枝冗余专家

Result: 在DeepSeek和Qwen3等细粒度MoE-LLM上，数学、代码和通用推理基准测试显示准确率显著提升（如Qwen3-30B-A3B在AIME2024从80.67提升到84.66），推理速度提升1.25倍

Conclusion: Ban&Pick无需重新训练或架构修改即可为MoE模型带来免费的性能增益和推理加速，解决了现有路由策略的关键限制

Abstract: Sparse Mixture-of-Experts (MoE) has become a key architecture for scaling
large language models (LLMs) efficiently. Recent fine-grained MoE designs
introduce hundreds of experts per layer, with multiple experts activated per
token, enabling stronger specialization. However, during pre-training, routers
are optimized mainly for stability and robustness: they converge prematurely
and enforce balanced usage, limiting the full potential of model performance
and efficiency. In this work, we uncover two overlooked issues: (i) a few
highly influential experts are underutilized due to premature and balanced
routing decisions; and (ii) enforcing a fixed number of active experts per
token introduces substantial redundancy. Instead of retraining models or
redesigning MoE architectures, we introduce Ban&Pick, a post-training,
plug-and-play strategy for smarter MoE routing. Pick discovers and reinforces
key experts-a small group with outsized impact on performance-leading to
notable accuracy gains across domains. Ban complements this by dynamically
pruning redundant experts based on layer and token sensitivity, delivering
faster inference with minimal accuracy loss. Experiments on fine-grained
MoE-LLMs (DeepSeek, Qwen3) across math, code, and general reasoning benchmarks
demonstrate that Ban&Pick delivers free performance gains and inference
acceleration without retraining or architectural changes. For instance, on
Qwen3-30B-A3B, it improves accuracy from 80.67 to 84.66 on AIME2024 and from
65.66 to 68.18 on GPQA-Diamond, while accelerating inference by 1.25x under the
vLLM.

</details>


### [133] [Breaking SafetyCore: Exploring the Risks of On-Device AI Deployment](https://arxiv.org/abs/2509.06371)
*Victor Guyomard,Mathis Mauvisseau,Marie Paindavoine*

Main category: cs.LG

TL;DR: 本文通过分析Android系统服务SafetyCore中的图像内容检测AI模型，揭示了设备端AI模型的安全漏洞，展示了攻击者如何提取和操纵模型来绕过检测保护。


<details>
  <summary>Details</summary>
Motivation: 随着硬件和软件改进，越来越多的AI模型部署在设备端，这虽然提高了隐私性和降低了延迟，但也带来了与传统软件不同的安全风险。

Method: 通过真实案例研究SafetyCore Android系统服务，分析其敏感图像内容检测机制，演示如何提取和操纵设备端AI模型来绕过检测。

Result: 研究发现设备端AI模型存在可被提取和操纵的漏洞，攻击者能够有效绕过检测保护，使防护机制失效。

Conclusion: 设备端AI模型存在严重安全漏洞，需要加强安全防护措施来防止模型被提取和操纵。

Abstract: Due to hardware and software improvements, an increasing number of AI models
are deployed on-device. This shift enhances privacy and reduces latency, but
also introduces security risks distinct from traditional software. In this
article, we examine these risks through the real-world case study of
SafetyCore, an Android system service incorporating sensitive image content
detection. We demonstrate how the on-device AI model can be extracted and
manipulated to bypass detection, effectively rendering the protection
ineffective. Our analysis exposes vulnerabilities of on-device AI models and
provides a practical demonstration of how adversaries can exploit them.

</details>


### [134] [Variational Garrote for Statistical Physics-based Sparse and Robust Variable Selection](https://arxiv.org/abs/2509.06383)
*Hyungjoon Soh,Dongha Lee,Vipul Periwal,Junghyo Jo*

Main category: cs.LG

TL;DR: 本文重新审视并改进了基于统计物理的变分门控(VG)方法，用于高维数据中的稀疏变量选择。通过引入自动微分技术，VG在高度稀疏场景下表现优于Ridge和LASSO回归，并能识别相关变量的正确数量。


<details>
  <summary>Details</summary>
Motivation: 在大数据时代，从高维数据中选择关键变量变得越来越重要。稀疏回归通过促进模型简洁性和可解释性为此提供了强大工具，但现有方法在高度稀疏场景下的表现有待改进。

Method: 重新利用基于统计物理的变分门控(VG)方法，引入显式特征选择自旋变量，利用变分推理推导可处理的损失函数，并整合现代自动微分技术实现可扩展的优化。

Result: VG在高度稀疏场景下表现优异，比Ridge和LASSO回归提供更一致和鲁棒的变量选择。发现了一个尖锐的过渡现象：当引入多余变量时，泛化能力急剧下降，选择变量的不确定性增加。

Conclusion: VG方法在稀疏建模方面具有强大潜力，适用于压缩感知和机器学习中的模型剪枝等多种应用场景，能够有效识别真实数据中的关键预测变量。

Abstract: Selecting key variables from high-dimensional data is increasingly important
in the era of big data. Sparse regression serves as a powerful tool for this
purpose by promoting model simplicity and explainability. In this work, we
revisit a valuable yet underutilized method, the statistical physics-based
Variational Garrote (VG), which introduces explicit feature selection spin
variables and leverages variational inference to derive a tractable loss
function. We enhance VG by incorporating modern automatic differentiation
techniques, enabling scalable and efficient optimization. We evaluate VG on
both fully controllable synthetic datasets and complex real-world datasets. Our
results demonstrate that VG performs especially well in highly sparse regimes,
offering more consistent and robust variable selection than Ridge and LASSO
regression across varying levels of sparsity. We also uncover a sharp
transition: as superfluous variables are admitted, generalization degrades
abruptly and the uncertainty of the selection variables increases. This
transition point provides a practical signal for estimating the correct number
of relevant variables, an insight we successfully apply to identify key
predictors in real-world data. We expect that VG offers strong potential for
sparse modeling across a wide range of applications, including compressed
sensing and model pruning in machine learning.

</details>


### [135] [Beyond the Pre-Service Horizon: Infusing In-Service Behavior for Improved Financial Risk Forecasting](https://arxiv.org/abs/2509.06385)
*Senhao Liu,Zhiyu Guo,Zhiyuan Ji,Yueguo Chen,Yateng Tang,Yunhai Wang,Xuehao Zheng,Xiang Ao*

Main category: cs.LG

TL;DR: 提出MGKD框架，通过知识蒸馏整合服务中用户行为数据来提升服务前风险评估性能


<details>
  <summary>Details</summary>
Motivation: 传统金融风险管理将服务前风险评估和服务中违约检测分开建模，缺乏有效整合

Method: 采用多粒度知识蒸馏策略（粗粒度、细粒度和自蒸馏），教师模型基于历史服务中数据训练，指导学生模型的服务前风险预测

Result: 在腾讯移动支付大规模真实数据集上验证了离线和在线的有效性

Conclusion: MGKD框架能有效提升服务前风险评估性能，通过知识蒸馏实现服务中行为模式向服务前预测的迁移

Abstract: Typical financial risk management involves distinct phases for pre-service
risk assessment and in-service default detection, often modeled separately.
This paper proposes a novel framework, Multi-Granularity Knowledge Distillation
(abbreviated as MGKD), aimed at improving pre-service risk prediction through
the integration of in-service user behavior data. MGKD follows the idea of
knowledge distillation, where the teacher model, trained on historical
in-service data, guides the student model, which is trained on pre-service
data. By using soft labels derived from in-service data, the teacher model
helps the student model improve its risk prediction prior to service
activation. Meanwhile, a multi-granularity distillation strategy is introduced,
including coarse-grained, fine-grained, and self-distillation, to align the
representations and predictions of the teacher and student models. This
approach not only reinforces the representation of default cases but also
enables the transfer of key behavioral patterns associated with defaulters from
the teacher to the student model, thereby improving the overall performance of
pre-service risk assessment. Moreover, we adopt a re-weighting strategy to
mitigate the model's bias towards the minority class. Experimental results on
large-scale real-world datasets from Tencent Mobile Payment demonstrate the
effectiveness of our proposed approach in both offline and online scenarios.

</details>


### [136] [Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints](https://arxiv.org/abs/2509.06395)
*Lili Chen,Changyang She,Jingge Zhu,Jamie Evans*

Main category: cs.LG

TL;DR: 提出了一种基于GNN和拉格朗日对偶优化的理论保障框架JCPGNN-M，用于无线通信系统中满足QoS约束的资源分配，在保持性能的同时显著提升计算效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法通过启发式惩罚项处理QoS约束，缺乏理论收敛保证且在实际场景中经常无法满足服务质量要求，需要一种理论严谨的可扩展解决方案。

Method: 1. 扩展WMMSE算法到多通道QoS约束场景(eWMMSE)；2. 开发支持多通道分配的GNN算法(JCPGNN-M)；3. 将GNN与拉格朗日原始-对偶优化方法结合，在拉格朗日框架内训练GNN。

Result: JCPGNN-M在性能上匹配eWMMSE，同时在推理速度、对大网络的泛化能力以及不完美信道状态信息下的鲁棒性方面都有显著提升。

Conclusion: 该工作为未来无线网络中的约束资源分配提供了一个可扩展且理论严谨的解决方案，确保了QoS约束的满足和收敛到稳定点。

Abstract: Meeting minimum data rate constraints is a significant challenge in wireless
communication systems, particularly as network complexity grows. Traditional
deep learning approaches often address these constraints by incorporating
penalty terms into the loss function and tuning hyperparameters empirically.
However, this heuristic treatment offers no theoretical convergence guarantees
and frequently fails to satisfy QoS requirements in practical scenarios.
Building upon the structure of the WMMSE algorithm, we first extend it to a
multi-channel setting with QoS constraints, resulting in the enhanced WMMSE
(eWMMSE) algorithm, which is provably convergent to a locally optimal solution
when the problem is feasible. To further reduce computational complexity and
improve scalability, we develop a GNN-based algorithm, JCPGNN-M, capable of
supporting simultaneous multi-channel allocation per user. To overcome the
limitations of traditional deep learning methods, we propose a principled
framework that integrates GNN with a Lagrangian-based primal-dual optimization
method. By training the GNN within the Lagrangian framework, we ensure
satisfaction of QoS constraints and convergence to a stationary point.
Extensive simulations demonstrate that JCPGNN-M matches the performance of
eWMMSE while offering significant gains in inference speed, generalization to
larger networks, and robustness under imperfect channel state information. This
work presents a scalable and theoretically grounded solution for constrained
resource allocation in future wireless networks.

</details>


### [137] [NeuroDeX: Unlocking Diverse Support in Decompiling Deep Neural Network Executables](https://arxiv.org/abs/2509.06402)
*Yilin Li,Guozhu Meng,Mingyang Sun,Yanzhong Wang,Kun Sun,Hailong Chang,Yuekang Li*

Main category: cs.LG

TL;DR: NeuroDeX是一个利用LLM语义理解和动态分析的DNN可执行文件反编译器，能够处理编译优化和量化模型，实现操作符识别、属性恢复和模型重建。


<details>
  <summary>Details</summary>
Motivation: 设备端深度学习模型需求广泛，但编译后的可执行文件面临逆向工程威胁。现有反编译方法难以处理编译优化和量化编译模型。

Method: 结合LLM的语义理解能力和动态分析，进行操作符类型识别、操作符属性恢复和模型重建。

Result: 在96个DNN可执行文件上测试，非量化可执行文件可反编译为几乎相同的高级模型，量化可执行文件可恢复功能相似模型，平均top-1准确率达72%。

Conclusion: NeuroDeX相比之前的DNN可执行文件反编译器提供了更全面有效的解决方案。

Abstract: On-device deep learning models have extensive real world demands. Deep
learning compilers efficiently compile models into executables for deployment
on edge devices, but these executables may face the threat of reverse
engineering. Previous studies have attempted to decompile DNN executables, but
they face challenges in handling compilation optimizations and analyzing
quantized compiled models. In this paper, we present NeuroDeX to unlock diverse
support in decompiling DNN executables. NeuroDeX leverages the semantic
understanding capabilities of LLMs along with dynamic analysis to accurately
and efficiently perform operator type recognition, operator attribute recovery
and model reconstruction. NeuroDeX can recover DNN executables into high-level
models towards compilation optimizations, different architectures and quantized
compiled models. We conduct experiments on 96 DNN executables across 12 common
DNN models. Extensive experimental results demonstrate that NeuroDeX can
decompile non-quantized executables into nearly identical high-level models.
NeuroDeX can recover functionally similar high-level models for quantized
executables, achieving an average top-1 accuracy of 72%. NeuroDeX offers a more
comprehensive and effective solution compared to previous DNN executables
decompilers.

</details>


### [138] [CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup](https://arxiv.org/abs/2509.06419)
*Xudong Mou,Rui Wang,Tiejun Wang,Renyu Yang,Shiru Chen,Jie Sun,Tianyu Wo,Xudong Liu*

Main category: cs.LG

TL;DR: CAPMix是一个可控异常增强框架，通过CutAddPaste机制注入多样化异常，使用标签修正策略减少异常偏移，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列异常检测中标注异常稀缺的问题，现有异常假设方法存在补丁式生成和异常偏移两个根本性限制。

Method: 设计CutAddPaste机制进行目标化异常注入，引入标签修正策略自适应优化异常标签，在时序卷积网络中使用双空间混合增强决策边界。

Result: 在AIOps、UCR、SWaT、WADI和ESA五个基准数据集上的广泛实验显示，CAPMix相比最先进基线方法取得显著改进，对污染训练数据具有更强鲁棒性。

Conclusion: CAPMix有效解决了异常注入中的补丁式生成和异常偏移问题，为时间序列异常检测提供了有效的可控异常增强解决方案。

Abstract: Time series anomaly detection (TSAD) is a vital yet challenging task,
particularly in scenarios where labeled anomalies are scarce and temporal
dependencies are complex. Recent anomaly assumption (AA) approaches alleviate
the lack of anomalies by injecting synthetic samples and training
discriminative models. Despite promising results, these methods often suffer
from two fundamental limitations: patchy generation, where scattered anomaly
knowledge leads to overly simplistic or incoherent anomaly injection, and
Anomaly Shift, where synthetic anomalies either resemble normal data too
closely or diverge unrealistically from real anomalies, thereby distorting
classification boundaries. In this paper, we propose CAPMix, a controllable
anomaly augmentation framework that addresses both issues. First, we design a
CutAddPaste mechanism to inject diverse and complex anomalies in a targeted
manner, avoiding patchy generation. Second, we introduce a label revision
strategy to adaptively refine anomaly labels, reducing the risk of anomaly
shift. Finally, we employ dual-space mixup within a temporal convolutional
network to enforce smoother and more robust decision boundaries. Extensive
experiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and
ESA, demonstrate that CAPMix achieves significant improvements over
state-of-the-art baselines, with enhanced robustness against contaminated
training data. The code is available at https://github.com/alsike22/CAPMix.

</details>


### [139] [CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction](https://arxiv.org/abs/2509.06465)
*Hongzong Li,Jiahao Ma,Zhanpeng Shi,Fanming Jin,Ye-Fan Hu,Jian-Dong Huang*

Main category: cs.LG

TL;DR: CAME-AB是一个用于抗体结合位点预测的多模态注意力框架，融合了五种生物特征模态，通过自适应模态融合和专家混合机制实现优异的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单视图特征，无法准确识别抗原上的抗体特异性结合位点，存在表示和预测的双重局限性。

Method: 提出跨模态注意力框架，整合氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征和GCN精化生化图五种模态，采用自适应模态融合模块、Transformer编码器和MoE模块，结合监督对比学习和随机权重平均。

Result: 在基准抗体-抗原数据集上，CAME-AB在精确率、召回率、F1分数、AUC-ROC和MCC等多个指标上均优于强基线方法。

Conclusion: CAME-AB通过多模态特征整合和先进的架构设计，显著提升了抗体结合位点预测的性能，消融研究验证了各组件有效性。

Abstract: Antibody binding site prediction plays a pivotal role in computational
immunology and therapeutic antibody design. Existing sequence or structure
methods rely on single-view features and fail to identify antibody-specific
binding sites on the antigens-a dual limitation in representation and
prediction. In this paper, we propose CAME-AB, a novel Cross-modality Attention
framework with a Mixture-of-Experts (MoE) backbone for robust antibody binding
site prediction. CAME-AB integrates five biologically grounded modalities,
including raw amino acid encodings, BLOSUM substitution profiles, pretrained
language model embeddings, structure-aware features, and GCN-refined
biochemical graphs-into a unified multimodal representation. To enhance
adaptive cross-modal reasoning, we propose an adaptive modality fusion module
that learns to dynamically weight each modality based on its global relevance
and input-specific contribution. A Transformer encoder combined with an MoE
module further promotes feature specialization and capacity expansion. We
additionally incorporate a supervised contrastive learning objective to
explicitly shape the latent space geometry, encouraging intra-class compactness
and inter-class separability. To improve optimization stability and
generalization, we apply stochastic weight averaging during training. Extensive
experiments on benchmark antibody-antigen datasets demonstrate that CAME-AB
consistently outperforms strong baselines on multiple metrics, including
Precision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further
validate the effectiveness of each architectural component and the benefit of
multimodal feature integration. The model implementation details and the codes
are available on https://anonymous.4open.science/r/CAME-AB-C525

</details>


### [140] [DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT](https://arxiv.org/abs/2509.06483)
*Guanjie Cheng,Boyi Li,Peihan Wu,Feiyi Chen,Xinkui Zhao,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: 提出了DyC-STG框架，通过事件驱动的动态图模块和因果推理模块解决IoT时空数据可信度分析问题，在真实数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: IoT传感器产生大量时空数据流，但数据可信度是关键挑战。现有时空图模型在动态人本环境中存在两个根本局限：静态图拓扑无法捕捉物理事件驱动动态，以及容易混淆虚假相关与真实因果关系

Method: DyC-STG框架包含两个协同贡献：事件驱动的动态图模块实时调整图拓扑以反映物理状态变化，因果推理模块通过严格强制执行时间优先性来提取因果感知表示

Result: 综合实验表明DyC-STG建立了新的最先进水平，比最强基线高出1.4个百分点，F1分数最高达到0.930

Conclusion: 该框架有效解决了IoT时空数据可信度分析的关键挑战，为相关领域研究提供了新的数据集和方法论

Abstract: The wide spreading of Internet of Things (IoT) sensors generates vast
spatio-temporal data streams, but ensuring data credibility is a critical yet
unsolved challenge for applications like smart homes. While spatio-temporal
graph (STG) models are a leading paradigm for such data, they often fall short
in dynamic, human-centric environments due to two fundamental limitations: (1)
their reliance on static graph topologies, which fail to capture physical,
event-driven dynamics, and (2) their tendency to confuse spurious correlations
with true causality, undermining robustness in human-centric environments. To
address these gaps, we propose the Dynamic Causal Spatio-Temporal Graph Network
(DyC-STG), a novel framework designed for real-time data credibility analysis
in IoT. Our framework features two synergistic contributions: an event-driven
dynamic graph module that adapts the graph topology in real-time to reflect
physical state changes, and a causal reasoning module to distill causally-aware
representations by strictly enforcing temporal precedence. To facilitate the
research in this domain we release two new real-world datasets. Comprehensive
experiments show that DyC-STG establishes a new state-of-the-art, outperforming
the strongest baselines by 1.4 percentage points and achieving an F1-Score of
up to 0.930.

</details>


### [141] [A machine-learned expression for the excess Gibbs energy](https://arxiv.org/abs/2509.06484)
*Marco Hoffmann,Thomas Specht,Quirin Göttl,Jakob Burger,Stephan Mandt,Hans Hasse,Fabian Jirasek*

Main category: cs.LG

TL;DR: HANNA模型通过将物理定律作为硬约束集成到神经网络中，准确预测多组分混合物的过量吉布斯能，在精度和范围上明显优于现有方法


<details>
  <summary>Details</summary>
Motivation: 预测多组分混合物的过量吉布斯能仅从其分子结构出发是一个长期挑战，这对于化学工程和化学中的热力学性质建模至关重要

Method: 将物理定律作为硬约束集成到灵活的神经网络中，使用杜伊斯堡数据库的二元混合物实验数据进行端到端训练，开发新型替代求解器包含液液平衡数据，应用几何投影方法实现多组分混合物的稳健外推

Result: HANNA模型提供了优异预测性能，在准确性和范围上明显优于最先进的基准方法

Conclusion: 该模型实现了热力学一致预测，无需额外参数即可外推到多组分体系，训练模型和代码已公开提供，并在MLPROP网站上提供交互界面

Abstract: The excess Gibbs energy plays a central role in chemical engineering and
chemistry, providing a basis for modeling the thermodynamic properties of
liquid mixtures. Predicting the excess Gibbs energy of multi-component mixtures
solely from the molecular structures of their components is a long-standing
challenge. In this work, we address this challenge by integrating physical laws
as hard constraints within a flexible neural network. The resulting model,
HANNA, was trained end-to-end on an extensive experimental dataset for binary
mixtures from the Dortmund Data Bank, guaranteeing thermodynamically consistent
predictions. A novel surrogate solver developed in this work enabled the
inclusion of liquid-liquid equilibrium data in the training process.
Furthermore, a geometric projection method was applied to enable robust
extrapolations to multi-component mixtures, without requiring additional
parameters. We demonstrate that HANNA delivers excellent predictions, clearly
outperforming state-of-the-art benchmark methods in accuracy and scope. The
trained model and corresponding code are openly available, and an interactive
interface is provided on our website, MLPROP.

</details>


### [142] [On optimal solutions of classical and sliced Wasserstein GANs with non-Gaussian data](https://arxiv.org/abs/2509.06505)
*Yu-Jui Huang,Hsin-Hua Shen,Yu-Chih Huang,Wan-Yi Lin,Shih-Chun Lin*

Main category: cs.LG

TL;DR: 本文研究了Wasserstein GAN（WGAN）在非LQG设置下的最优参数选择问题，推导出了一维WGAN的闭式最优参数解，并通过切片Wasserstein框架扩展到高维情况。


<details>
  <summary>Details</summary>
Motivation: 现有WGAN参数选择方法主要局限于线性二次高斯（LQG）设置，缺乏对非线性激活函数和非高斯数据的理论最优参数研究。

Method: 推导一维WGAN在非线性激活函数和非高斯数据下的闭式最优参数；采用切片Wasserstein框架扩展到高维情况，用原始数据联合分布约束替代边际分布约束。

Result: 证明了线性生成器在切片WGAN中对于非高斯数据的渐近最优性；实验表明闭式参数在高斯和拉普拉斯分布下具有良好的收敛性；相比r-PCA方法，计算资源需求更少且性能相当。

Conclusion: 本文提出的闭式WGAN参数选择方法突破了LQG限制，为非线性网络和非高斯数据提供了理论最优解，在计算效率和性能方面均有优势。

Abstract: The generative adversarial network (GAN) aims to approximate an unknown
distribution via a parameterized neural network (NN). While GANs have been
widely applied in reinforcement and semisupervised learning as well as computer
vision tasks, selecting their parameters often needs an exhaustive search and
only a few selection methods can be proved to be theoretically optimal. One of
the most promising GAN variants is the Wasserstein GAN (WGAN). Prior work on
optimal parameters for WGAN is limited to the linear-quadratic-Gaussian (LQG)
setting, where the NN is linear and the data is Gaussian. In this paper, we
focus on the characterization of optimal WGAN parameters beyond the LQG
setting. We derive closed-form optimal parameters for one-dimensional WGANs
when the NN has non-linear activation functions and the data is non-Gaussian.
To extend this to high-dimensional WGANs, we adopt the sliced Wasserstein
framework and replace the constraint on marginal distributions of the randomly
projected data by a constraint on the joint distribution of the original
(unprojected) data. We show that the linear generator can be asymptotically
optimal for sliced WGAN with non-Gaussian data. Empirical studies show that our
closed-form WGAN parameters have good convergence behavior with data under both
Gaussian and Laplace distributions. Also, compared to the r principal component
analysis (r-PCA) solution, our proposed solution for sliced WGAN can achieve
the same performance while requiring less computational resources.

</details>


### [143] [QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients](https://arxiv.org/abs/2509.06516)
*Zongheng Guo,Tao Chen,Manuela Ferrario*

Main category: cs.LG

TL;DR: QualityFM是一个用于PPG和ECG生理信号质量评估的多模态基础模型，通过大规模预训练和双轨架构实现跨任务的信号质量理解


<details>
  <summary>Details</summary>
Motivation: ICU和手术室中PPG和ECG信号质量差、不完整和不一致的问题导致误报警和诊断不准确，现有方法泛化性有限且需要大量标注数据

Method: 采用双轨架构处理不同质量的配对生理信号，使用自蒸馏策略，集成窗口稀疏注意力机制的Transformer模型，结合蒸馏损失和重建损失的复合损失函数

Result: 在三个临床任务上验证效果：心室心动过速误报警检测、房颤识别、以及从PPG和ECG信号估计动脉血压

Conclusion: QualityFM展示了在生理信号质量理解方面的有效性和实用价值，能够提升临床监测的准确性和可靠性

Abstract: Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded in
intesive care unit (ICU) and operating room (OR). However, the high incidence
of poor, incomplete, and inconsistent signal quality, can lead to false alarms
or diagnostic inaccuracies. The methods explored so far suffer from limited
generalizability, reliance on extensive labeled data, and poor cross-task
transferability. To overcome these challenges, we introduce QualityFM, a novel
multimodal foundation model for these physiological signals, designed to
acquire a general-purpose understanding of signal quality. Our model is
pre-trained on an large-scale dataset comprising over 21 million 30-second
waveforms and 179,757 hours of data. Our approach involves a dual-track
architecture that processes paired physiological signals of differing quality,
leveraging a self-distillation strategy where an encoder for high-quality
signals is used to guide the training of an encoder for low-quality signals. To
efficiently handle long sequential signals and capture essential local
quasi-periodic patterns, we integrate a windowed sparse attention mechanism
within our Transformer-based model. Furthermore, a composite loss function,
which combines direct distillation loss on encoder outputs with indirect
reconstruction loss based on power and phase spectra, ensures the preservation
of frequency-domain characteristics of the signals. We pre-train three models
with varying parameter counts (9.6 M to 319 M) and demonstrate their efficacy
and practical value through transfer learning on three distinct clinical tasks:
false alarm of ventricular tachycardia detection, the identification of atrial
fibrillation and the estimation of arterial blood pressure (ABP) from PPG and
ECG signals.

</details>


### [144] [Lane Change Intention Prediction of two distinct Populations using a Transformer](https://arxiv.org/abs/2509.06529)
*Francesco De Cristofaro,Cornelia Lex,Jia Hu,Arno Eichberger*

Main category: cs.LG

TL;DR: 这篇论文研究了变换器模型在不同数据集上的车道更换意图预测性能，发现单一数据集训练的模型在异构数据集上性能大幅下降，而多数据集训练可提升模型的演化能力。


<details>
  <summary>Details</summary>
Motivation: 目前大多数车道更换意图预测算法仅在单一数据集上训练和测试，缺乏对模型在不同地区和人群中演化能力的验证。研究者想要测试变换器模型在不同数据集上的表现，以评估其真实场景下的应用潜力。

Method: 使用LevelX在德国和香港收集的两个数据集，对变换器模型进行车道更换意图预测。测试了单一数据集训练的模型在异构数据集上的表现，以及同时在两个数据集上训练的模型性能。

Result: 单一数据集训练的变换器模型在异构数据集上性能大幅下降，准确率仅为39.43%。然而，当模型同时在两个数据集上训练时，准确率可达到86.71%，显著提升了模型的演化能力。

Conclusion: 研究结果显示，仅在单一数据集上训练的车道更换意图预测模型在异构数据集上表现差异，而多数据集训练能够有效提升模型的演化性能。这为实际应用中的模型部署提供了重要的实践指导。

Abstract: As a result of the growing importance of lane change intention prediction for
a safe and efficient driving experience in complex driving scenarios,
researchers have in recent years started to train novel machine learning
algorithms on available datasets with promising results. A shortcoming of this
recent research effort, though, is that the vast majority of the proposed
algorithms are trained on a single datasets. In doing so, researchers failed to
test if their algorithm would be as effective if tested on a different dataset
and, by extension, on a different population with respect to the one on which
they were trained. In this article we test a transformer designed for lane
change intention prediction on two datasets collected by LevelX in Germany and
Hong Kong. We found that the transformer's accuracy plummeted when tested on a
population different to the one it was trained on with accuracy values as low
as 39.43%, but that when trained on both populations simultaneously it could
achieve an accuracy as high as 86.71%. - This work has been submitted to the
IEEE for possible publication. Copyright may be transferred without notice,
after which this version may no longer be accessible.

</details>


### [145] [Outcome-based Exploration for LLM Reasoning](https://arxiv.org/abs/2509.06941)
*Yuda Song,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: 强化学习虽然能提高大语言模型的推理准确性，但会导致生成多样性下降。本文提出基于结果的探索方法，通过历史探索和批量探索两种算法，在保持准确性的同时缓解多样性崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 基于结果的强化学习虽然能显著提高大语言模型的推理准确性，但会系统性降低生成多样性，这在需要多样性的实际应用场景中会影响性能。研究发现RL即使在训练集上也会降低有效多样性，且多样性下降会从已解决问题传播到未解决问题。

Method: 提出基于结果的探索方法：1）历史探索：使用UCB风格的奖励鼓励罕见答案；2）批量探索：惩罚批次内重复以促进测试时多样性。在Llama和Qwen模型上进行标准数学竞赛实验验证。

Result: 实验表明两种方法都能在提高准确性的同时缓解多样性崩溃问题。理论方面通过新的基于结果的多臂老虎机模型形式化了基于结果探索的益处。

Conclusion: 这些贡献为开发既能增强推理能力又不牺牲多样性（对可扩展部署至关重要）的RL方法指明了实用路径。

Abstract: Reinforcement learning (RL) has emerged as a powerful method for improving
the reasoning abilities of large language models (LLMs). Outcome-based RL,
which rewards policies solely for the correctness of the final answer, yields
substantial accuracy gains but also induces a systematic loss in generation
diversity. This collapse undermines real-world performance, where diversity is
critical for test-time scaling. We analyze this phenomenon by viewing RL
post-training as a sampling process and show that, strikingly, RL can reduce
effective diversity even on the training set relative to the base model. Our
study highlights two central findings: (i) a transfer of diversity degradation,
where reduced diversity on solved problems propagates to unsolved ones, and
(ii) the tractability of the outcome space, since reasoning tasks admit only a
limited set of distinct answers. Motivated by these insights, we propose
outcome-based exploration, which assigns exploration bonuses according to final
outcomes. We introduce two complementary algorithms: historical exploration,
which encourages rarely observed answers via UCB-style bonuses, and batch
exploration, which penalizes within-batch repetition to promote test-time
diversity. Experiments on standard competition math with Llama and Qwen models
demonstrate that both methods improve accuracy while mitigating diversity
collapse. On the theoretical side, we formalize the benefit of outcome-based
exploration through a new model of outcome-based bandits. Together, these
contributions chart a practical path toward RL methods that enhance reasoning
without sacrificing the diversity essential for scalable deployment.

</details>


### [146] [Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model](https://arxiv.org/abs/2509.06539)
*Duc Huy Le,Rolf Stadler*

Main category: cs.LG

TL;DR: 本文为CAGE-2网络安全防御基准构建了POMDP形式化模型，提出了基于PPO和粒子滤波的BF-PPO方法，在防御策略效果和训练时间上均优于当前最佳方法CARDIFF。


<details>
  <summary>Details</summary>
Motivation: CAGE-2是网络安全防御策略的标准测试基准，现有防御方法虽然众多，但缺乏形式化建模和最优策略定义，需要更高效的训练方法来提升防御效果。

Method: 使用部分可观测马尔可夫决策过程(POMDP)对CAGE-2进行形式化建模，提出BF-PPO方法：基于PPO算法，利用粒子滤波处理大规模状态空间的计算复杂度问题。

Result: 在CAGE-2 CybORG环境中评估，BF-PPO方法在学习的防御策略质量和所需训练时间两方面均优于当前排行榜第一的CARDIFF方法。

Conclusion: 通过POMDP形式化建模和BF-PPO方法，成功实现了对CAGE-2防御策略的优化，证明了该方法在网络安全防御领域的有效性和效率优势。

Abstract: CAGE-2 is an accepted benchmark for learning and evaluating defender
strategies against cyberattacks. It reflects a scenario where a defender agent
protects an IT infrastructure against various attacks. Many defender methods
for CAGE-2 have been proposed in the literature. In this paper, we construct a
formal model for CAGE-2 using the framework of Partially Observable Markov
Decision Process (POMDP). Based on this model, we define an optimal defender
strategy for CAGE-2 and introduce a method to efficiently learn this strategy.
Our method, called BF-PPO, is based on PPO, and it uses particle filter to
mitigate the computational complexity due to the large state space of the
CAGE-2 model. We evaluate our method in the CAGE-2 CybORG environment and
compare its performance with that of CARDIFF, the highest ranked method on the
CAGE-2 leaderboard. We find that our method outperforms CARDIFF regarding the
learned defender strategy and the required training time.

</details>


### [147] [Predicting Fetal Outcomes from Cardiotocography Signals Using a Supervised Variational Autoencoder](https://arxiv.org/abs/2509.06540)
*John Tolladay,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: 这篇论文开发了一种可解释的监督变分自编码器模型，用于根据孕期结果分类肺儿心率信号，并在保持竞争性预测性能的同时部分实现了临床意义特征的编码。


<details>
  <summary>Details</summary>
Motivation: 解决当前深度学习方法在肺儿心率信号分析中的可解释性限制，开发能够同时进行信号重建和结果预测的可解释模型。

Method: 使用OxMat CTG数据集训练监督VAE模型，对五分钟肺儿心率段进行分析，结合Kullback-Leibler散度和总相关约束来结构潜空间，并使用AUROC和MSE评估性能。

Result: 模型在段级别和CTG级别分别获得0.752和0.779的AUROC。改善总相关约束后重建和分类性能都提升。潜空分析显示基线相关特征得到良好表征，但短期和长期变异性特征编码较弱。

Conclusion: 监督VAE能够在保持竞争性预测性能的同时部分实现临床意义特征编码，为未来可解释生成模型奠定了基础。

Abstract: Objective: To develop and interpret a supervised variational autoencoder
(VAE) model for classifying cardiotocography (CTG) signals based on pregnancy
outcomes, addressing interpretability limits of current deep learning
approaches. Methods: The OxMat CTG dataset was used to train a VAE on
five-minute fetal heart rate (FHR) segments, labeled with postnatal outcomes.
The model was optimised for signal reconstruction and outcome prediction,
incorporating Kullback-Leibler divergence and total correlation (TC)
constraints to structure the latent space. Performance was evaluated using area
under the receiver operating characteristic curve (AUROC) and mean squared
error (MSE). Interpretability was assessed using coefficient of determination,
latent traversals and unsupervised component analyses. Results: The model
achieved an AUROC of 0.752 at the segment level and 0.779 at the CTG level,
where predicted scores were aggregated. Relaxing TC constraints improved both
reconstruction and classification. Latent analysis showed that baseline-related
features (e.g., FHR baseline, baseline shift) were well represented and aligned
with model scores, while metrics like short- and long-term variability were
less strongly encoded. Traversals revealed clear signal changes for baseline
features, while other properties were entangled or subtle. Unsupervised
decompositions corroborated these patterns. Findings: This work demonstrates
that supervised VAEs can achieve competitive fetal outcome prediction while
partially encoding clinically meaningful CTG features. The irregular,
multi-timescale nature of FHR signals poses challenges for disentangling
physiological components, distinguishing CTG from more periodic signals such as
ECG. Although full interpretability was not achieved, the model supports
clinically useful outcome prediction and provides a basis for future
interpretable, generative models.

</details>


### [148] [Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs](https://arxiv.org/abs/2509.06550)
*Jack Wilkie,Hanan Hindy,Christos Tachtatzis,Robert Atkinson*

Main category: cs.LG

TL;DR: 提出CLAN方法，通过将增强样本作为负样本对来改进网络入侵检测，在二进制和多分类任务中均优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有监督学习需要大量标注数据，异常检测方法误报率高，自监督学习方法通过数据增强生成正负样本对但效果有限

Method: CLAN方法将增强样本视为负样本对（代表恶意分布），其他良性样本作为正样本对，通过对比学习训练模型

Result: 在Lycos2017数据集上，二进制分类任务超越现有自监督和异常检测方法，有限标注数据下的多分类任务也表现更优

Conclusion: CLAN通过创新的负样本对构建方式，提高了网络入侵检测的准确性和推理效率，为实际应用提供了有效解决方案

Abstract: Network intrusion detection remains a critical challenge in cybersecurity.
While supervised machine learning models achieve state-of-the-art performance,
their reliance on large labelled datasets makes them impractical for many
real-world applications. Anomaly detection methods, which train exclusively on
benign traffic to identify malicious activity, suffer from high false positive
rates, limiting their usability. Recently, self-supervised learning techniques
have demonstrated improved performance with lower false positive rates by
learning discriminative latent representations of benign traffic. In
particular, contrastive self-supervised models achieve this by minimizing the
distance between similar (positive) views of benign traffic while maximizing it
between dissimilar (negative) views. Existing approaches generate positive
views through data augmentation and treat other samples as negative. In
contrast, this work introduces Contrastive Learning using Augmented Negative
pairs (CLAN), a novel paradigm for network intrusion detection where augmented
samples are treated as negative views - representing potentially malicious
distributions - while other benign samples serve as positive views. This
approach enhances both classification accuracy and inference efficiency after
pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset
demonstrates that the proposed method surpasses existing self-supervised and
anomaly detection techniques in a binary classification task. Furthermore, when
fine-tuned on a limited labelled dataset, the proposed approach achieves
superior multi-class classification performance compared to existing
self-supervised models.

</details>


### [149] [AI for Scientific Discovery is a Social Problem](https://arxiv.org/abs/2509.06580)
*Georgia Channing,Avijit Ghosh*

Main category: cs.LG

TL;DR: 本文认为AI科学发展的主要障碍是社会和制度性的，而非技术性的，需要将AI科学重新定义为集体社会项目，通过社区建设、跨学科教育和基础设施共享来解决核心挑战。


<details>
  <summary>Details</summary>
Motivation: 人工智能在科学发现中具有巨大潜力，但其效益分布不均。虽然技术障碍如数据稀缺、标准碎片化和计算资源不平等是重要因素，但作者认为主要障碍是社会和制度层面的。

Method: 通过分析当前AI科学领域的现状，识别出四个相互关联的挑战：社区功能障碍、研究重点与上游需求错位、数据碎片化和基础设施不平等，并探讨其文化组织根源。

Result: 提出了需要技术创新的同时，更需有意识的社区建设、跨学科教育、共享基准和可访问基础设施的综合解决方案框架。

Conclusion: 呼吁将AI科学重新定义为集体社会项目，将可持续合作和公平参与视为技术进步的前提条件，而不仅仅是技术工具的应用。

Abstract: Artificial intelligence promises to accelerate scientific discovery, yet its
benefits remain unevenly distributed. While technical obstacles such as scarce
data, fragmented standards, and unequal access to computation are significant,
we argue that the primary barriers are social and institutional. Narratives
that defer progress to speculative "AI scientists," the undervaluing of data
and infrastructure contributions, misaligned incentives, and gaps between
domain experts and machine learning researchers all constrain impact. We
highlight four interconnected challenges: community dysfunction, research
priorities misaligned with upstream needs, data fragmentation, and
infrastructure inequities. We argue that their roots lie in cultural and
organizational practices. Addressing them requires not only technical
innovation but also intentional community-building, cross-disciplinary
education, shared benchmarks, and accessible infrastructure. We call for
reframing AI for science as a collective social project, where sustainable
collaboration and equitable participation are treated as prerequisites for
technical progress.

</details>


### [150] [Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems](https://arxiv.org/abs/2509.06599)
*Sri Satish Krishna Chaitanya Bulusu,Mikko Sillanpää*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于结构化分解的理论框架，通过方差分析和任务复杂度界限来建模动态非线性系统的耦合静态和动态扬异。框架提出了行为不确定性原理，证明静态和动态扬异无法同时最小化，并引入了一种与任务相关的复杂度量约。


<details>
  <summary>Details</summary>
Motivation: 动态非线性系统中的静态和动态效应紧密耦合，这种交织特性给数据驱动建模带来了重大挑战。需要一种理论框架来理解和处理这种复杂的系统行为。

Method: 采用结构化分解、方差分析和任务复杂度界限的方法。框架引入了方向性下界来描述系统组件间的交互作用，并建立了与热力学第一定律相关联的有限记忆条件。

Result: 提出了"行为不确定性原理"，证明静态和动态扬异无法同时最小化。得到了一种与模型无关的、任务意识的复杂度量约，显示低方差组件更容易学习。这些见解解释了结构化残差学习在实验中的好处。

Conclusion: 该框架为建模复杂动态非线性系统提供了一种可扩展的、理论基础坚实的方法。通过理论分析揭示了实际系统中静态和动态效应的深层歧缘关系，为进一步的系统建模和控制提供了重要的理论指导。

Abstract: Dynamic nonlinear systems exhibit distortions arising from coupled static and
dynamic effects. Their intertwined nature poses major challenges for
data-driven modeling. This paper presents a theoretical framework grounded in
structured decomposition, variance analysis, and task-centric complexity
bounds.
  The framework employs a directional lower bound on interactions between
measurable system components, extending orthogonality in inner product spaces
to structurally asymmetric settings. This bound supports variance inequalities
for decomposed systems. Key behavioral indicators are introduced along with a
memory finiteness index. A rigorous power-based condition establishes a
measurable link between finite memory in realizable systems and the First Law
of Thermodynamics. This offers a more foundational perspective than classical
bounds based on the Second Law.
  Building on this foundation, we formulate a `Behavioral Uncertainty
Principle,' demonstrating that static and dynamic distortions cannot be
minimized simultaneously. We identify that real-world systems seem to resist
complete deterministic decomposition due to entangled static and dynamic
effects. We also present two general-purpose theorems linking function variance
to mean-squared Lipschitz continuity and learning complexity. This yields a
model-agnostic, task-aware complexity metric, showing that lower-variance
components are inherently easier to learn.
  These insights explain the empirical benefits of structured residual
learning, including improved generalization, reduced parameter count, and lower
training cost, as previously observed in power amplifier linearization
experiments. The framework is broadly applicable and offers a scalable,
theoretically grounded approach to modeling complex dynamic nonlinear systems.

</details>


### [151] [PAC-Bayesian Generalization Bounds for Graph Convolutional Networks on Inductive Node Classification](https://arxiv.org/abs/2509.06600)
*Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: 本文提出了图卷积网络在动态图环境中的PAC-Bayesian理论分析，推导了包含数据依赖性和非平稳性影响的新泛化边界，为理解GNN在动态图中的泛化性能提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图具有动态特性，新节点不断加入，现有连接随时间变化。之前的理论研究主要基于转导学习框架，无法充分建模这种时间演化和结构动态性。

Method: 使用PAC-Bayesian理论分析方法，将节点视为依赖且非同分布的数据点，对一层和两层图卷积网络进行理论分析，推导泛化边界。

Result: 推导出了显式包含数据依赖性和非平稳性影响的泛化边界，建立了在节点数量增加时泛化差距收敛到零的充分条件，发现两层GCN需要更强的图拓扑假设来保证收敛。

Conclusion: 这项工作为理解和改进动态图环境中图神经网络的泛化性能建立了理论基础，揭示了网络深度与图拓扑要求之间的关系。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in processing
graph-structured data across various applications. A critical aspect of
real-world graphs is their dynamic nature, where new nodes are continually
added and existing connections may change over time. Previous theoretical
studies, largely based on the transductive learning framework, fail to
adequately model such temporal evolution and structural dynamics. In this
paper, we presents a PAC-Bayesian theoretical analysis of graph convolutional
networks (GCNs) for inductive node classification, treating nodes as dependent
and non-identically distributed data points. We derive novel generalization
bounds for one-layer GCNs that explicitly incorporate the effects of data
dependency and non-stationarity, and establish sufficient conditions under
which the generalization gap converges to zero as the number of nodes
increases. Furthermore, we extend our analysis to two-layer GCNs, and reveal
that it requires stronger assumptions on graph topology to guarantee
convergence. This work establishes a theoretical foundation for understanding
and improving GNN generalization in dynamic graph environments.

</details>


### [152] [Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards](https://arxiv.org/abs/2509.06602)
*Noel Codella,Sam Preston,Hao Qiu,Leonardo Schettini,Wen-wai Yim,Mert Öz,Shrey Jain,Matthew P. Lungren,Thomas Osborne*

Main category: cs.LG

TL;DR: 使用LLM驱动的HAO系统自动生成治疗总结，TBFact框架评估摘要质量，提高分子诊疗会议效率


<details>
  <summary>Details</summary>
Motivation: 传统手工编写患者摘要效率低下、主观性强且容易遗漏关键信息，需要自动化解决方案

Method: 开发HAO多代理人工智能系统生成患者摘要，设计TBFact"模型作为判官"框架评估摘要的全面性和简洁性

Result: 系统捐损了重要信息的94%（包含部分含义），在严格含义标准下TBFact回归率达0.84，支持无需共享敏感数据的本地部署

Conclusion: HAO与TBFact为分子诊疗会议提供了可靠、可扩展的自动化支持基础

Abstract: Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology
specialists collaboratively assess complex patient cases to determine optimal
treatment strategies. A central element of this process is the patient summary,
typically compiled by a medical oncologist, radiation oncologist, or surgeon,
or their trained medical assistant, who distills heterogeneous medical records
into a concise narrative to facilitate discussion. This manual approach is
often labor-intensive, subjective, and prone to omissions of critical
information. To address these limitations, we introduce the Healthcare Agent
Orchestrator (HAO), a Large Language Model (LLM)-driven AI agent that
coordinates a multi-agent clinical workflow to generate accurate and
comprehensive patient summaries for MTBs. Evaluating predicted patient
summaries against ground truth presents additional challenges due to stylistic
variation, ordering, synonym usage, and phrasing differences, which complicate
the measurement of both succinctness and completeness. To overcome these
evaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework
designed to assess the comprehensiveness and succinctness of generated
summaries. Using a benchmark dataset derived from de-identified tumor board
discussions, we applied TBFact to evaluate our Patient History agent. Results
show that the agent captured 94% of high-importance information (including
partial entailments) and achieved a TBFact recall of 0.84 under strict
entailment criteria. We further demonstrate that TBFact enables a data-free
evaluation framework that institutions can deploy locally without sharing
sensitive clinical data. Together, HAO and TBFact establish a robust foundation
for delivering reliable and scalable support to MTBs.

</details>


### [153] [Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors](https://arxiv.org/abs/2509.06608)
*Viacheslav Sinii,Nikita Balagansky,Yaroslav Aksenov,Vadim Kurochkin,Daniil Laptev,Gleb Gerasimov,Alexey Gorbatovski,Boris Shaposhnikov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 论文通过分析轻量级引导向量在语言模型推理训练中的作用机制，发现最后一层引导向量主要影响首个生成token的替换偏差，而倒数第二层引导向量通过MLP和反嵌入层优先加权处理词和结构符号。


<details>
  <summary>Details</summary>
Motivation: 理解推理训练如何重塑语言模型的计算机制，目前仍缺乏深入研究。研究旨在通过可解释的小型加性干预方法来分析这些机制。

Method: 使用强化学习目标训练插入基础模型残差流中的轻量级引导向量，采用logit-lens读取、路径修补和电路分析等技术分析两个模型。

Result: 发现最后一层引导向量表现为集中在首个生成token上的token替换偏差，持续提升如"To"和"Step"等token；倒数第二层引导向量保持注意力模式基本不变，主要通过MLP和反嵌入层作用。

Conclusion: 这些结果为解释推理训练引起的行为变化建立了一个原则性框架，证明了轻量级干预方法在保持可解释性的同时能达到全微调性能。

Abstract: The mechanisms by which reasoning training reshapes language-model
computations remain poorly understood. We study lightweight steering vectors
inserted into the base model's residual stream and trained with a
reinforcement-learning objective, which can match full fine-tuning performance
while retaining the interpretability of small, additive interventions. Using
logit-lens readouts, path patching, and circuit analyses, we analyze two models
and find: (i) the last-layer steering vector behaves like a token-substitution
bias concentrated on the first generated token, consistently boosting tokens
such as "To" and "Step"; and (ii) the penultimate-layer steering vector leaves
attention patterns largely unchanged and instead acts through the MLP and
unembedding, preferentially up-weighting process words and structure symbols.
These results establish a principled framework for interpreting the behavioral
changes induced by reasoning training.

</details>


### [154] [A Survey of Generalization of Graph Anomaly Detection: From Transfer Learning to Foundation Models](https://arxiv.org/abs/2509.06609)
*Junjun Pan,Yu Zheng,Yue Tan,Yixin Liu*

Main category: cs.LG

TL;DR: 本文对图异常检测（GAD）中的泛化问题进行了系统性综述，分析了现有方法的局限性并提出了分类框架。


<details>
  <summary>Details</summary>
Motivation: 传统GAD方法假设训练和测试分布相同且针对特定任务设计，难以适应现实世界中数据分布变化和训练样本稀缺的新应用场景。

Method: 通过追溯GAD泛化研究的发展历程，形式化问题设置，建立系统性分类体系，并对现有广义GAD方法进行全面综述。

Result: 提出了一个细粒度的分类框架，对当前广义GAD方法进行了最新且全面的回顾。

Conclusion: 识别了当前开放挑战并提出了未来研究方向，以启发这一新兴领域的未来发展。

Abstract: Graph anomaly detection (GAD) has attracted increasing attention in recent
years for identifying malicious samples in a wide range of graph-based
applications, such as social media and e-commerce. However, most GAD methods
assume identical training and testing distributions and are tailored to
specific tasks, resulting in limited adaptability to real-world scenarios such
as shifting data distributions and scarce training samples in new applications.
To address the limitations, recent work has focused on improving the
generalization capability of GAD models through transfer learning that
leverages knowledge from related domains to enhance detection performance, or
developing "one-for-all" GAD foundation models that generalize across multiple
applications. Since a systematic understanding of generalization in GAD is
still lacking, in this paper, we provide a comprehensive review of
generalization in GAD. We first trace the evolution of generalization in GAD
and formalize the problem settings, which further leads to our systematic
taxonomy. Rooted in this fine-grained taxonomy, an up-to-date and comprehensive
review is conducted for the existing generalized GAD methods. Finally, we
identify current open challenges and suggest future directions to inspire
future research in this emerging field.

</details>


### [155] [BEAM: Brainwave Empathy Assessment Model for Early Childhood](https://arxiv.org/abs/2509.06620)
*Chen Xie,Gaofeng Wu,Kaidong Wang,Zihao Zhu,Xiaoshu Luo,Yan Liang,Feiyu Quan,Ruoxi Wu,Xianghui Huang,Han Zhang*

Main category: cs.LG

TL;DR: 提出了BEAM深度学习框架，利用多视角EEG信号预测4-6岁儿童共情水平，通过时空特征提取和对比学习，在CBCP数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统共情预测方法依赖主观报告和观察标签，存在偏见且无法客观捕捉共情形成过程；现有EEG方法主要提取静态模式，忽略了时间动态性。

Method: BEAM框架包含三个核心组件：1)LaBraM编码器进行时空特征提取；2)特征融合模块整合多视角信号；3)对比学习模块增强类别分离。

Result: 在CBCP数据集上验证，BEAM在多个指标上优于最先进方法，证明了其客观评估共情水平的潜力。

Conclusion: BEAM为儿童共情能力提供了客观评估工具，并为早期干预儿童亲社会发展提供了初步见解。

Abstract: Empathy in young children is crucial for their social and emotional
development, yet predicting it remains challenging. Traditional methods often
only rely on self-reports or observer-based labeling, which are susceptible to
bias and fail to objectively capture the process of empathy formation. EEG
offers an objective alternative; however, current approaches primarily extract
static patterns, neglecting temporal dynamics. To overcome these limitations,
we propose a novel deep learning framework, the Brainwave Empathy Assessment
Model (BEAM), to predict empathy levels in children aged 4-6 years. BEAM
leverages multi-view EEG signals to capture both cognitive and emotional
dimensions of empathy. The framework comprises three key components: 1) a
LaBraM-based encoder for effective spatio-temporal feature extraction, 2) a
feature fusion module to integrate complementary information from multi-view
signals, and 3) a contrastive learning module to enhance class separation.
Validated on the CBCP dataset, BEAM outperforms state-of-the-art methods across
multiple metrics, demonstrating its potential for objective empathy assessment
and providing a preliminary insight into early interventions in children's
prosocial development.

</details>


### [156] [Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing](https://arxiv.org/abs/2509.06640)
*Yung-Fu Chen,Sen Lin,Anish Arora*

Main category: cs.LG

TL;DR: 通过深度神经网络学习本地路由策略，仅需单个图的少量样本数据即可实现在欧几里得空间中多种几何随机图上的通用路由。新提出的GreedyTensile路由策略结合了目标距离和节点伸缩特征，性能超过传统贪心转发。


<details>
  <summary>Details</summary>
Motivation: 解决在多种几何随机图上实现高效、可扩展的全对路径路由问题，避免传统方法需要大量图数据进行训练的问题。

Method: 使用深度神经网络学习本地路由策略，利用网络领域知识选择输入特征和设计策略函数，仅需从单个"种子图"采样少量数据。

Result: 学习到的以距离为唯一特征的策略与传统贪心转发完全匹配；新提出的GreedyTensile路由策略（结合距离和节点伸缩）性能几乎总是超过贪心转发，且具有超低延迟和可解释性。

Conclusion: 通过结合领域知识的深度学习方法，可以用少量数据实现在广泛类型几何图上的高效通用路由，GreedyTensile路由策略为路由问题提供了更优的解决方案。

Abstract: We propose a simple algorithm that needs only a few data samples from a
single graph for learning local routing policies that generalize across a rich
class of geometric random graphs in Euclidean metric spaces. We thus solve the
all-pairs near-shortest path problem by training deep neural networks (DNNs)
that let each graph node efficiently and scalably route (i.e., forward) packets
by considering only the node's state and the state of the neighboring nodes.
Our algorithm design exploits network domain knowledge in the selection of
input features and design of the policy function for learning an approximately
optimal policy. Domain knowledge also provides theoretical assurance that the
choice of a ``seed graph'' and its node data sampling suffices for
generalizable learning. Remarkably, one of these DNNs we train -- using
distance-to-destination as the only input feature -- learns a policy that
exactly matches the well-known Greedy Forwarding policy, which forwards packets
to the neighbor with the shortest distance to the destination. We also learn a
new policy, which we call GreedyTensile routing -- using both
distance-to-destination and node stretch as the input features -- that almost
always outperforms greedy forwarding. We demonstrate the explainability and
ultra-low latency run-time operation of Greedy Tensile routing by symbolically
interpreting its DNN in low-complexity terms of two linear actions.

</details>


### [157] [Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives](https://arxiv.org/abs/2509.06656)
*Yuanyuan Wu,Zhenlin Qin,Leizhen Wang,Xiaolei Ma,Zhenliang Ma*

Main category: cs.LG

TL;DR: 提出了gcGAIL模型，通过利用乘客群体间的共享行为模式，提高了个体出行行为建模效率，在准确性、泛化性和模式展示效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 个体出行行为响应建模对城市交通监管和政策评估至关重要，但传统MDP方法数据需求量大，面临数据量、时空覆盖和情境多样性等挑战。

Method: 提出群体效应增强的生成对抗模仿学习（gcGAIL）模型，利用乘客群体间的共享行为模式来提高建模效率。

Result: 实验结果表明，gcGAIL在准确性、泛化性和模式展示效率方面优于AIRL、基线GAIL和条件GAIL等方法，对空间变化、数据稀疏性和行为多样性具有鲁棒性。

Conclusion: gcGAIL模型能够预测任何时间的个体行为响应，为个性化激励提供基础，以引导可持续行为改变（更好的激励时机选择）。

Abstract: Understanding and modeling individual travel behavior responses is crucial
for urban mobility regulation and policy evaluation. The Markov decision
process (MDP) provides a structured framework for dynamic travel behavior
modeling at the individual level. However, solving an MDP in this context is
highly data-intensive and faces challenges of data quantity, spatial-temporal
coverage, and situational diversity. To address these, we propose a
group-effect-enhanced generative adversarial imitation learning (gcGAIL) model
that improves the individual behavior modeling efficiency by leveraging shared
behavioral patterns among passenger groups. We validate the gcGAIL model using
a public transport fare-discount case study and compare against
state-of-the-art benchmarks, including adversarial inverse reinforcement
learning (AIRL), baseline GAIL, and conditional GAIL. Experimental results
demonstrate that gcGAIL outperforms these methods in learning individual travel
behavior responses to incentives over time in terms of accuracy,
generalization, and pattern demonstration efficiency. Notably, gcGAIL is robust
to spatial variation, data sparsity, and behavioral diversity, maintaining
strong performance even with partial expert demonstrations and underrepresented
passenger groups. The gcGAIL model predicts the individual behavior response at
any time, providing the basis for personalized incentives to induce sustainable
behavior changes (better timing of incentive injections).

</details>


### [158] [TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable VANETs under Partial Observations](https://arxiv.org/abs/2509.06665)
*Xiaolu Fu,Ziyuan Bao,Eiman Kanjo*

Main category: cs.LG

TL;DR: TrajAware是一个基于强化学习的VANET路由框架，通过动作空间剪枝、图交叉注意力和轨迹感知预测来解决动态拓扑和资源受限问题，在边缘设备上实现高效路由。


<details>
  <summary>Details</summary>
Motivation: VANETs在智能交通系统中至关重要，但现有RL方法假设固定图结构且需要重新训练，不适合资源受限的边缘设备部署。需要开发能够适应动态网络条件的高效路由方案。

Method: TrajAware包含三个核心组件：1)动作空间剪枝减少冗余邻居选项；2)图交叉注意力将修剪后的邻居映射到全局图上下文；3)轨迹感知预测利用历史路线和路口信息估计实时位置。

Result: 在SUMO模拟器中使用真实城市地图进行评估，TrajAware实现了接近最短路径和高投递率，在完整和部分观测场景下均优于现有基线方法，同时保持边缘设备适用效率。

Conclusion: TrajAware框架成功解决了VANETs中的动态路由挑战，为边缘AI部署提供了高效且通用的解决方案，在保持性能的同时适应不同的网络规模和观测条件。

Abstract: Vehicular ad hoc networks (VANETs) are a crucial component of intelligent
transportation systems; however, routing remains challenging due to dynamic
topologies, incomplete observations, and the limited resources of edge devices.
Existing reinforcement learning (RL) approaches often assume fixed graph
structures and require retraining when network conditions change, making them
unsuitable for deployment on constrained hardware. We present TrajAware, an
RL-based framework designed for edge AI deployment in VANETs. TrajAware
integrates three components: (i) action space pruning, which reduces redundant
neighbour options while preserving two-hop reachability, alleviating the curse
of dimensionality; (ii) graph cross-attention, which maps pruned neighbours to
the global graph context, producing features that generalise across diverse
network sizes; and (iii) trajectory-aware prediction, which uses historical
routes and junction information to estimate real-time positions under partial
observations. We evaluate TrajAware in the open-source SUMO simulator using
real-world city maps with a leave-one-city-out setup. Results show that
TrajAware achieves near-shortest paths and high delivery ratios while
maintaining efficiency suitable for constrained edge devices, outperforming
state-of-the-art baselines in both full and partial observation scenarios.

</details>


### [159] [Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation](https://arxiv.org/abs/2509.06694)
*Victor Toscano-Duran,Rocio Gonzalez-Diaz,Miguel A. Gutiérrez-Naranjo*

Main category: cs.LG

TL;DR: 提出了一种新的小型浅层神经网络Barycentric Neural Network (BNN)，利用基点和重心坐标来精确表示连续分段线性函数，并引入长度加权持久熵(LWPE)作为损失函数，在资源受限环境下实现快速高效的非线性函数逼近。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络计算成本高，需要开发小型高效的网络架构。BNN旨在通过固定基点和重心坐标实现连续分段线性函数的精确表示，为函数逼近提供灵活可解释的工具。

Method: 提出BNN网络结构，使用固定基点和重心坐标定义网络参数；引入长度加权持久熵(LWPE)作为稳定的拓扑特征；直接优化BNN的基点而非内部权重。

Result: 实验结果表明，该方法相比MSE、RMSE、MAE和log-cosh等传统损失函数，具有更优越和更快的逼近性能。

Conclusion: BNN结合LWPE损失函数为资源受限环境提供了灵活、几何可解释的非线性函数逼近框架，在有限基点和训练轮次下仍能实现高效性能。

Abstract: While it is well-established that artificial neural networks are
\emph{universal approximators} for continuous functions on compact domains,
many modern approaches rely on deep or overparameterized architectures that
incur high computational costs. In this paper, a new type of \emph{small
shallow} neural network, called the \emph{Barycentric Neural Network} ($\BNN$),
is proposed, which leverages a fixed set of \emph{base points} and their
\emph{barycentric coordinates} to define both its structure and its parameters.
We demonstrate that our $\BNN$ enables the exact representation of
\emph{continuous piecewise linear functions} ($\CPLF$s), ensuring strict
continuity across segments. Since any continuous function over a compact domain
can be approximated arbitrarily well by $\CPLF$s, the $\BNN$ naturally emerges
as a flexible and interpretable tool for \emph{function approximation}. Beyond
the use of this representation, the main contribution of the paper is the
introduction of a new variant of \emph{persistent entropy}, a topological
feature that is stable and scale invariant, called the \emph{length-weighted
persistent entropy} ($\LWPE$), which is weighted by the lifetime of topological
features. Our framework, which combines the $\BNN$ with a loss function based
on our $\LWPE$, aims to provide flexible and geometrically interpretable
approximations of nonlinear continuous functions in resource-constrained
settings, such as those with limited base points for $\BNN$ design and few
training epochs. Instead of optimizing internal weights, our approach directly
\emph{optimizes the base points that define the $\BNN$}. Experimental results
show that our approach achieves \emph{superior and faster approximation
performance} compared to classical loss functions such as MSE, RMSE, MAE, and
log-cosh.

</details>


### [160] [Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks](https://arxiv.org/abs/2509.06701)
*Su Hyeong Lee,Risi Kondor,Richard Ngo*

Main category: cs.LG

TL;DR: 基于概率模型的智能代理理论，通过对数加权池化组合定义代理组合，证明在三种或更多结果空间中存在严格一致性，并形式化了LLM中的代理对齐现象


<details>
  <summary>Details</summary>
Motivation: 为神经模型建立智能代理的理论基础，探索子代理如何聚集成协调的高级实体，以提供代理AI系统对齐的新见解

Method: 将代理表示为结果分布，以对数得分为认知效用，通过对数加权池化定义组合，并使用克隆不变性、连续性和开放性来建立递归结构

Result: 证明在线性池化或二元结果空间中不可能存在严格一致性，但在三种或更多结果中可能，形式化了LLM中的Waluigi效应，发现显现-压制策略比纯粹Luigi增强更能减少一阶错误对齐

Conclusion: 建立了一个理论框架来理解子代理如何聚集成协调的高级实体，这为代理AI系统的对齐问题提供了新的理论基础和实践意义

Abstract: We develop a theory of intelligent agency grounded in probabilistic modeling
for neural models. Agents are represented as outcome distributions with
epistemic utility given by log score, and compositions are defined through
weighted logarithmic pooling that strictly improves every member's welfare. We
prove that strict unanimity is impossible under linear pooling or in binary
outcome spaces, but possible with three or more outcomes. Our framework admits
recursive structure via cloning invariance, continuity, and openness, while
tilt-based analysis rules out trivial duplication. Finally, we formalize an
agentic alignment phenomenon in LLMs using our theory: eliciting a benevolent
persona ("Luigi'") induces an antagonistic counterpart ("Waluigi"), while a
manifest-then-suppress Waluigi strategy yields strictly larger first-order
misalignment reduction than pure Luigi reinforcement alone. These results
clarify how developing a principled mathematical framework for how subagents
can coalesce into coherent higher-level entities provides novel implications
for alignment in agentic AI systems.

</details>


### [161] [Nested Optimal Transport Distances](https://arxiv.org/abs/2509.06702)
*Ruben Bontorno,Songyan Hou*

Main category: cs.LG

TL;DR: 提出使用嵌套最优传输距离作为金融时间序列生成模型的评估指标，并开发了高效的并行计算算法


<details>
  <summary>Details</summary>
Motivation: 金融时间序列模拟对于压力测试和决策制定至关重要，但缺乏统一的生成模型评估标准

Method: 采用时间因果的最优传输距离变体（嵌套最优传输距离），并提出统计一致且可并行化的计算算法

Result: 相比现有方法实现了显著的速度提升，该指标对套期保值、最优停止和强化学习等任务具有鲁棒性

Conclusion: 嵌套最优传输距离是评估金融时间序列生成模型的有效指标，新算法大幅提高了计算效率

Abstract: Simulating realistic financial time series is essential for stress testing,
scenario generation, and decision-making under uncertainty. Despite advances in
deep generative models, there is no consensus metric for their evaluation. We
focus on generative AI for financial time series in decision-making
applications and employ the nested optimal transport distance, a time-causal
variant of optimal transport distance, which is robust to tasks such as
hedging, optimal stopping, and reinforcement learning. Moreover, we propose a
statistically consistent, naturally parallelizable algorithm for its
computation, achieving substantial speedups over existing approaches.

</details>


### [162] [RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly on Robotic Platforms](https://arxiv.org/abs/2509.06714)
*Zakariae El Asri,Ibrahim Laiche,Clément Rambour,Olivier Sigaud,Nicolas Thome*

Main category: cs.LG

TL;DR: 通过建立动作序列框架解决模型基于动力学RL推理延迟问题，提出RT-HCP算法在样本效率、性能和推理时间之间取得优称平衡


<details>
  <summary>Details</summary>
Motivation: 直接在机器人上学习控制器需要极高的样本效率，模型基于RL方法样本效率最高但推理时间过长，无法满足高频控制要求

Method: 定义了处理推理延迟的通用框架，让慢速推理器一次性提供动作序列以满足高频控制需求，并在此框架下比较多种RL算法后提出RT-HCP算法

Result: 在高频FURUTA摇摆平台上验证了RT-HCP的优越性，实现了样本效率、控制性能和推理速度的良好平衡

Conclusion: RT-HCP算法有效解决了模型基于RL在高频机器人控制中的样本效率和推理延迟问题，为直接机器人学习提供了高效解决方案

Abstract: Learning a controller directly on the robot requires extreme sample
efficiency. Model-based reinforcement learning (RL) methods are the most sample
efficient, but they often suffer from a too long inference time to meet the
robot control frequency requirements. In this paper, we address the sample
efficiency and inference time challenges with two contributions. First, we
define a general framework to deal with inference delays where the slow
inference robot controller provides a sequence of actions to feed the
control-hungry robotic platform without execution gaps. Then, we compare
several RL algorithms in the light of this framework and propose RT-HCP, an
algorithm that offers an excellent trade-off between performance, sample
efficiency and inference time. We validate the superiority of RT-HCP with
experiments where we learn a controller directly on a simple but high frequency
FURUTA pendulum platform. Code: github.com/elasriz/RTHCP

</details>


### [163] [Long-Range Graph Wavelet Networks](https://arxiv.org/abs/2509.06743)
*Filippo Guerranti,Fabrizio Forte,Simon Geisler,Stephan Günnemann*

Main category: cs.LG

TL;DR: LR-GWN通过将图小波滤波器分解为局部和全局组件，使用低阶多项式处理局部聚合，通过谱域参数化捕获长程交互，在长程基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决图机器学习中长程交互建模的挑战，现有基于小波的图神经网络依赖有限阶多项式近似，限制了感受野并阻碍长程传播。

Method: 将小波滤波器分解为互补的局部和全局组件：局部聚合使用高效低阶多项式处理，长程交互通过灵活的谱域参数化捕获。

Result: 在长程基准测试中达到基于小波方法的最先进性能，同时在短程数据集上保持竞争力。

Conclusion: LR-GWN在统一的小波框架内整合了短距离和长距离信息流，有效解决了图长程交互建模问题。

Abstract: Modeling long-range interactions, the propagation of information across
distant parts of a graph, is a central challenge in graph machine learning.
Graph wavelets, inspired by multi-resolution signal processing, provide a
principled way to capture both local and global structures. However, existing
wavelet-based graph neural networks rely on finite-order polynomial
approximations, which limit their receptive fields and hinder long-range
propagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), which
decompose wavelet filters into complementary local and global components. Local
aggregation is handled with efficient low-order polynomials, while long-range
interactions are captured through a flexible spectral domain parameterization.
This hybrid design unifies short- and long-distance information flow within a
principled wavelet framework. Experiments show that LR-GWN achieves
state-of-the-art performance among wavelet-based methods on long-range
benchmarks, while remaining competitive on short-range datasets.

</details>


### [164] [Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization](https://arxiv.org/abs/2509.06759)
*Thanh Thi Nguyen,Campbell Wilson,Janis Dalins*

Main category: cs.LG

TL;DR: 本文综述了使用深度强化学习(DRL)和直接偏好优化(DPO)技术来微调大型视觉语言模型(LVLMs)，以使其与人类价值观对齐并提升任务性能的方法。


<details>
  <summary>Details</summary>
Motivation: 虽然大规模预训练推动了多模态大语言模型的显著进展，但如何对这些模型进行微调以使其与人类价值观对齐或适应特定任务仍然是一个关键挑战。

Method: 探索了DRL和DPO两种范式：DRL通过奖励信号优化模型行为，而DPO直接根据偏好数据对齐策略，无需显式奖励模型。文章对关键方法进行了分类，并分析了偏好数据来源和奖励信号。

Result: 提供了对DRL和DPO在LVLMs对齐中应用的全面概述，展示了这些技术如何改善模型的任务性能和自适应多模态交互能力。

Conclusion: DRL和DPO技术为构建强大且与人类对齐的大型视觉语言模型提供了重要框架，但仍面临可扩展性、样本效率、持续学习、泛化能力和安全性等开放挑战。

Abstract: Large Vision-Language Models (LVLMs) or multimodal large language models
represent a significant advancement in artificial intelligence, enabling
systems to understand and generate content across both visual and textual
modalities. While large-scale pretraining has driven substantial progress,
fine-tuning these models for aligning with human values or engaging in specific
tasks or behaviors remains a critical challenge. Deep Reinforcement Learning
(DRL) and Direct Preference Optimization (DPO) offer promising frameworks for
this aligning process. While DRL enables models to optimize actions using
reward signals instead of relying solely on supervised preference data, DPO
directly aligns the policy with preferences, eliminating the need for an
explicit reward model. This overview explores paradigms for fine-tuning LVLMs,
highlighting how DRL and DPO techniques can be used to align models with human
preferences and values, improve task performance, and enable adaptive
multimodal interaction. We categorize key approaches, examine sources of
preference data, reward signals, and discuss open challenges such as
scalability, sample efficiency, continual learning, generalization, and safety.
The goal is to provide a clear understanding of how DRL and DPO contribute to
the evolution of robust and human-aligned LVLMs.

</details>


### [165] [Asynchronous Message Passing for Addressing Oversquashing in Graph Neural Networks](https://arxiv.org/abs/2509.06777)
*Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 提出异步消息传递框架解决GNN中的过度压缩问题，通过基于节点中心性的分批更新来提升长距离交互性能


<details>
  <summary>Details</summary>
Motivation: GNN在长距离交互任务中存在过度压缩问题，传统图重连方法会破坏归纳偏置并导致信息损失，增加通道容量又会提高参数复杂度

Method: 提出模型无关的异步更新框架，在每层基于节点中心性创建批次，仅更新批次内节点特征，实现顺序信息处理避免同步压缩

Result: 在6个标准图数据集和2个长距离数据集上测试，在REDDIT-BINARY和Peptides-struct上分别获得5%和4%的性能提升

Conclusion: 异步消息传递框架能有效缓解过度压缩问题，保持更高的特征敏感度边界，在长距离任务中表现优异

Abstract: Graph Neural Networks (GNNs) suffer from Oversquashing, which occurs when
tasks require long-range interactions. The problem arises from the presence of
bottlenecks that limit the propagation of messages among distant nodes.
Recently, graph rewiring methods modify edge connectivity and are expected to
perform well on long-range tasks. Yet, graph rewiring compromises the inductive
bias, incurring significant information loss in solving the downstream task.
Furthermore, increasing channel capacity may overcome information bottlenecks
but enhance the parameter complexity of the model. To alleviate these
shortcomings, we propose an efficient model-agnostic framework that
asynchronously updates node features, unlike traditional synchronous message
passing GNNs. Our framework creates node batches in every layer based on the
node centrality values. The features of the nodes belonging to these batches
will only get updated. Asynchronous message updates process information
sequentially across layers, avoiding simultaneous compression into
fixed-capacity channels. We also theoretically establish that our proposed
framework maintains higher feature sensitivity bounds compared to standard
synchronous approaches. Our framework is applied to six standard graph datasets
and two long-range datasets to perform graph classification and achieves
impressive performances with a $5\%$ and $4\%$ improvements on REDDIT-BINARY
and Peptides-struct, respectively.

</details>


### [166] [Physics-informed Value Learner for Offline Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2509.06782)
*Vittorio Giammarino,Ruiqi Ni,Ahmed H. Qureshi*

Main category: cs.LG

TL;DR: 提出基于物理信息的正则化损失函数，通过Eikonal偏微分方程在值函数学习中引入几何归纳偏置，提升离线目标条件强化学习的性能和泛化能力


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习在自主导航和运动控制等应用中具有重要价值，但面临状态-动作空间覆盖有限和长时程任务泛化的挑战

Method: 基于Eikonal偏微分方程推导物理信息正则化损失，与时间差分值学习兼容，可集成到现有离线GCRL算法中，提出了Pi-HIQL方法

Result: 在性能和泛化方面取得显著提升，特别是在stitching机制和大规模导航任务中表现突出

Conclusion: 物理信息正则化为离线GCRL提供了有效的几何归纳偏置，能够更好地对齐代价-收益结构，提升算法性能

Abstract: Offline Goal-Conditioned Reinforcement Learning (GCRL) holds great promise
for domains such as autonomous navigation and locomotion, where collecting
interactive data is costly and unsafe. However, it remains challenging in
practice due to the need to learn from datasets with limited coverage of the
state-action space and to generalize across long-horizon tasks. To improve on
these challenges, we propose a Physics-informed (Pi) regularized loss for value
learning, derived from the Eikonal Partial Differential Equation (PDE) and
which induces a geometric inductive bias in the learned value function. Unlike
generic gradient penalties that are primarily used to stabilize training, our
formulation is grounded in continuous-time optimal control and encourages value
functions to align with cost-to-go structures. The proposed regularizer is
broadly compatible with temporal-difference-based value learning and can be
integrated into existing Offline GCRL algorithms. When combined with
Hierarchical Implicit Q-Learning (HIQL), the resulting method, Physics-informed
HIQL (Pi-HIQL), yields significant improvements in both performance and
generalization, with pronounced gains in stitching regimes and large-scale
navigation tasks.

</details>


### [167] [\texttt{R$^\textbf{2}$AI}: Towards Resistant and Resilient AI in an Evolving World](https://arxiv.org/abs/2509.06786)
*Youbang Sun,Xiang Wang,Jie Fu,Chaochao Lu,Bowen Zhou*

Main category: cs.LG

TL;DR: 提出safe-by-coevolution新范式，通过R²AI框架实现AI安全与能力的协同进化，结合抗性和韧性机制应对已知和未知风险


<details>
  <summary>Details</summary>
Motivation: 解决AI能力快速增长与安全进展滞后之间的差距，现有安全范式存在局限性：事后对齐方法脆弱被动，内在安全方法难以应对开放环境中的未知风险

Method: 提出R²AI框架，整合快速和慢速安全模型、通过安全风洞进行对抗模拟和验证、建立持续反馈循环，使安全与能力协同进化

Result: 提供了一个可扩展的主动安全路径，能够应对动态环境中的安全挑战，处理从近期漏洞到长期存在性风险的各种安全问题

Conclusion: safe-by-coevolution和R²AI框架为AI安全提供了新的解决方案，特别适用于AI向AGI和ASI发展的过程中维持持续安全

Abstract: In this position paper, we address the persistent gap between rapidly growing
AI capabilities and lagging safety progress. Existing paradigms divide into
``Make AI Safe'', which applies post-hoc alignment and guardrails but remains
brittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety
but struggles to address unforeseen risks in open-ended environments. We
therefore propose \textit{safe-by-coevolution} as a new formulation of the
``Make Safe AI'' paradigm, inspired by biological immunity, in which safety
becomes a dynamic, adversarial, and ongoing learning process. To operationalize
this vision, we introduce \texttt{R$^2$AI} -- \textit{Resistant and Resilient
AI} -- as a practical framework that unites resistance against known threats
with resilience to unforeseen risks. \texttt{R$^2$AI} integrates \textit{fast
and slow safe models}, adversarial simulation and verification through a
\textit{safety wind tunnel}, and continual feedback loops that guide safety and
capability to coevolve. We argue that this framework offers a scalable and
proactive path to maintain continual safety in dynamic environments, addressing
both near-term vulnerabilities and long-term existential risks as AI advances
toward AGI and ASI.

</details>


### [168] [floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL](https://arxiv.org/abs/2509.06863)
*Bhavya Agrawalla,Michal Nauman,Khush Agarwal,Aviral Kumar*

Main category: cs.LG

TL;DR: FLOQ方法将强化学习中的Q函数参数化为速度场，使用流匹配技术进行训练，通过数值积分步骤实现迭代计算，相比传统方法性能提升1.8倍，容量扩展性更好


<details>
  <summary>Details</summary>
Motivation: 受现代大规模机器学习技术使用密集监督训练中间计算的启发，研究迭代计算在强化学习TD方法中的优势，传统方法以整体方式表示价值函数而没有迭代计算

Method: 引入floq方法，使用速度场参数化Q函数，采用流匹配技术训练，通过目标速度场的多步数值积分进行TD学习目标训练

Result: 在具有挑战性的离线RL基准测试和在线微调任务中，floq性能提升近1.8倍，容量扩展性显著优于标准TD学习架构

Conclusion: floq展示了迭代计算在价值学习中的潜力，通过适当设置积分步骤数可以实现比整体架构更精细的控制和Q函数容量扩展

Abstract: A hallmark of modern large-scale machine learning techniques is the use of
training objectives that provide dense supervision to intermediate
computations, such as teacher forcing the next token in language models or
denoising step-by-step in diffusion models. This enables models to learn
complex functions in a generalizable manner. Motivated by this observation, we
investigate the benefits of iterative computation for temporal difference (TD)
methods in reinforcement learning (RL). Typically they represent value
functions in a monolithic fashion, without iterative compute. We introduce floq
(flow-matching Q-functions), an approach that parameterizes the Q-function
using a velocity field and trains it using techniques from flow-matching,
typically used in generative modeling. This velocity field underneath the flow
is trained using a TD-learning objective, which bootstraps from values produced
by a target velocity field, computed by running multiple steps of numerical
integration. Crucially, floq allows for more fine-grained control and scaling
of the Q-function capacity than monolithic architectures, by appropriately
setting the number of integration steps. Across a suite of challenging offline
RL benchmarks and online fine-tuning tasks, floq improves performance by nearly
1.8x. floq scales capacity far better than standard TD-learning architectures,
highlighting the potential of iterative computation for value learning.

</details>


### [169] [Concolic Testing on Individual Fairness of Neural Network Models](https://arxiv.org/abs/2509.06864)
*Ming-I Huang,Chih-Duo Hong,Fang Yu*

Main category: cs.LG

TL;DR: PyFair是一个用于评估和验证深度神经网络个体公平性的形式化框架，通过改进PyCT工具生成公平性路径约束，采用双网络架构提供完整性保证，在25个基准模型上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决深度神经网络中存在的算法公平性问题，需要一种系统化、严谨的方法来测试和验证预训练DNN的公平性，特别是在关键领域中确保模型不会产生歧视性结果。

Method: 通过改进concolic测试工具PyCT，生成公平性特定的路径约束来系统探索DNN行为；采用双网络架构实现全面的公平性评估，并为某些网络类型提供完整性保证。

Result: 在25个基准模型（包括经过现有偏置缓解技术增强的模型）上评估显示，PyFair能有效检测歧视性实例并验证公平性，但也揭示了复杂模型的可扩展性挑战。

Conclusion: PyFair通过提供严谨、系统化的公平性测试和验证方法，推动了关键领域中算法公平性的发展，为预训练DNN的公平性评估提供了有效工具。

Abstract: This paper introduces PyFair, a formal framework for evaluating and verifying
individual fairness of Deep Neural Networks (DNNs). By adapting the concolic
testing tool PyCT, we generate fairness-specific path constraints to
systematically explore DNN behaviors. Our key innovation is a dual network
architecture that enables comprehensive fairness assessments and provides
completeness guarantees for certain network types. We evaluate PyFair on 25
benchmark models, including those enhanced by existing bias mitigation
techniques. Results demonstrate PyFair's efficacy in detecting discriminatory
instances and verifying fairness, while also revealing scalability challenges
for complex models. This work advances algorithmic fairness in critical domains
by offering a rigorous, systematic method for fairness testing and verification
of pre-trained DNNs.

</details>


### [170] [AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced Classification](https://arxiv.org/abs/2509.06875)
*Sukumar Kishanthan,Asela Hevapathige*

Main category: cs.LG

TL;DR: AxelSMOTE是一种基于智能体交互的新型过采样方法，通过文化传播模型解决传统过采样技术的局限性，在八个不平衡数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统过采样技术存在特征独立处理、缺乏相似性控制、样本多样性受限和合成样本管理不足等问题，需要更有效的解决方案来处理类别不平衡问题。

Method: 基于Axelrod文化传播模型，采用四个关键创新：(1)基于特征的分组保持相关性；(2)基于相似性的概率交换机制；(3)Beta分布混合实现真实插值；(4)受控多样性注入避免过拟合。

Result: 在八个不平衡数据集上的实验表明，AxelSMOTE在性能上优于最先进的采样方法，同时保持了计算效率。

Conclusion: AxelSMOTE通过智能体交互框架有效解决了传统过采样方法的局限性，为类别不平衡问题提供了更优的解决方案。

Abstract: Class imbalance in machine learning poses a significant challenge, as skewed
datasets often hinder performance on minority classes. Traditional oversampling
techniques, which are commonly used to alleviate class imbalance, have several
drawbacks: they treat features independently, lack similarity-based controls,
limit sample diversity, and fail to manage synthetic variety effectively. To
overcome these issues, we introduce AxelSMOTE, an innovative agent-based
approach that views data instances as autonomous agents engaging in complex
interactions. Based on Axelrod's cultural dissemination model, AxelSMOTE
implements four key innovations: (1) trait-based feature grouping to preserve
correlations; (2) a similarity-based probabilistic exchange mechanism for
meaningful interactions; (3) Beta distribution blending for realistic
interpolation; and (4) controlled diversity injection to avoid overfitting.
Experiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms
state-of-the-art sampling methods while maintaining computational efficiency.

</details>


### [171] [Not All Samples Are Equal: Quantifying Instance-level Difficulty in Targeted Data Poisoning](https://arxiv.org/abs/2509.06896)
*William Xu,Yiwei Lu,Yihan Wang,Matthew Y. R. Yang,Zuoqiu Liu,Gautam Kamath,Yaoliang Yu*

Main category: cs.LG

TL;DR: 本文研究了目标数据投毒攻击中不同测试样本的易受攻击性差异，提出了三个预测标准来评估攻击难度，并通过实验验证了这些指标的有效性。


<details>
  <summary>Details</summary>
Motivation: 目标数据投毒攻击因其部署简单且成功率高而构成严重威胁，但现有研究缺乏对不同测试样本易受攻击性差异的理解，需要识别影响漏洞的关键特征。

Method: 提出了三个预测目标数据投毒难度的标准：遍历预测准确性（通过清洁训练动态分析）、毒物距离和毒物预算，并在多样化场景中进行实验验证。

Result: 实验结果表明，这些指标能够有效预测现实世界中目标投毒攻击的难度变化，为漏洞评估提供了实用工具。

Conclusion: 研究揭示了目标数据投毒攻击的难度变化规律，提出的预测标准为从业者提供了评估漏洞和理解数据投毒攻击的重要见解。

Abstract: Targeted data poisoning attacks pose an increasingly serious threat due to
their ease of deployment and high success rates. These attacks aim to
manipulate the prediction for a single test sample in classification models.
Unlike indiscriminate attacks that aim to decrease overall test performance,
targeted attacks present a unique threat to individual test instances. This
threat model raises a fundamental question: what factors make certain test
samples more susceptible to successful poisoning than others? We investigate
how attack difficulty varies across different test instances and identify key
characteristics that influence vulnerability. This paper introduces three
predictive criteria for targeted data poisoning difficulty: ergodic prediction
accuracy (analyzed through clean training dynamics), poison distance, and
poison budget. Our experimental results demonstrate that these metrics
effectively predict the varying difficulty of real-world targeted poisoning
attacks across diverse scenarios, offering practitioners valuable insights for
vulnerability assessment and understanding data poisoning attacks.

</details>


### [172] [Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition](https://arxiv.org/abs/2509.06918)
*Tarhib Al Azad,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 这篇论文研究在噪声标签条件下的健壁分布外检测问题，提出了一种结合损失缩正和低秩稀疏分解的新方法，在严重标签噪声环境下显著提升了OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在安全关键应用中需要健壁的分布外检测能力，但标签噪声会严重影响OOD检测性能，而现有方法无法有效解决这一问题。

Method: 提出了一种健壁OOD检测框架，结合了噪声标签学习中的损失缩正技术和信号处理中的低秩稀疏分解方法。

Result: 在合成和真实数据集上进行了大量实验，结果显示该方法显著超过了现有最先进的OOD检测技术，尤其在严重标签噪声环境下表现优异。

Conclusion: 该研究成功地解决了标签噪声对OOD检测的负面影响，提供了一种有效的健壁检测方案，对于安全关键应用中的AI系统具有重要意义。

Abstract: Robust out-of-distribution (OOD) detection is an indispensable component of
modern artificial intelligence (AI) systems, especially in safety-critical
applications where models must identify inputs from unfamiliar classes not seen
during training. While OOD detection has been extensively studied in the
machine learning literature--with both post hoc and training-based
approaches--its effectiveness under noisy training labels remains
underexplored. Recent studies suggest that label noise can significantly
degrade OOD performance, yet principled solutions to this issue are lacking. In
this work, we demonstrate that directly combining existing label noise-robust
methods with OOD detection strategies is insufficient to address this critical
challenge. To overcome this, we propose a robust OOD detection framework that
integrates loss correction techniques from the noisy label learning literature
with low-rank and sparse decomposition methods from signal processing.
Extensive experiments on both synthetic and real-world datasets demonstrate
that our method significantly outperforms the state-of-the-art OOD detection
techniques, particularly under severe noisy label settings.

</details>


### [173] [Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding](https://arxiv.org/abs/2509.06923)
*Ziheng Li,Zexu Sun,Jinman Zhao,Erxue Min,Yongcheng Zeng,Hui Wu,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: SEELE是一个新颖的监督辅助强化学习框架，通过动态调整问题难度来提升大型语言模型的推理能力探索效率。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法存在探索效率低下的问题，当问题过于困难时模型无法找到可行的推理路径，而问题过于简单时学习效果有限。

Method: SEELE通过为每个训练样本添加可调节长度的提示（部分解决方案），使用多轮抽样策略和项目反应理论模型来动态确定最优提示长度，使问题难度与模型能力相匹配。

Result: 在六个数学推理基准测试中，SEELE比GRPO和SFT分别高出11.8和10.5个百分点，比之前最好的监督辅助方法平均高出3.6个百分点。

Conclusion: SEELE通过实例级实时难度调整有效解决了RLVR中的探索效率问题，显著提升了模型推理性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable
success in enhancing the reasoning capabilities of large language models
(LLMs). However, existing RLVR methods often suffer from exploration
inefficiency due to mismatches between the training data's difficulty and the
model's capability. LLMs fail to discover viable reasoning paths when problems
are overly difficult, while learning little new capability when problems are
too simple. In this work, we formalize the impact of problem difficulty by
quantifying the relationship between loss descent speed and rollout accuracy.
Building on this analysis, we propose SEELE, a novel supervision-aided RLVR
framework that dynamically adjusts problem difficulty to stay within the
high-efficiency region. SEELE augments each training sample by appending a hint
(part of a full solution) after the original problem. Unlike previous
hint-based approaches, SEELE deliberately and adaptively adjusts the hint
length for each problem to achieve an optimal difficulty. To determine the
optimal hint length, SEELE employs a multi-round rollout sampling strategy. In
each round, it fits an item response theory model to the accuracy-hint pairs
collected in preceding rounds to predict the required hint length for the next
round. This instance-level, real-time difficulty adjustment aligns problem
difficulty with the evolving model capability, thereby improving exploration
efficiency. Experimental results show that SEELE outperforms Group Relative
Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5
points, respectively, and surpasses the best previous supervision-aided
approach by +3.6 points on average across six math reasoning benchmarks.

</details>


### [174] [Neutron Reflectometry by Gradient Descent](https://arxiv.org/abs/2509.06924)
*Max D. ~Champneys,Andrew J. ~Parnell,Philipp Gutfreund,Maximilian W. A. Skoda,. Patrick A. Fairclough,Timothy J. ~Rogers,Stephanie L. ~Burg*

Main category: cs.LG

TL;DR: 通过自动微分技术实现中子反射法的可微分前向模型优化，提高反射数据分析效率和准确性


<details>
  <summary>Details</summary>
Motivation: 中子反射法(NR)作为间接测量技术，需要解决复杂的反向模型问题，传统优化方法效率低且失去物理直觉

Method: 使用自动微分技术计算错误函数对关键参数的准确梯度，并应用现代优化算法进行反射数据分析

Result: 在氏石氧化膜和有机LED多层设备上展现出领先的性能，并提供了开源可微分反射核心库

Conclusion: 该方法为中子反射法提供了高效、准确且保持物理直觉的数据分析方案，有望推广到其他NR数据集

Abstract: Neutron reflectometry (NR) is a powerful technique to probe surfaces and
interfaces. NR is inherently an indirect measurement technique, access to the
physical quantities of interest (layer thickness, scattering length density,
roughness), necessitate the solution of an inverse modelling problem, that is
inefficient for large amounts of data or complex multiplayer structures (e.g.
lithium batteries / electrodes). Recently, surrogate machine learning models
have been proposed as an alternative to existing optimisation routines.
Although such approaches have been successful, physical intuition is lost when
replacing governing equations with fast neural networks. Instead, we propose a
novel and efficient approach; to optimise reflectivity data analysis by
performing gradient descent on the forward reflection model itself. Herein,
automatic differentiation techniques are used to evaluate exact gradients of
the error function with respect to the parameters of interest. Access to these
quantities enables users of neutron reflectometry to harness a host of powerful
modern optimisation and inference techniques that remain thus far unexploited
in the context of neutron reflectometry. This paper presents two benchmark case
studies; demonstrating state-of-the-art performance on a thick oxide quartz
film, and robust co-fitting performance in the high complexity regime of
organic LED multilayer devices. Additionally, we provide an open-source library
of differentiable reflectometry kernels in the python programming language so
that gradient based approaches can readily be applied to other NR datasets.

</details>


### [175] [Learning words in groups: fusion algebras, tensor ranks and grokking](https://arxiv.org/abs/2509.06931)
*Maor Shutman,Oren Louidor,Ran Tessler*

Main category: cs.LG

TL;DR: 两层神经网络可以学习有限群中的任意单词操作，并在学习过程中表现出grokking现象。通过将问题重构为学习低秩3-张量，网络能够找到低秩实现方式，使用有限宽度来近似单词张量。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何学习有限群中的代数操作，特别是理解grokking现象背后的机制，以及网络如何利用低秩结构来实现高效计算。

Method: 将单词操作学习问题重构为3-张量学习问题，分析张量的低秩特性。通过分解到群的基本自共轭表示三重态，利用融合结构排除冗余组件。使用替代模型研究低秩实现的发现过程。

Result: 证明神经网络能够找到单词张量的低秩实现或近似，在有限宽度下实现泛化。对于简单乘法单词，网络实现了类似Strassen矩阵乘法的高效计算。揭示了梯度下降下网络达到此类解的机制。

Conclusion: 神经网络通过学习低秩张量分解来学习群操作，这种机制解释了grokking现象，并为理解神经网络如何发现高效计算算法提供了新的视角。

Abstract: In this work, we demonstrate that a simple two-layer neural network with
standard activation functions can learn an arbitrary word operation in any
finite group, provided sufficient width is available and exhibits grokking
while doing so. To explain the mechanism by which this is achieved, we reframe
the problem as that of learning a particular $3$-tensor, which we show is
typically of low rank. A key insight is that low-rank implementations of this
tensor can be obtained by decomposing it along triplets of basic self-conjugate
representations of the group and leveraging the fusion structure to rule out
many components. Focusing on a phenomenologically similar but more tractable
surrogate model, we show that the network is able to find such low-rank
implementations (or approximations thereof), thereby using limited width to
approximate the word-tensor in a generalizable way. In the case of the simple
multiplication word, we further elucidate the form of these low-rank
implementations, showing that the network effectively implements efficient
matrix multiplication in the sense of Strassen. Our work also sheds light on
the mechanism by which a network reaches such a solution under gradient
descent.

</details>


### [176] [From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers](https://arxiv.org/abs/2509.06938)
*Praneet Suresh,Jack Stanley,Sonia Joseph,Luca Scimeca,Danilo Bzdok*

Main category: cs.LG

TL;DR: 本文通过稀疏自编码器分析预训练transformer模型中的幻觉现象，发现在输入不确定性增加时，模型会激活更多语义概念，导致产生与输入无关的幻觉输出。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统在科学、商业和政府领域的普及，理解其失败模式（如幻觉现象）对于高风险领域的可信应用至关重要。

Method: 使用稀疏自编码器捕捉概念表示，在实验控制输入空间不确定性的场景下，系统分析transformer模型中幻觉的产生机制。

Result: 研究发现输入信息越无结构化，模型使用的语义概念数量越多；在纯噪声输入下，模型中间激活中仍能识别出大量有意义的概念；幻觉输出可以从transformer层激活中的概念模式可靠预测。

Conclusion: 这些发现对AI模型与人类价值观对齐、AI安全、对抗攻击防御以及自动量化模型幻觉风险具有重要意义。

Abstract: As generative AI systems become competent and democratized in science,
business, and government, deeper insight into their failure modes now poses an
acute need. The occasional volatility in their behavior, such as the propensity
of transformer models to hallucinate, impedes trust and adoption of emerging AI
solutions in high-stakes areas. In the present work, we establish how and when
hallucinations arise in pre-trained transformer models through concept
representations captured by sparse autoencoders, under scenarios with
experimentally controlled uncertainty in the input space. Our systematic
experiments reveal that the number of semantic concepts used by the transformer
model grows as the input information becomes increasingly unstructured. In the
face of growing uncertainty in the input space, the transformer model becomes
prone to activate coherent yet input-insensitive semantic features, leading to
hallucinated output. At its extreme, for pure-noise inputs, we identify a wide
variety of robustly triggered and meaningful concepts in the intermediate
activations of pre-trained transformer models, whose functional integrity we
confirm through targeted steering. We also show that hallucinations in the
output of a transformer model can be reliably predicted from the concept
patterns embedded in transformer layer activations. This collection of insights
on transformer internal processing mechanics has immediate consequences for
aligning AI models with human values, AI safety, opening the attack surface for
potential adversarial attacks, and providing a basis for automatic
quantification of a model's hallucination risk.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [177] [Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across Multiple Formats](https://arxiv.org/abs/2509.05303)
*Sam Davidson,Li Sun,Bhavana Bhasker,Laurent Callot,Anoop Deoras*

Main category: cs.DC

TL;DR: 这篇论文提出了一个名为Multi-IaC-Bench的标准化测试集，用于评估大语言模型在多种基础设施作为代码(IaC)格式上的生成和修改能力，解决云计算部署中的格式差异问题。


<details>
  <summary>Details</summary>
Motivation: 不同云服务提供商使用不同的IaC格式，缺乏标准化格式导致云架构师需要掌握多种语言，增加了部署复杂性。虽然大语言模型在自动化IaC创建方面有潜力，但缺乏跨多格式的综合性测试标准限制了进展。

Method: 构建Multi-IaC-Bench数据集，包含AWS CloudFormation、Terraform和CDK格式的三元组数据（初始模板、自然语言修改请求、更新后模板），通过合成数据生成流水线创建并经过严格验证。评估多个先进的大语言模型。

Result: 现代大语言模型在生成语法有效的IaC方面可以达到高成功率（>95%），但在语义对齐和处理复杂基础设施模式方面仍面临重大挑战。消融研究显示提示工程和重试机制对成功生成IaC至关重要。

Conclusion: 论文提供了一个标准化的测试集来促进AI辅助基础设施管理的研究，并为这个重要领域建立了标准化的评估指标。

Abstract: Infrastructure as Code (IaC) is fundamental to modern cloud computing,
enabling teams to define and manage infrastructure through machine-readable
configuration files. However, different cloud service providers utilize diverse
IaC formats. The lack of a standardized format requires cloud architects to be
proficient in multiple IaC languages, adding complexity to cloud deployment.
While Large Language Models (LLMs) show promise in automating IaC creation and
maintenance, progress has been limited by the lack of comprehensive benchmarks
across multiple IaC formats. We present Multi-IaC-Bench, a novel benchmark
dataset for evaluating LLM-based IaC generation and mutation across AWS
CloudFormation, Terraform, and Cloud Development Kit (CDK) formats. The dataset
consists of triplets containing initial IaC templates, natural language
modification requests, and corresponding updated templates, created through a
synthetic data generation pipeline with rigorous validation. We evaluate
several state-of-the-art LLMs on Multi-IaC-Bench, demonstrating that while
modern LLMs can achieve high success rates (>95%) in generating syntactically
valid IaC across formats, significant challenges remain in semantic alignment
and handling complex infrastructure patterns. Our ablation studies highlight
the importance of prompt engineering and retry mechanisms in successful IaC
generation. We release Multi-IaC-Bench to facilitate further research in
AI-assisted infrastructure management and establish standardized evaluation
metrics for this crucial domain.

</details>


### [178] [A Simple and Robust Protocol for Distributed Counting](https://arxiv.org/abs/2509.05870)
*Edith Cohen,Moshe Shechner,Uri Stemmer*

Main category: cs.DC

TL;DR: 重新研究分布式计数问题，证明Huang等人(2012)的随机协议在适应性攻击下不稳健，并提出了一种简单的稳健协议，达到最优通信复杂度


<details>
  <summary>Details</summary>
Motivation: 解决分布式计数问题中适应性攻击对协议稳健性的挑战，而之前的方案或需要偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏极复杂或不能达到最优通信复杂度

Method: 首先构造明确的适应性攻击来证明Huang等人协议的非稳健性，然后提出一种新的简单稳健协议，该协议能够在适应性设置下达到最优通信复杂度

Result: 证明了Huang等人的协议在适应性攻击下失效，并成功设计出一种简单的稳健协议，实现了O(√k/ε log N)的最优通信复杂度

Conclusion: 该研究解决了分布式计数问题中的重要问题，证明了简单协议的非稳健性，并提供了简单且最优的稳健解决方案，在适应性设置下首次达到了偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏称偏极复杂度

Abstract: We revisit the distributed counting problem, where a server must continuously
approximate the total number of events occurring across $k$ sites while
minimizing communication. The communication complexity of this problem is known
to be $\Theta(\frac{k}{\epsilon}\log N)$ for deterministic protocols. Huang,
Yi, and Zhang (2012) showed that randomization can reduce this to
$\Theta(\frac{\sqrt{k}}{\epsilon}\log N)$, but their analysis is restricted to
the {\em oblivious setting}, where the stream of events is independent of the
protocol's outputs.
  Xiong, Zhu, and Huang (2023) presented a robust protocol for distributed
counting that removes the oblivious assumption. However, their communication
complexity is suboptimal by a $polylog(k)$ factor and their protocol is
substantially more complex than the oblivious protocol of Huang et al. (2012).
This left open a natural question: could it be that the simple protocol of
Huang et al. (2012) is already robust?
  We resolve this question with two main contributions. First, we show that the
protocol of Huang et al. (2012) is itself not robust by constructing an
explicit adaptive attack that forces it to lose its accuracy. Second, we
present a new, surprisingly simple, robust protocol for distributed counting
that achieves the optimal communication complexity of
$O(\frac{\sqrt{k}}{\epsilon} \log N)$. Our protocol is simpler than that of
Xiong et al. (2023), perhaps even simpler than that of Huang et al. (2012), and
is the first to match the optimal oblivious complexity in the adaptive setting.

</details>


### [179] [DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers](https://arxiv.org/abs/2509.06046)
*Philip Adams,Menghao Li,Shi Zhang,Li Tan,Qi Chen,Mingqin Li,Zengzhong Li,Knut Risvik,Harsha Vardhan Simhadri*

Main category: cs.DC

TL;DR: DISTRIBUTEDANN是一个分布式向量搜索服务，能在1000多台机器上搜索500亿向量的图索引，提供26ms中位数查询延迟和10万QPS，比现有方案效率高6倍


<details>
  <summary>Details</summary>
Motivation: 解决大规模向量搜索系统的扩展性问题，传统分区和路由策略效率不足，需要更高效的分布式搜索架构

Method: 结合分布式键值存储和内存ANN索引两个成熟组件构建分布式向量搜索服务

Result: 实现了500亿向量规模的搜索，26ms中位数延迟，10万QPS，比现有方案效率提升6倍，已成功应用于Bing搜索引擎

Conclusion: DISTRIBUTEDANN证明了结合成熟组件构建分布式向量搜索的可行性，为大规模向量搜索提供了高效解决方案，并成功在生产环境中替代传统架构

Abstract: We present DISTRIBUTEDANN, a distributed vector search service that makes it
possible to search over a single 50 billion vector graph index spread across
over a thousand machines that offers 26ms median query latency and processes
over 100,000 queries per second. This is 6x more efficient than existing
partitioning and routing strategies that route the vector query to a subset of
partitions in a scale out vector search system. DISTRIBUTEDANN is built using
two well-understood components: a distributed key-value store and an in-memory
ANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for
serving the Bing search engine, and we share our experience from making this
transition.

</details>


### [180] [Gathering in Non-Vertex-Transitive Graphs Under Round Robin](https://arxiv.org/abs/2509.06064)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 在非顶点传递图中研究机器人聚集问题，考虑存在多重性且无法检测多重性的敌对环境，提出了一种适用于任意配置的解析算法并分析了时间复杂度


<details>
  <summary>Details</summary>
Motivation: 研究在非顶点传递图中，当初始配置可能存在多重性且机器人无法检测多重性时的聚集问题，扩展了传统聚集问题的研究范围

Method: 使用轮询调度器激活机器人，提出一种解析算法来处理非顶点传递图中的任意机器人配置，确保最终聚集到同一顶点

Result: 提供了算法的正确性证明，并分析了算法的时间复杂度，实现了在敌对环境下的有效聚集

Conclusion: 该研究完全表征了非顶点传递图中的聚集问题，提出的算法能够处理包含多重性的任意初始配置，为分布式机器人系统在复杂环境中的聚集提供了理论保障

Abstract: The Gathering problem for a swarm of robots asks for a distributed algorithm
that brings such entities to a common place, not known in advance. We consider
the well-known OBLOT model with robots constrained to move along the edges of a
graph, hence gathering in one vertex, eventually. Despite the classical setting
under which the problem has been usually approached, we consider the `hostile'
case where: i) the initial configuration may contain multiplicities, i.e. more
than one robot may occupy the same vertex; ii) robots cannot detect
multiplicities. As a scheduler for robot activation, we consider the
"favorable" round-robin case, where robots are activated one at a time.
  Our objective is to achieve a complete characterization of the problem in the
broad context of non-vertex-transitive graphs, i.e., graphs where the vertices
are partitioned into at least two different classes of equivalence. We provide
a resolution algorithm for any configuration of robots moving on such graphs,
along with its correctness. Furthermore, we analyze its time complexity.

</details>


### [181] [20 Years in Life of a Smart Building: A retrospective](https://arxiv.org/abs/2509.06229)
*Karolina Skrivankova,Mark Handley,Stephen Hailes*

Main category: cs.DC

TL;DR: KaOS是一个基于现成IoT硬件的分布式控制平台，旨在解决智能建筑自动化系统面临的硬件故障、供应商淘汰、安全威胁等挑战，提供灵活、安全、容错的解决方案。


<details>
  <summary>Details</summary>
Motivation: 智能建筑自动化系统在2025年面临硬件故障、供应商淘汰、安全威胁等多重挑战，现有工业建筑和家庭自动化行业未能全面解决这些问题，限制了大规模智能自动化部署的可行性。

Method: KaOS采用容器化和受管资源访问技术，构建分布式控制平台，支持控制应用和分布式系统操作，使用经济实惠的现成IoT硬件。

Result: 初步评估证实了该方法的实际可行性，突显了其在延长的时间范围内可持续维护和逐步演进建筑控制功能的潜力。

Conclusion: KaOS平台能够在不牺牲成本效益的前提下，实现智能建筑自动化系统的灵活性、安全性和容错性，为解决当前行业挑战提供了可行方案。

Abstract: Operating an intelligent smart building automation system in 2025 is met with
many challenges: hardware failures, vendor obsolescence, evolving security
threats and more. None of these have been comprehensibly addressed by the
industrial building nor home automation industries, limiting feasibility of
operating large, truly smart automation deployments. This paper introduces
KaOS, a distributed control platform for constructing robust and evolvable
smart building automation systems using affordable, off-the-shelf IoT hardware.
Supporting control applications and distributed system operations by leveraging
containerisation and managed resource access, KaOS seeks to achieve
flexibility, security, and fault tolerance without sacrificing
cost-effectiveness. Initial evaluation confirms the practical feasibility of
our approach, highlighting its potential to sustainably maintain and
incrementally evolve building control functionalities over extended timeframes.

</details>


### [182] [FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving](https://arxiv.org/abs/2509.06261)
*Kyungmin Bin,Seungbeom Choi,Jimyoung Son,Jieun Choi,Daseul Bae,Daehyeon Baek,Kihyo Moon,Minsung Jang,Hyojung Lee*

Main category: cs.DC

TL;DR: FineServe是一个用于混合精度大语言模型推理服务的框架，通过KV Slab内存管理和两级调度系统，解决了量化模型的内存碎片和调度效率问题，显著提升了吞吐量和SLO达成率。


<details>
  <summary>Details</summary>
Motivation: 量化大语言模型虽然能提高吞吐量和减少内存使用，但存在KV缓存块大小较小导致的内存碎片问题，以及量化与非量化模型资源使用模式不同带来的调度挑战。

Method: 提出了FineServe框架，包含：(1) KV Slab - 基于模型量化特性的自适应内存管理技术，动态分配KV缓存；(2) 两级调度框架 - 全局调度器根据请求率、延迟SLO和内存约束进行模型放置，本地调度器根据实时请求波动自适应调整批大小。

Result: 实验结果显示，FineServe相比最先进的GPU共享系统，实现了高达2.2倍的SLO达成率和1.8倍的token生成吞吐量提升。

Conclusion: FineServe通过创新的内存管理和调度机制，有效解决了混合精度LLM推理服务中的关键挑战，显著提升了系统性能和资源利用率。

Abstract: Recent advances in Post-Training Quantization (PTQ) techniques have
significantly increased demand for serving quantized large language models
(LLMs), enabling higher throughput and substantially reduced memory usage with
minimal accuracy loss. Quantized models address memory constraints in LLMs and
enhance GPU resource utilization through efficient GPU sharing. However,
quantized models have smaller KV block sizes than non-quantized models, causing
limited memory efficiency due to memory fragmentation. Also, distinct resource
usage patterns between quantized and non-quantized models require efficient
scheduling to maximize throughput. To address these challenges, we propose
FineServe, an inference serving framework for mixed-precision LLMs. FineServe's
key contributions include: (1) KV Slab, a precision-aware adaptive memory
management technique dynamically allocating KV cache based on model
quantization characteristics, significantly reducing GPU memory fragmentation,
and (2) a two-level scheduling framework comprising a global scheduler that
places models to GPUs based on request rates, latency SLOs, and memory
constraints and efficiency, and a local scheduler that adaptively adjusts batch
sizes according to real-time request fluctuations. Experimental results
demonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x
higher token generation throughput compared to the state-of-the-art GPU sharing
systems.

</details>


### [183] [MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS](https://arxiv.org/abs/2509.06362)
*Mo Xuan,Zhang yue,Wu Weigang*

Main category: cs.DC

TL;DR: MaaSO是一个针对MaaS平台的智能编排系统，通过异构实例配置优化和SLO感知请求分发，显著提升服务等级目标满足率和降低延迟


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理系统通常使用相同配置的实例，无法利用不同并行策略和推理批处理大小带来的性能差异来满足多样化的SLO需求

Method: 包含三个模块：性能分析器（分析不同配置下的实例性能）、配置优化器（优化异构实例配置）、分发器（SLO感知请求分发和防止级联超时）

Result: 相比现有方法，SLO满足率提升15-30%，响应延迟降低40-60%，显著降低整体编排开销

Conclusion: MaaSO通过利用LLM实例的异构配置能力，有效解决了MaaS平台多样化SLO需求的服务编排问题

Abstract: Model-as-a-Service (MaaS) platforms face diverse Service Level Objective
(SLO) requirements stemming from various large language model (LLM)
applications, manifested in contextual complexity, first-token latency, and
between-token latency. On the other hand, an LLM instance, when configured with
different parallelism strategies and inference batch sizes, exhibits distinct
performance characteristics and can thus be used to serve different SLO
requirements. However, current LLM inference systems typically deploy instances
of the same model with identical configurations, lacking mechanisms to leverage
such heterogeneity. To fill this research gap, we propose MaaSO, the first MaaS
Orchestrator, which comprises three modules: (1) a profiler characterizing
instance performance under diverse parallelism strategies and inference batch
sizes; (2) a placer optimizing heterogeneous instance configurations; (3) a
distributor enabling SLO-aware request distribution and preventing cascaded
timeouts in continuous batching. Experiments show that MaaSO improves the SLO
satisfaction ratio by 15 to 30% and reduces response latency by 40 to 60%
compared to existing approaches, and significantly lowers overall orchestration
overhead.

</details>


### [184] [IM-PIR: In-Memory Private Information Retrieval](https://arxiv.org/abs/2509.06514)
*Mpoki Mwaisela,Peterson Yuhala,Pascal Felber,Valerio Schiavoni*

Main category: cs.DC

TL;DR: 本文提出了首个基于内存计算(PIM)的多服务器私有信息检索(PIR)架构IM-PIR，相比传统CPU实现实现了3.7倍以上的查询吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 当前PIR实现受限于内存带宽瓶颈，需要扫描GB级数据库，而PIM架构通过内存内计算能力解决了内存带宽瓶颈并提供了大规模并行性

Method: 基于UPMEM PIM商业架构设计实现IM-PIR系统，利用PIM架构的并行性和高内存带宽优势来优化多服务器PIR算法操作

Result: 评估显示基于PIM的多服务器PIR实现相比标准CPU方法提升了超过3.7倍的查询吞吐量

Conclusion: PIM架构为计算密集型的PIR应用提供了显著性能优势，证明了内存内计算在处理内存密集型密码学原语方面的潜力

Abstract: Private information retrieval (PIR) is a cryptographic primitive that allows
a client to securely query one or multiple servers without revealing their
specific interests. In spite of their strong security guarantees, current PIR
constructions are computationally costly. Specifically, most PIR
implementations are memory-bound due to the need to scan extensive databases
(in the order of GB), making them inherently constrained by the limited memory
bandwidth in traditional processor-centric computing
architectures.Processing-in-memory (PIM) is an emerging computing paradigm that
augments memory with compute capabilities, addressing the memory bandwidth
bottleneck while simultaneously providing extensive parallelism.Recent research
has demonstrated PIM's potential to significantly improve performance across a
range of data-intensive workloads, including graph processing, genome analysis,
and machine learning.
  In this work, we propose the first PIM-based architecture for multi-server
PIR. We discuss the algorithmic foundations of the latter and show how its
operations align with the core strengths of PIM architectures: extensive
parallelism and high memory bandwidth. Based on this observation, we design and
implement IM-PIR, a PIM-based multi-server PIR approach on top of UPMEM PIM,
the first openly commercialized PIM architecture. Our evaluation demonstrates
that a PIM-based multi-server PIR implementation significantly improves query
throughput by more than 3.7x when compared to a standard CPU-based PIR
approach.

</details>


### [185] [Mangrove: Fast and Parallelizable State Replication for Blockchains](https://arxiv.org/abs/2509.06616)
*Anton Paramonov,Yann Vonlanthen,Quentin Kniep,Jakub Sliwinski,Roger Wattenhofer*

Main category: cs.DC

TL;DR: Mangrove是一种新颖的区块链扩展方法，通过为每个智能合约使用独立的共识实例来实现并行交易处理，无需全局排序。


<details>
  <summary>Details</summary>
Motivation: 解决传统单体区块链中单一共识机制导致交易处理效率低下的问题，通过并行化提升区块链性能。

Method: 采用Parallel Optimistic Agreement机制确保并行实例运行时不提交冲突交易，同时使用轻量级Byzantine Reliable Broadcast原语降低简单交易的延迟。

Result: 在无恶意行为和网络同步的乐观条件下，协议可以实现创建和执行交易之间仅需2个通信步骤的低延迟。

Conclusion: Mangrove为构建支持并行智能合约的高性能区块链提供了一种有效的扩展方案，特别适用于乐观网络环境。

Abstract: Mangrove is a novel scaling approach to building blockchains with parallel
smart contract support. Unlike in monolithic blockchains, where a single
consensus mechanism determines a strict total order over all transactions,
Mangrove uses separate consensus instances per smart contract, without a global
order. To allow multiple instances to run in parallel while ensuring that no
conflicting transactions are committed, we propose a mechanism called Parallel
Optimistic Agreement. Additionally, for simple transactions, we leverage a
lightweight Byzantine Reliable Broadcast primitive to reduce latency. Mangrove
is optimized for performance under optimistic conditions, where there is no
misbehavior and the network is synchronous. Under these conditions, our
protocol can achieve a latency of 2 communication steps between creating and
executing a transaction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [186] [Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts](https://arxiv.org/abs/2509.05323)
*Adam Cole,Mick Grierson*

Main category: cs.AI

TL;DR: 本文提出了一种可视化视频扩散变换器中注意力机制的方法，通过提取交叉注意力图来分析文本到视频生成过程中的时空注意力行为，为艺术创作提供新的工具和材料。


<details>
  <summary>Details</summary>
Motivation: 受早期视频艺术家操纵模拟视频信号创造新视觉美学的启发，本研究旨在探索生成式视频模型的注意力机制，为艺术家提供理解AI内部工作原理的工具，并将其作为创意媒介。

Method: 基于开源Wan模型构建工具，提取和可视化交叉注意力图，通过探索性探测和艺术案例研究分析注意力图在文本到视频生成中的时空行为。

Result: 开发了一个可解释的工具，能够揭示视频生成过程中注意力的时空动态，证明了注意力图既可以作为分析工具，也可以作为原始艺术材料使用。

Conclusion: 这项工作推动了可解释AI艺术(XAIxArts)领域的发展，邀请艺术家将AI内部机制重新作为创意媒介，为艺术与技术的融合开辟了新途径。

Abstract: This paper presents an artistic and technical investigation into the
attention mechanisms of video diffusion transformers. Inspired by early video
artists who manipulated analog video signals to create new visual aesthetics,
this study proposes a method for extracting and visualizing cross-attention
maps in generative video models. Built on the open-source Wan model, our tool
provides an interpretable window into the temporal and spatial behavior of
attention in text-to-video generation. Through exploratory probes and an
artistic case study, we examine the potential of attention maps as both
analytical tools and raw artistic material. This work contributes to the
growing field of Explainable AI for the Arts (XAIxArts), inviting artists to
reclaim the inner workings of AI as a creative medium.

</details>


### [187] [Perception Graph for Cognitive Attack Reasoning in Augmented Reality](https://arxiv.org/abs/2509.05324)
*Rongqian Chen,Shu Hong,Rifatul Islam,Mahdi Imani,G. Gary Tan,Tian Lan*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的知觉图模型，用于检测和分析增强现实系统中的认知攻击对用户知觉的扬曲效果。


<details>
  <summary>Details</summary>
Motivation: 增强现实系统在战术环境中应用越来越广泛，但其依赖于无缝人机交互的特性使得系统容易受到操纵用户知觉的认知攻击，影响用户的决策能力。

Method: 研究者提出了知觉图模型，该模型首先模仿人类从混合现实环境中解释关键信息的过程，然后使用语义有意义的结构来表示解释结果。

Result: 该模型能够计算出一个量化的分数，反映知觉扬曲的程度，为检测和分析认知攻击的影响提供了稳健且可测量的方法。

Conclusion: 知觉图模型为增强现实系统提供了一种有效的方法来评估和应对认知攻击对用户知觉的威胁，有助于提高系统的安全性和可靠性。

Abstract: Augmented reality (AR) systems are increasingly deployed in tactical
environments, but their reliance on seamless human-computer interaction makes
them vulnerable to cognitive attacks that manipulate a user's perception and
severely compromise user decision-making. To address this challenge, we
introduce the Perception Graph, a novel model designed to reason about human
perception within these systems. Our model operates by first mimicking the
human process of interpreting key information from an MR environment and then
representing the outcomes using a semantically meaningful structure. We
demonstrate how the model can compute a quantitative score that reflects the
level of perception distortion, providing a robust and measurable method for
detecting and analyzing the effects of such cognitive attacks.

</details>


### [188] [SynDelay: A Synthetic Dataset for Delivery Delay Prediction](https://arxiv.org/abs/2509.05325)
*Liming Xu,Yunbo Long,Alexandra Brintrup*

Main category: cs.AI

TL;DR: 这篇论文提出了SynDelay同步数据集，专门用于供应链延迟预测任务，解决了相关预测任务中高质量公开数据集缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 人工智能在供应链管理中的应用受限于高质量公开数据集的缺乏，现有数据集存在专利、小规模或维护不一致等问题，影响了可复现性和基准测试。

Method: 使用基于真实数据训练的高级生成模型来生成SynDelay同步数据集，保持真实的交付模式同时确保隐私。

Result: 虽然数据集不能完全避免噪声或不一致性，但提供了具有挑战性和实用性的测试平台，并提供了基准结果和评估指标。

Conclusion: SynDelay通过Supply Chain Data Hub公开发布，促进了供应链AI领域的数据集共享和基准测试，引导社区贡献数据集、模型和评估实践以推动研究进步。

Abstract: Artificial intelligence (AI) is transforming supply chain management, yet
progress in predictive tasks -- such as delivery delay prediction -- remains
constrained by the scarcity of high-quality, openly available datasets.
Existing datasets are often proprietary, small, or inconsistently maintained,
hindering reproducibility and benchmarking. We present SynDelay, a synthetic
dataset designed for delivery delay prediction. Generated using an advanced
generative model trained on real-world data, SynDelay preserves realistic
delivery patterns while ensuring privacy. Although not entirely free of noise
or inconsistencies, it provides a challenging and practical testbed for
advancing predictive modelling. To support adoption, we provide baseline
results and evaluation metrics as initial benchmarks, serving as reference
points rather than state-of-the-art claims. SynDelay is publicly available
through the Supply Chain Data Hub, an open initiative promoting dataset sharing
and benchmarking in supply chain AI. We encourage the community to contribute
datasets, models, and evaluation practices to advance research in this area.
All code is openly accessible at https://supplychaindatahub.org.

</details>


### [189] [MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset](https://arxiv.org/abs/2509.05330)
*Seyed Muhammad Hossein Mousavi,Atiye Ilanloo*

Main category: cs.AI

TL;DR: MVRS数据集：包含13名参与者的多模态情绪识别数据集，整合了眼动追踪、身体运动、EMG和GSR信号，通过VR情绪刺激验证了情绪可分离性


<details>
  <summary>Details</summary>
Motivation: 当前缺乏包含身体运动和生理信号的多模态数据集，限制了情感计算领域的发展

Method: 使用VR情绪刺激，同步采集眼动（VR头显摄像头）、身体运动（Kinect v2）、EMG和GSR信号（Arduino UNO），采用早期和晚期融合技术进行特征融合和分类器评估

Result: 数据集质量良好，验证了情绪的可分离性，为多模态情感计算提供了有价值的数据资源

Conclusion: MVRS数据集填补了多模态情感数据集的空白，特别在身体运动和生理信号方面，对情感计算研究有重要贡献

Abstract: Automatic emotion recognition has become increasingly important with the rise
of AI, especially in fields like healthcare, education, and automotive systems.
However, there is a lack of multimodal datasets, particularly involving body
motion and physiological signals, which limits progress in the field. To
address this, the MVRS dataset is introduced, featuring synchronized recordings
from 13 participants aged 12 to 60 exposed to VR based emotional stimuli
(relaxation, fear, stress, sadness, joy). Data were collected using eye
tracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR
signals (Arduino UNO), all timestamp aligned. Participants followed a unified
protocol with consent and questionnaires. Features from each modality were
extracted, fused using early and late fusion techniques, and evaluated with
classifiers to confirm the datasets quality and emotion separability, making
MVRS a valuable contribution to multimodal affective computing.

</details>


### [190] [Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning](https://arxiv.org/abs/2509.05346)
*Bo Yuan,Jiazi Hu*

Main category: cs.AI

TL;DR: 本研究对GPT-4o、DeepSeek-V3和GLM-4.5三种大语言模型在个性化学习辅导任务中的表现进行了实证比较，使用Gemini作为虚拟评委进行多维度评估，结果显示GPT-4o整体表现最佳。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型越来越多地被设想为个性化学习的智能助手，但在真实学习场景中的系统性对比评估仍然有限，需要实证研究来验证不同模型在辅导任务中的实际表现。

Method: 使用包含学生答案和正确性标签的数据集，要求每个LLM完成三个任务：分析测验识别知识组件、推断学生掌握情况、生成针对性指导。采用Gemini作为虚拟评委进行多维度（准确性、清晰度、可操作性、适当性）的成对比较，并使用Bradley-Terry模型分析结果。

Result: GPT-4o整体表现最优，生成的反馈信息更丰富、结构更好；DeepSeek-V3和GLM-4.5在某些方面有优势但一致性较低。

Conclusion: 研究证实了将LLMs部署为高级教学助手进行个性化支持的可行性，并为未来LLM驱动的个性化学习实证研究提供了方法学指导。

Abstract: While Large Language Models (LLMs) are increasingly envisioned as intelligent
assistants for personalized learning, systematic head-to-head evaluations
within authentic learning scenarios remain limited. This study conducts an
empirical comparison of three state-of-the-art LLMs on a tutoring task that
simulates a realistic learning setting. Using a dataset comprising a student's
answers to ten questions of mixed formats with correctness labels, each LLM is
required to (i) analyze the quiz to identify underlying knowledge components,
(ii) infer the student's mastery profile, and (iii) generate targeted guidance
for improvement. To mitigate subjectivity and evaluator bias, we employ Gemini
as a virtual judge to perform pairwise comparisons along various dimensions:
accuracy, clarity, actionability, and appropriateness. Results analyzed via the
Bradley-Terry model indicate that GPT-4o is generally preferred, producing
feedback that is more informative and better structured than its counterparts,
while DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower
consistency. These findings highlight the feasibility of deploying LLMs as
advanced teaching assistants for individualized support and provide
methodological guidance for future empirical research on LLM-driven
personalized learning.

</details>


### [191] [SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis](https://arxiv.org/abs/2509.05363)
*Lijie Ding,Changwoo Do*

Main category: cs.AI

TL;DR: SasAgent是一个基于大语言模型的多代理AI系统，用于自动化小角散射数据分析，通过SasView工具和文本交互实现用户友好的科学工作流程。


<details>
  <summary>Details</summary>
Motivation: 为了解决小角散射数据分析的复杂性，提高自动化水平，利用大语言模型的能力来简化科学工作流程。

Method: 采用多代理架构，包括协调代理和三个专业代理（SLD计算、合成数据生成、实验数据拟合），结合SasView Python库的工具和RAG文档工具。

Result: 系统能够准确解释复杂提示、计算散射长度密度、生成精确散射数据，并以高精度拟合实验数据集。

Conclusion: 这项工作展示了基于大语言模型的AI系统在小角散射研究中简化科学工作流程和增强自动化能力的潜力。

Abstract: We introduce SasAgent, a multi-agent AI system powered by large language
models (LLMs) that automates small-angle scattering (SAS) data analysis by
leveraging tools from the SasView software and enables user interaction via
text input. SasAgent features a coordinator agent that interprets user prompts
and delegates tasks to three specialized agents for scattering length density
(SLD) calculation, synthetic data generation, and experimental data fitting.
These agents utilize LLM-friendly tools to execute tasks efficiently. These
tools, including the model data tool, Retrieval-Augmented Generation (RAG)
documentation tool, bump fitting tool, and SLD calculator tool, are derived
from the SasView Python library. A user-friendly Gradio-based interface
enhances user accessibility. Through diverse examples, we demonstrate
SasAgent's ability to interpret complex prompts, calculate SLDs, generate
accurate scattering data, and fit experimental datasets with high precision.
This work showcases the potential of LLM-driven AI systems to streamline
scientific workflows and enhance automation in SAS research.

</details>


### [192] [Characterizing Fitness Landscape Structures in Prompt Engineering](https://arxiv.org/abs/2509.05375)
*Arend Hintze*

Main category: cs.AI

TL;DR: 本文通过自相关分析揭示了提示工程优化地形的结构特征，发现系统化生成与多样化生成的提示具有本质不同的地形拓扑结构


<details>
  <summary>Details</summary>
Motivation: 当前提示优化被视为黑盒问题，缺乏对优化地形拓扑结构的理解，需要系统性分析来揭示其复杂性

Method: 使用自相关分析在语义嵌入空间中分析适度地形结构，通过错误检测任务进行实验，比较两种提示生成策略：系统枚举（1,024个提示）和新颖多样化（1,000个提示）

Result: 系统提示生成产生平滑衰减的自相关，而多样化生成显示出非单调模式和中间语义距离处的峰值相关，表明地形更为凸难、层次结构化；不同错误类型显示出不同程度的凸难性

Conclusion: 研究为理解提示工程优化地形的复杂性提供了实证基础，揭示了不同提示生成策略造成的本质性地形差异

Abstract: While prompt engineering has emerged as a crucial technique for optimizing
large language model performance, the underlying optimization landscape remains
poorly understood. Current approaches treat prompt optimization as a black-box
problem, applying sophisticated search algorithms without characterizing the
landscape topology they navigate. We present a systematic analysis of fitness
landscape structures in prompt engineering using autocorrelation analysis
across semantic embedding spaces. Through experiments on error detection tasks
with two distinct prompt generation strategies -- systematic enumeration (1,024
prompts) and novelty-driven diversification (1,000 prompts) -- we reveal
fundamentally different landscape topologies. Systematic prompt generation
yields smoothly decaying autocorrelation, while diversified generation exhibits
non-monotonic patterns with peak correlation at intermediate semantic
distances, indicating rugged, hierarchically structured landscapes.
Task-specific analysis across 10 error detection categories reveals varying
degrees of ruggedness across different error types. Our findings provide an
empirical foundation for understanding the complexity of optimization in prompt
engineering landscapes.

</details>


### [193] [Code Like Humans: A Multi-Agent Solution for Medical Coding](https://arxiv.org/abs/2509.05378)
*Andreas Motzfeldt,Joakim Edin,Casper L. Christensen,Christian Hardmeier,Lars Maaløe,Anna Rogers*

Main category: cs.AI

TL;DR: 提出了Code Like Humans框架，这是首个支持完整ICD-10编码系统（7万+标签）的医疗编码解决方案，在罕见诊断代码上达到最佳性能


<details>
  <summary>Details</summary>
Motivation: 医疗编码需要将非结构化临床记录映射到诊断和程序的字母数字代码，现有方法无法支持完整的ICD-10编码系统

Method: 基于大语言模型的代理框架，实现了人类专家的官方编码指南

Result: 在罕见诊断代码上达到最佳性能，但微调判别分类器在高频代码上仍保持优势

Conclusion: 贡献了系统性能分析并识别了系统性的'盲点'（被系统低估的代码），为未来工作指明方向

Abstract: In medical coding, experts map unstructured clinical notes to alphanumeric
codes for diagnoses and procedures. We introduce Code Like Humans: a new
agentic framework for medical coding with large language models. It implements
official coding guidelines for human experts, and it is the first solution that
can support the full ICD-10 coding system (+70K labels). It achieves the best
performance to date on rare diagnosis codes (fine-tuned discriminative
classifiers retain an advantage for high-frequency codes, to which they are
limited). Towards future work, we also contribute an analysis of system
performance and identify its `blind spots' (codes that are systematically
undercoded).

</details>


### [194] [Murphys Laws of AI Alignment: Why the Gap Always Wins](https://arxiv.org/abs/2509.05381)
*Madhava Gaikwad*

Main category: cs.AI

TL;DR: 本文提出了对齐差距概念，系统分析了RLHF等对齐方法的失败模式，建立了AI对齐的墨菲定律分类，并提出对齐三难困境和MAPS框架来指导未来设计。


<details>
  <summary>Details</summary>
Motivation: 现有的人类偏好对齐方法（如RLHF、DPO等）虽然有效，但存在奖励黑客、奉承、标注者漂移和错误泛化等重复性失败模式，需要统一的理论框架来理解这些系统性缺陷。

Method: 使用KL倾斜形式化方法分析优化压力如何放大代理奖励与真实人类意图之间的分歧，通过小规模实证研究支持理论分析，并建立对齐失败分类体系。

Result: 提出了对齐差距概念、AI对齐墨菲定律分类、对齐三难困境理论框架，以及实用的MAPS（错误设定、标注、压力、偏移）设计杠杆框架。

Conclusion: 本文不是提出不可能定理，而是提供了一个重构对齐辩论的视角，围绕结构性限制和权衡提供更清晰的设计指导，帮助理解反馈式对齐的固有局限性。

Abstract: Large language models are increasingly aligned to human preferences through
reinforcement learning from human feedback (RLHF) and related methods such as
Direct Preference Optimization (DPO), Constitutional AI, and RLAIF. While
effective, these methods exhibit recurring failure patterns i.e., reward
hacking, sycophancy, annotator drift, and misgeneralization. We introduce the
concept of the Alignment Gap, a unifying lens for understanding recurring
failures in feedback-based alignment. Using a KL-tilting formalism, we
illustrate why optimization pressure tends to amplify divergence between proxy
rewards and true human intent. We organize these failures into a catalogue of
Murphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to
frame trade-offs among optimization strength, value capture, and
generalization. Small-scale empirical studies serve as illustrative support.
Finally, we propose the MAPS framework (Misspecification, Annotation, Pressure,
Shift) as practical design levers. Our contribution is not a definitive
impossibility theorem but a perspective that reframes alignment debates around
structural limits and trade-offs, offering clearer guidance for future design.

</details>


### [195] [From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation](https://arxiv.org/abs/2509.05469)
*Chenguang Wang,Xiang Yan,Yilong Dai,Ziyi Wang,Susu Xu*

Main category: cs.AI

TL;DR: 一种多段系统能够在真实街道图像上直接编辑和重新设计自行车设施，通过车道定位、提示优化、设计生成和自动评估等步骤生成现实且符合上下文的设计方案。


<details>
  <summary>Details</summary>
Motivation: 传统街道设计渲染方案需要大量人工劳动，影响公众参与和协作决策，而现有AI生成方法需要大量领域数据且难以实现精确的空间变化设计。

Method: 采用多段系统架构，整合车道定位、提示优化、设计生成和自动化评估等模块，直接在真实街道图像上编辑自行车设施。

Result: 在多样化城市场景中证明系统能够适应不同路径几何和环境条件，一致生成视觉一致且符合指令要求的结果。

Conclusion: 这项工作为应用多段流水线到交通基础设施规划和设施设计领域奠定了基础。

Abstract: Realistic visual renderings of street-design scenarios are essential for
public engagement in active transportation planning. Traditional approaches are
labor-intensive, hindering collective deliberation and collaborative
decision-making. While AI-assisted generative design shows transformative
potential by enabling rapid creation of design scenarios, existing generative
approaches typically require large amounts of domain-specific training data and
struggle to enable precise spatial variations of design/configuration in
complex street-view scenes. We introduce a multi-agent system that edits and
redesigns bicycle facilities directly on real-world street-view imagery. The
framework integrates lane localization, prompt optimization, design generation,
and automated evaluation to synthesize realistic, contextually appropriate
designs. Experiments across diverse urban scenarios demonstrate that the system
can adapt to varying road geometries and environmental conditions, consistently
yielding visually coherent and instruction-compliant results. This work
establishes a foundation for applying multi-agent pipelines to transportation
infrastructure planning and facility design.

</details>


### [196] [TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation](https://arxiv.org/abs/2509.05550)
*Zixi Li*

Main category: cs.AI

TL;DR: TreeGPT是一种新颖的神经网络架构，结合了transformer注意力机制和全局父子聚合，用于处理抽象语法树(AST)的程序合成任务，在ARC Prize 2025数据集上达到96%准确率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法要么依赖序列处理，要么使用图神经网络，无法有效捕捉程序语法树的层次结构和全局依赖关系。需要一种能够同时处理局部依赖和层次结构的混合架构。

Method: 采用混合设计：使用self-attention捕捉局部依赖，TreeFFN通过迭代消息传递建模层次树结构。核心创新是全局父子聚合机制，允许节点通过T次迭代逐步聚合整个树结构信息。包含门控聚合、残差连接和双向传播等增强功能。

Result: 在ARC Prize 2025数据集上达到96%准确率，显著优于transformer基线(1.3%)、Grok-4(15.9%)和SOAR(52%)等方法，仅使用150万参数。消融研究表明边缘投影是最关键组件。

Conclusion: TreeGPT通过结合注意力机制和树结构聚合，在程序合成任务中取得了优异性能，证明了混合架构在处理层次化数据结构方面的有效性，为神经程序合成提供了新的解决方案。

Abstract: We introduce TreeGPT, a novel neural architecture that combines
transformer-based attention mechanisms with global parent-child aggregation for
processing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.
Unlike traditional approaches that rely solely on sequential processing or
graph neural networks, TreeGPT employs a hybrid design that leverages both
self-attention for capturing local dependencies and a specialized Tree
Feed-Forward Network (TreeFFN) for modeling hierarchical tree structures
through iterative message passing.
  The core innovation lies in our Global Parent-Child Aggregation mechanism,
formalized as: $$h_i^{(t+1)} = \sigma \Big( h_i^{(0)} + W_{pc} \sum_{(p,c) \in
E_i} f(h_p^{(t)}, h_c^{(t)}) + b \Big)$$ where $h_i^{(t)}$ represents the
hidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges
involving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This
formulation enables each node to progressively aggregate information from the
entire tree structure through $T$ iterations.
  Our architecture integrates optional enhancements including gated aggregation
with learnable edge weights, residual connections for gradient stability, and
bidirectional propagation for capturing both bottom-up and top-down
dependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging
visual reasoning benchmark requiring abstract pattern recognition and rule
inference. Experimental results demonstrate that TreeGPT achieves 96\%
accuracy, significantly outperforming transformer baselines (1.3\%),
large-scale models like Grok-4 (15.9\%), and specialized program synthesis
methods like SOAR (52\%) while using only 1.5M parameters. Our comprehensive
ablation study reveals that edge projection is the most critical component,
with the combination of edge projection and gating achieving optimal
performance.

</details>


### [197] [OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision](https://arxiv.org/abs/2509.05578)
*Ruixun Liu,Lingyu Kong,Derun Li,Hang Zhao*

Main category: cs.AI

TL;DR: OccVLA是一个创新的多模态框架，通过将3D占用表示集成到统一推理过程中，解决了MLLM在3D空间理解方面的局限性，无需昂贵的手动标注即可实现自动驾驶的精细空间结构学习。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉语言推理方面表现出色，但缺乏稳健的3D空间理解能力，这对于自动驾驶至关重要。主要挑战包括：构建有效3D表示的难度和缺乏大规模3D视觉语言预训练导致的细节丢失。

Method: 提出OccVLA框架，将密集3D占用既作为预测输出又作为监督信号，使模型能够直接从2D视觉输入学习精细空间结构。占用预测被视为隐式推理过程，在推理时可跳过而不影响性能。

Result: 在nuScenes基准测试中实现了轨迹规划的最先进结果，在3D视觉问答任务上表现出优越性能。

Conclusion: OccVLA提供了一个可扩展、可解释且完全基于视觉的自动驾驶解决方案，无需额外计算开销即可实现强大的3D空间理解。

Abstract: Multimodal large language models (MLLMs) have shown strong vision-language
reasoning abilities but still lack robust 3D spatial understanding, which is
critical for autonomous driving. This limitation stems from two key challenges:
(1) the difficulty of constructing accessible yet effective 3D representations
without expensive manual annotations, and (2) the loss of fine-grained spatial
details in VLMs due to the absence of large-scale 3D vision-language
pretraining. To address these challenges, we propose OccVLA, a novel framework
that integrates 3D occupancy representations into a unified multimodal
reasoning process. Unlike prior approaches that rely on explicit 3D inputs,
OccVLA treats dense 3D occupancy as both a predictive output and a supervisory
signal, enabling the model to learn fine-grained spatial structures directly
from 2D visual inputs. The occupancy predictions are regarded as implicit
reasoning processes and can be skipped during inference without performance
degradation, thereby adding no extra computational overhead. OccVLA achieves
state-of-the-art results on the nuScenes benchmark for trajectory planning and
demonstrates superior performance on 3D visual question-answering tasks,
offering a scalable, interpretable, and fully vision-based solution for
autonomous driving.

</details>


### [198] [MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions](https://arxiv.org/abs/2509.05685)
*Jian Yang,Jiahui Wu,Li Fang,Hongchao Fan,Bianying Zhang,Huijie Zhao,Guangyi Yang,Rui Xin,Xiong You*

Main category: cs.AI

TL;DR: MSRFormer是一个新颖的道路网络表示学习框架，通过多尺度空间交互和轨迹数据整合，解决了道路网络异质性和层次性挑战，在道路网络分析任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 城市道路网络的异质性和层次性给准确表示学习带来挑战，传统图神经网络因同质性假设和单尺度结构关注而难以有效处理。

Method: 提出MSRFormer框架：使用空间流卷积从轨迹数据提取小尺度特征，识别尺度相关的空间交互区域，采用图变换器捕捉多尺度复杂空间依赖，通过残差连接融合特征，最后使用对比学习算法得到最终表示。

Result: 在两个真实数据集上的验证表明，MSRFormer在道路网络分析任务中优于基线方法，性能提升达16%，交通相关任务从轨迹数据整合中获益更多。

Conclusion: 该研究为开发任务无关的道路网络表示模型提供了实用框架，揭示了尺度效应与空间交互流异质性之间的关联模式。

Abstract: Transforming road network data into vector representations using deep
learning has proven effective for road network analysis. However, urban road
networks' heterogeneous and hierarchical nature poses challenges for accurate
representation learning. Graph neural networks, which aggregate features from
neighboring nodes, often struggle due to their homogeneity assumption and focus
on a single structural scale. To address these issues, this paper presents
MSRFormer, a novel road network representation learning framework that
integrates multi-scale spatial interactions by addressing their flow
heterogeneity and long-distance dependencies. It uses spatial flow convolution
to extract small-scale features from large trajectory datasets, and identifies
scale-dependent spatial interaction regions to capture the spatial structure of
road networks and flow heterogeneity. By employing a graph transformer,
MSRFormer effectively captures complex spatial dependencies across multiple
scales. The spatial interaction features are fused using residual connections,
which are fed to a contrastive learning algorithm to derive the final road
network representation. Validation on two real-world datasets demonstrates that
MSRFormer outperforms baseline methods in two road network analysis tasks. The
performance gains of MSRFormer suggest the traffic-related task benefits more
from incorporating trajectory data, also resulting in greater improvements in
complex road network structures with up to 16% improvements compared to the
most competitive baseline method. This research provides a practical framework
for developing task-agnostic road network representation models and highlights
distinct association patterns of the interplay between scale effects and flow
heterogeneity of spatial interactions.

</details>


### [199] [Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs](https://arxiv.org/abs/2509.05714)
*Zhaoyu Fan,Kaihang Pan,Mingze Zhou,Bosheng Qin,Juncheng Li,Shengyu Zhang,Wenqiao Zhang,Siliang Tang,Fei Wu,Yueting Zhuang*

Main category: cs.AI

TL;DR: CogEdit是一个评估多模态大语言模型元认知知识编辑能力的新基准，包含三个层次：反事实驱动编辑、边界约束编辑和噪声鲁棒编辑。作者提出了MIND框架来提升元认知编辑能力，实验表明其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑基准主要关注认知层面的修改，缺乏对更深层次元认知过程的关注，需要开发能够评估模型自我意识、泛化能力和不确定性处理的基准。

Method: 提出了MIND框架，包含三个核心组件：构建元知识记忆库实现自我意识、使用博弈论交互监控知识激活、通过标签精化实现噪声鲁棒更新。

Result: MIND框架在传统和元认知知识编辑基准上都取得了显著优于现有认知编辑方法的性能表现。

Conclusion: CogEdit基准和MIND框架有效填补了多模态大语言模型元认知知识编辑评估的空白，为提升模型的知识更新和自我监控能力提供了新方向。

Abstract: Knowledge editing enables multimodal large language models (MLLMs) to
efficiently update outdated or incorrect information. However, existing
benchmarks primarily emphasize cognitive-level modifications while lacking a
focus on deeper meta-cognitive processes. To bridge this gap, we introduce
CogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge
editing abilities across three levels: (1) Counterfactual-Driven Editing,
assessing self-awareness of knowledge correctness changes; (2) Boundary
Constraint Editing, ensuring appropriate generalization without unintended
interference; and (3) Noise-Robust Editing, promoting reflective evaluation of
uncertain information. To advance meta-cognitive editing, we propose MIND
(Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that
constructs a meta-knowledge memory for self-awareness, employs game-theoretic
interactions to monitor knowledge activation, and incorporates label refinement
for noise-robust updates. Extensive experiments show that MIND significantly
outperforms existing cognitive editing approaches, achieving strong performance
on both traditional and meta-cognitive knowledge editing benchmarks.

</details>


### [200] [Hyperbolic Large Language Models](https://arxiv.org/abs/2509.05757)
*Sarang Patil,Zeyong Zhang,Yiran Huang,Tengfei Ma,Mengjia Xu*

Main category: cs.AI

TL;DR: 该论文综述了双曲几何在大语言模型中的应用，探讨了如何利用非欧几里得空间来更好地表示层次化结构数据，并提出了Hyperbolic LLMs的分类框架。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据往往具有高度非欧几里得的层次化结构（如蛋白质网络、交通网络、语言结构等），传统LLMs在处理这类数据的语义蕴含和层次关系方面存在局限，而双曲几何能有效建模树状层次结构。

Method: 提出了Hyperbolic LLMs的四大分类框架：(1)通过指数/对数映射的双曲LLMs；(2)双曲微调模型；(3)完全双曲LLMs；(4)双曲状态空间模型。

Result: 建立了双曲几何在LLMs中应用的系统性分类体系，总结了该领域的最新进展，并提供了相关论文、模型、数据集和代码的资源库。

Conclusion: 双曲几何为LLMs处理层次化结构数据提供了有前景的方向，未来需要在应用开发和理论研究方面进一步探索。

Abstract: Large language models (LLMs) have achieved remarkable success and
demonstrated superior performance across various tasks, including natural
language processing (NLP), weather forecasting, biological protein folding,
text generation, and solving mathematical problems. However, many real-world
data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein
networks, transportation networks, financial networks, brain networks, and
linguistic structures or syntactic trees in natural languages. Effectively
learning intrinsic semantic entailment and hierarchical relationships from
these raw, unstructured input data using LLMs remains an underexplored area.
Due to its effectiveness in modeling tree-like hierarchical structures,
hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity
as an expressive latent representation space for complex data modeling across
domains such as graphs, images, languages, and multi-modal data. Here, we
provide a comprehensive and contextual exposition of recent advancements in
LLMs that leverage hyperbolic geometry as a representation space to enhance
semantic representation learning and multi-scale reasoning. Specifically, the
paper presents a taxonomy of the principal techniques of Hyperbolic LLMs
(HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log
maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4)
hyperbolic state-space models. We also explore crucial potential applications
and outline future research directions. A repository of key papers, models,
datasets, and code implementations is available at
https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.

</details>


### [201] [DRF: LLM-AGENT Dynamic Reputation Filtering Framework](https://arxiv.org/abs/2509.05764)
*Yuwei Lou,Hao Hu,Shaocong Ma,Zongfei Zhang,Liang Wang,Jidong Ge,Xianping Tao*

Main category: cs.AI

TL;DR: DRF框架通过动态信誉评分和过滤机制，解决了多智能体系统中性能量化和信誉评估的挑战，显著提升了任务完成质量和协作效率。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和多智能体系统的发展，现有系统缺乏量化智能体性能和评估智能体可信度的机制，这限制了系统处理复杂任务的能力。

Method: 提出了DRF动态信誉过滤框架，包括：1）构建交互评分网络量化智能体性能；2）设计信誉评分机制衡量智能体诚实度和能力；3）集成基于上置信界策略提升智能体选择效率。

Result: 实验表明，DRF在逻辑推理和代码生成任务中显著提高了任务完成质量和协作效率。

Conclusion: DRF为多智能体系统处理大规模任务提供了一种新的有效方法，通过信誉机制提升了系统整体性能。

Abstract: With the evolution of generative AI, multi - agent systems leveraging large -
language models(LLMs) have emerged as a powerful tool for complex tasks.
However, these systems face challenges in quantifying agent performance and
lack mechanisms to assess agent credibility. To address these issues, we
introduce DRF, a dynamic reputation filtering framework. DRF constructs an
interactive rating network to quantify agent performance, designs a reputation
scoring mechanism to measure agent honesty and capability, and integrates an
Upper Confidence Bound - based strategy to enhance agent selection efficiency.
Experiments show that DRF significantly improves task completion quality and
collaboration efficiency in logical reasoning and code - generation tasks,
offering a new approach for multi - agent systems to handle large - scale
tasks.

</details>


### [202] [Decision-Focused Learning Enhanced by Automated Feature Engineering for Energy Storage Optimisation](https://arxiv.org/abs/2509.05772)
*Nasser Alkhulaifi,Ismail Gokay Dogan,Timothy R. Cargan,Alexander L. Bowler,Direnc Pekaslan,Nicholas J. Watson,Isaac Triguero*

Main category: cs.AI

TL;DR: 本文提出了一种结合自动特征工程(AFE)和决策聚焦学习(DFL)的框架，用于解决电池能量存储系统(BESS)中的优化问题，在小数据集上实现了更优的运营成本效果。


<details>
  <summary>Details</summary>
Motivation: 能源管理中的不确定性参数导致优化困难，传统预测-优化(PTO)方法存在预测错误传播问题，而新兴的DFL方法在实际应用中面临数据稀缺和变异性挑战。

Method: 提出AFE-DFL框架，通过自动特征工程提取更丰富的表征，在小数据集上同时进行电价需求预测和BESS运营优化，以最小化成本。

Result: 在英国实际房屋数据集上验证，DFL法比PTO方法运营成本更低，加入AFE后让DFL性能提升22.9-56.5%。

Conclusion: 证明了DFL在实际应用中的可行性，领域特定的AFE能够增强DFL效果并减少对领域专家知识的依赖，对面临类似挑战的能源管理系统具有广泛意义。

Abstract: Decision-making under uncertainty in energy management is complicated by
unknown parameters hindering optimal strategies, particularly in Battery Energy
Storage System (BESS) operations. Predict-Then-Optimise (PTO) approaches treat
forecasting and optimisation as separate processes, allowing prediction errors
to cascade into suboptimal decisions as models minimise forecasting errors
rather than optimising downstream tasks. The emerging Decision-Focused Learning
(DFL) methods overcome this limitation by integrating prediction and
optimisation; however, they are relatively new and have been tested primarily
on synthetic datasets or small-scale problems, with limited evidence of their
practical viability. Real-world BESS applications present additional
challenges, including greater variability and data scarcity due to collection
constraints and operational limitations. Because of these challenges, this work
leverages Automated Feature Engineering (AFE) to extract richer representations
and improve the nascent approach of DFL. We propose an AFE-DFL framework
suitable for small datasets that forecasts electricity prices and demand while
optimising BESS operations to minimise costs. We validate its effectiveness on
a novel real-world UK property dataset. The evaluation compares DFL methods
against PTO, with and without AFE. The results show that, on average, DFL
yields lower operating costs than PTO and adding AFE further improves the
performance of DFL methods by 22.9-56.5% compared to the same models without
AFE. These findings provide empirical evidence for DFL's practical viability in
real-world settings, indicating that domain-specific AFE enhances DFL and
reduces reliance on domain expertise for BESS optimisation, yielding economic
benefits with broader implications for energy management systems facing similar
challenges.

</details>


### [203] [Chatbot To Help Patients Understand Their Health](https://arxiv.org/abs/2509.05818)
*Won Seok Jang,Hieu Tran,Manav Mistry,SaiKiran Gandluri,Yifan Zhang,Sharmin Sultana,Sunjae Kown,Yuan Zhang,Zonghai Yao,Hong Yu*

Main category: cs.AI

TL;DR: NoteAid-Chatbot是一个基于多智能体LLM和强化学习的对话AI，通过'学习即对话'框架帮助患者理解医疗信息，无需人工标注数据，在轻量级LLaMA 3.2 3B模型上训练，在医疗对话中表现出色。


<details>
  <summary>Details</summary>
Motivation: 患者需要具备相关知识才能积极参与自身护理，因此需要开发能够促进患者理解的对话AI系统。

Method: 采用两阶段训练：首先在合成生成的医疗对话数据上进行监督微调，然后通过强化学习（PPO）在模拟医院出院场景中进行奖励建模训练。

Result: NoteAid-Chatbot展现出关键的新兴行为（清晰性、相关性和结构化对话），在图灵测试中超越非专家人类表现。

Conclusion: 该方法展示了低成本PPO强化学习在现实开放域对话中的应用可行性，扩展了基于RL的对齐方法的适用性。

Abstract: Patients must possess the knowledge necessary to actively participate in
their care. We present NoteAid-Chatbot, a conversational AI that promotes
patient understanding via a novel 'learning as conversation' framework, built
on a multi-agent large language model (LLM) and reinforcement learning (RL)
setup without human-labeled data. NoteAid-Chatbot was built on a lightweight
LLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on
conversational data synthetically generated using medical conversation
strategies, followed by RL with rewards derived from patient understanding
assessments in simulated hospital discharge scenarios. Our evaluation, which
includes comprehensive human-aligned assessments and case studies, demonstrates
that NoteAid-Chatbot exhibits key emergent behaviors critical for patient
education, such as clarity, relevance, and structured dialogue, even though it
received no explicit supervision for these attributes. Our results show that
even simple Proximal Policy Optimization (PPO)-based reward modeling can
successfully train lightweight, domain-specific chatbots to handle multi-turn
interactions, incorporate diverse educational strategies, and meet nuanced
communication objectives. Our Turing test demonstrates that NoteAid-Chatbot
surpasses non-expert human. Although our current focus is on healthcare, the
framework we present illustrates the feasibility and promise of applying
low-cost, PPO-based RL to realistic, open-ended conversational domains,
broadening the applicability of RL-based alignment methods.

</details>


### [204] [MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration](https://arxiv.org/abs/2509.05933)
*Md Hasebul Hasan,Mahir Labib Dihan,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: MapAgent是一个分层多智能体框架，专门用于地图集成的地理空间推理，通过解耦规划和执行来提升LLM在地理空间任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理框架主要针对数学、编程和网页自动化等领域，在地理空间任务中表现不足，需要空间推理、多跳规划和实时地图交互能力。

Method: 采用分层多智能体架构：高层规划器分解复杂查询为子目标，专用地图工具代理并行协调相关API获取地理空间数据，简单模块无需额外代理开销。

Result: 在四个地理空间基准测试(MapEval-Textual, MapEval-API, MapEval-Visual, MapQA)上显著优于现有工具增强和代理基线方法。

Conclusion: MapAgent通过分层设计和专用工具集有效解决了地理空间推理的挑战，降低了认知负载，提高了工具选择准确性，实现了跨相似API的精确协调。

Abstract: Agentic AI has significantly extended the capabilities of large language
models (LLMs) by enabling complex reasoning and tool use. However, most
existing frameworks are tailored to domains such as mathematics, coding, or web
automation, and fall short on geospatial tasks that require spatial reasoning,
multi-hop planning, and real-time map interaction. To address these challenges,
we introduce MapAgent, a hierarchical multi-agent plug-and-play framework with
customized toolsets and agentic scaffolds for map-integrated geospatial
reasoning. Unlike existing flat agent-based approaches that treat tools
uniformly-often overwhelming the LLM when handling similar but subtly different
geospatial APIs-MapAgent decouples planning from execution. A high-level
planner decomposes complex queries into subgoals, which are routed to
specialized modules. For tool-heavy modules-such as map-based services-we then
design a dedicated map-tool agent that efficiently orchestrates related APIs
adaptively in parallel to effectively fetch geospatial data relevant for the
query, while simpler modules (e.g., solution generation or answer extraction)
operate without additional agent overhead. This hierarchical design reduces
cognitive load, improves tool selection accuracy, and enables precise
coordination across similar APIs. We evaluate MapAgent on four diverse
geospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and
MapQA-and demonstrate substantial gains over state-of-the-art tool-augmented
and agentic baselines. We open-source our framwork at
https://github.com/Hasebul/MapAgent.

</details>


### [205] [Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL](https://arxiv.org/abs/2509.06024)
*Haoyang He,Zihua Rong,Kun Ji,Chenyang Li,Qing Huang,Chong Xia,Lan Yang,Honggang Zhang*

Main category: cs.AI

TL;DR: 提出了DRER强化学习奖励框架，通过推理质量奖励和动态长度优势来优化大语言模型的推理能力，在Logictree数据集上7B模型达到GPT-3水平，推理置信度提升30%


<details>
  <summary>Details</summary>
Motivation: 现有的基于规则的奖励函数只评估答案格式和正确性，无法判断推理链是否真正改善答案，且任务特定训练对逻辑深度的控制有限，难以揭示模型的真实推理能力

Method: 提出动态推理效率奖励(DRER)框架：1)推理质量奖励为有益推理链分配细粒度信用；2)动态长度优势对偏离验证阈值的响应进行优势衰减；同时发布Logictree数据集用于训练和评估

Result: 7B模型在400训练步后达到GPT-3-mini水平，推理增强答案的平均置信度提升30%，在多种逻辑推理数据集和AIME24数学基准上表现出良好的泛化能力

Conclusion: DRER框架有效塑造了CoT行为，为增强大语言模型的形式推理能力提供了实用路径

Abstract: Reinforcement learning (RL) has recently become the dominant paradigm for
strengthening the reasoning abilities of large language models (LLMs). Yet the
rule-based reward functions commonly used on mathematical or programming
benchmarks assess only answer format and correctness, providing no signal as to
whether the induced Chain-of-Thought (CoT) actually improves the answer.
Furthermore, such task-specific training offers limited control over logical
depth and therefore may fail to reveal a model's genuine reasoning capacity. We
propose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward
framework that reshapes both reward and advantage signals. (i) A Reasoning
Quality Reward assigns fine-grained credit to those reasoning chains that
demonstrably raise the likelihood of the correct answer, directly incentivising
the trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage
decays the advantage of responses whose length deviates from a
validation-derived threshold, stabilising training. To facilitate rigorous
assessment, we also release Logictree, a dynamically constructed deductive
reasoning dataset that functions both as RL training data and as a
comprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B
model attains GPT-o3-mini level performance on Logictree with 400 trianing
steps, while the average confidence of CoT-augmented answers rises by 30%. The
model further exhibits generalisation across diverse logical-reasoning
datasets, and the mathematical benchmark AIME24. These results illuminate how
RL shapes CoT behaviour and chart a practical path toward enhancing
formal-reasoning skills in large language models. All code and data are
available in repository https://github.com/Henryhe09/DRER.

</details>


### [206] [Reverse-Engineered Reasoning for Open-Ended Generation](https://arxiv.org/abs/2509.06160)
*Haozhe Wang,Haoran Que,Qixin Xu,Minghao Liu,Wangchunshu Zhou,Jiazhan Feng,Wanjun Zhong,Wei Ye,Tong Yang,Wenhao Huang,Ge Zhang,Fangzhen Lin*

Main category: cs.AI

TL;DR: REER是一种新的深度推理范式，通过从已知良好解决方案反向工程发现潜在的逐步推理过程，解决了开放创造性生成中的推理挑战


<details>
  <summary>Details</summary>
Motivation: 传统强化学习和指令蒸馏方法在开放创造性生成中存在局限性：RL缺乏清晰奖励信号，蒸馏方法成本高昂且受限于教师模型能力

Method: 提出REER（反向工程推理）范式，从已知良好解决方案反向计算发现潜在的逐步深度推理过程，使用可扩展的无梯度方法构建DeepWriting-20K数据集

Result: 基于该数据训练的DeepWriter-8B模型超越了强大的开源基线，性能与GPT-4o和Claude 3.5等领先专有模型相当甚至更优

Conclusion: REER为开放创造性任务中的深度推理提供了新的可行范式，通过反向工程方法有效解决了传统方法的局限性

Abstract: While the ``deep reasoning'' paradigm has spurred significant advances in
verifiable domains like mathematics, its application to open-ended, creative
generation remains a critical challenge. The two dominant methods for
instilling reasoning -- reinforcement learning (RL) and instruction
distillation -- falter in this area; RL struggles with the absence of clear
reward signals and high-quality reward models, while distillation is
prohibitively expensive and capped by the teacher model's capabilities. To
overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a
new paradigm that fundamentally shifts the approach. Instead of building a
reasoning process ``forwards'' through trial-and-error or imitation, REER works
``backwards'' from known-good solutions to computationally discover the latent,
step-by-step deep reasoning process that could have produced them. Using this
scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a
large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.
Our model, DeepWriter-8B, trained on this data, not only surpasses strong
open-source baselines but also achieves performance competitive with, and at
times superior to, leading proprietary models like GPT-4o and Claude 3.5.

</details>


### [207] [From Long to Short: LLMs Excel at Trimming Own Reasoning Chains](https://arxiv.org/abs/2509.06174)
*Wei Han,Geng Zhan,Sicheng Yu,Chenyu Wang,Bryan Hooi*

Main category: cs.AI

TL;DR: EDIT是一种测试时缩放方法，通过约束引导生成和联合跟踪长度与答案分布，帮助大型推理模型找到最短的正确推理路径，平衡简洁性和正确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)在复杂推理任务中表现出色，但存在过度思考问题，即过度复杂化简单问题，导致推理路径冗长、策略频繁切换，影响可解释性和用户体验。

Method: 提出EDIT方法，采用约束引导生成，在测试时联合跟踪推理路径长度和答案分布，在不同约束条件下选择在简洁性和正确性之间达到最优平衡的响应。

Result: 在多种模型和数据集上的广泛实验表明，EDIT显著提高了推理效率，生成紧凑且信息丰富的输出，改善了可读性和用户体验。

Conclusion: EDIT有效解决了大型推理模型的过度思考问题，通过动态推理修剪实现了推理效率的显著提升，为平衡多个生成目标提供了有效解决方案。

Abstract: O1/R1 style large reasoning models (LRMs) signal a substantial leap forward
over conventional instruction-following LLMs. By applying test-time scaling to
generate extended reasoning paths, they establish many SOTAs across a wide
range of complex reasoning tasks. However, recent studies show that LRMs are
prone to suffer from overthinking -- the tendency to overcomplicate simple
problems, leading to excessive strategy switching and long, convoluted
reasoning traces that hinder their interpretability. To mitigate this issue, we
conduct a systematic investigation into the reasoning efficiency of a broad set
of LRMs and uncover a common dilemma: the difficulty in balancing multiple
generation objectives such as correctness and brevity. Based on this discovery,
we propose a test-time scaling method, EDIT (Efficient Dynamic Inference
Trimming), which efficiently guides LRMs to identify the shortest correct
reasoning paths at test time. EDIT employs constraint-guided generation while
jointly tracking length and answer distributions under varying constraints,
allowing it to select responses that strike an optimal balance between
conciseness and correctness. Extensive experiments across diverse models and
datasets show that EDIT substantially enhance the reasoning efficiency,
producing compact yet informative outputs that improve readability and user
experience.

</details>


### [208] [PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments](https://arxiv.org/abs/2509.06235)
*Olivier Schipper,Yudi Zhang,Yali Du,Mykola Pechenizkiy,Meng Fang*

Main category: cs.AI

TL;DR: 这篇论文提出了PillagerBench框架和TactiCrafter系统，用于在Minecraft竞争性多代理环境中评估LLM基代理的战略性能力。TactiCrafter通过人类可读战术、因果关系学习和适应性学习显著超过基准方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基代理在合作性任务中表现良好，但在竞争性多代理环境中的效果仍未充分探索，需要专门的评测框架来填补这一空白。

Method: 开发了PillagerBench框架，提供可扩展API、多轮测试和规则基内置对手。同时提出TactiCrafter系统，通过人类可读战术促进团队合作，学习因果关系并适应对手策略。

Result: TactiCrafter在性能评测中显著超过基准方法，并通过自我对抗展现出适应性学习能力。还分析了其在多个游戏式中的学习过程和战略迭代。

Conclusion: 该研究为竞争性多代理AI领域提供了重要的评测工具和有效的解决方案。开源PillagerBench框架将促进该领域的进一步研究发展。

Abstract: LLM-based agents have shown promise in various cooperative and strategic
reasoning tasks, but their effectiveness in competitive multi-agent
environments remains underexplored. To address this gap, we introduce
PillagerBench, a novel framework for evaluating multi-agent systems in
real-time competitive team-vs-team scenarios in Minecraft. It provides an
extensible API, multi-round testing, and rule-based built-in opponents for
fair, reproducible comparisons. We also propose TactiCrafter, an LLM-based
multi-agent system that facilitates teamwork through human-readable tactics,
learns causal dependencies, and adapts to opponent strategies. Our evaluation
demonstrates that TactiCrafter outperforms baseline approaches and showcases
adaptive learning through self-play. Additionally, we analyze its learning
process and strategic evolution over multiple game episodes. To encourage
further research, we have open-sourced PillagerBench, fostering advancements in
multi-agent AI for competitive environments.

</details>


### [209] [Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning](https://arxiv.org/abs/2509.06239)
*Manvi Jha,Jiaxin Wan,Deming Chen*

Main category: cs.AI

TL;DR: Proof2Silicon是一个端到端的硬件综合框架，通过PREFACE的强化学习提示优化生成可验证的Dafny代码，然后自动转换为C代码并最终合成RTL硬件设计，实现了从自然语言规范到硅实现的自动化流程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现出色，但生成的代码经常无法通过形式化验证，这在硬件和安全关键领域是基本要求。需要一种方法能够生成经过形式化验证的正确代码。

Method: 1) 使用PREFACE的验证器驱动RL代理迭代优化提示生成，确保Dafny代码正确性；2) 将验证后的Dafny程序自动转换为可综合的高级C代码；3) 使用Vivado HLS生成RTL实现

Result: 在100个任务的基准测试中，PREFACE的RL引导提示优化将Dafny验证成功率提高了21%。Proof2Silicon实现了高达72%的端到端硬件综合成功率。

Conclusion: 该研究展示了一个强大、可扩展的自动化流程，能够实现LLM驱动的形式化验证硬件综合，桥接了自然语言规范和硅实现之间的鸿沟。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
automated code generation but frequently produce code that fails formal
verification, an essential requirement for hardware and safety-critical
domains. To overcome this fundamental limitation, we previously proposed
PREFACE, a model-agnostic framework based on reinforcement learning (RL) that
iteratively repairs the prompts provided to frozen LLMs, systematically
steering them toward generating formally verifiable Dafny code without costly
fine-tuning. This work presents Proof2Silicon, a novel end-to-end synthesis
framework that embeds the previously proposed PREFACE flow to enable the
generation of correctness-by-construction hardware directly from natural
language specifications. Proof2Silicon operates by: (1) leveraging PREFACE's
verifier-driven RL agent to optimize prompt generation iteratively, ensuring
Dafny code correctness; (2) automatically translating verified Dafny programs
into synthesizable high-level C using Dafny's Python backend and PyLog; and (3)
employing Vivado HLS to produce RTL implementations. Evaluated rigorously on a
challenging 100-task benchmark, PREFACE's RL-guided prompt optimization
consistently improved Dafny verification success rates across diverse LLMs by
up to 21%. Crucially, Proof2Silicon achieved an end-to-end hardware synthesis
success rate of up to 72%, generating RTL designs through Vivado HLS synthesis
flows. These results demonstrate a robust, scalable, and automated pipeline for
LLM-driven, formally verified hardware synthesis, bridging natural-language
specification and silicon realization.

</details>


### [210] [REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents](https://arxiv.org/abs/2509.06269)
*Vishal Raman,Vijai Aravindh R,Abhijith Ragav*

Main category: cs.AI

TL;DR: REMI是一个基于因果模式记忆的多模态生活方式助手，通过个人因果知识图谱和因果推理引擎提供可解释的个性化推荐


<details>
  <summary>Details</summary>
Motivation: 现有AI助手难以整合复杂个人数据和因果知识，导致建议缺乏个性化和解释性

Method: 采用因果模式记忆架构，包含个人因果图谱、因果推理引擎和模式规划模块，由大语言模型协调工作

Result: 基于CSM的代理比基线LLM代理能提供更上下文感知、用户对齐的推荐

Conclusion: 这项工作展示了在个性化代理中进行记忆增强因果推理的新方法，推动了透明可信AI生活方式助手的发展

Abstract: Personalized AI assistants often struggle to incorporate complex personal
data and causal knowledge, leading to generic advice that lacks explanatory
power. We propose REMI, a Causal Schema Memory architecture for a multimodal
lifestyle agent that integrates a personal causal knowledge graph, a causal
reasoning engine, and a schema based planning module. The idea is to deliver
explainable, personalized recommendations in domains like fashion, personal
wellness, and lifestyle planning. Our architecture uses a personal causal graph
of the user's life events and habits, performs goal directed causal traversals
enriched with external knowledge and hypothetical reasoning, and retrieves
adaptable plan schemas to generate tailored action plans. A Large Language
Model orchestrates these components, producing answers with transparent causal
explanations. We outline the CSM system design and introduce new evaluation
metrics for personalization and explainability, including Personalization
Salience Score and Causal Reasoning Accuracy, to rigorously assess its
performance. Results indicate that CSM based agents can provide more context
aware, user aligned recommendations compared to baseline LLM agents. This work
demonstrates a novel approach to memory augmented, causal reasoning in
personalized agents, advancing the development of transparent and trustworthy
AI lifestyle assistants.

</details>


### [211] [TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning](https://arxiv.org/abs/2509.06278)
*Chuang Jiang,Mingyue Cheng,Xiaoyu Tao,Qingyang Mao,Jie Ouyang,Qi Liu*

Main category: cs.AI

TL;DR: TableMind是一个基于大语言模型的表格推理代理，通过自主工具调用、代码执行和策略自适应，显著提升表格数据推理的准确性和计算精度。


<details>
  <summary>Details</summary>
Motivation: 传统文本方法在复杂数值计算和细粒度操作上表现不佳，现有工具集成方法缺乏真正的自主适应性，需要更灵活的表格推理解决方案。

Method: 采用两阶段微调范式：先在高质量推理轨迹上进行监督微调建立工具使用模式，然后通过强化微调优化多目标策略，并提出Rank-Aware Policy Optimization (RAPO)算法。

Result: 在多个主流基准测试中，TableMind相比竞争基线实现了卓越性能，在推理准确性和计算精度方面都有显著提升。

Conclusion: TableMind通过自主工具调用、安全代码执行和策略自适应能力，为表格推理任务提供了一个高效且准确的解决方案，在复杂数值计算场景中表现出色。

Abstract: Table reasoning is crucial for leveraging structured data in domains such as
finance, healthcare, and scientific research. While large language models
(LLMs) show promise in multi-step reasoning, purely text-based methods often
struggle with the complex numerical computations and fine-grained operations
inherently required in this task. Tool-integrated reasoning improves
computational accuracy via explicit code execution, yet existing systems
frequently rely on rigid patterns, supervised imitation, and lack true
autonomous adaptability. In this paper, we present TableMind, an LLM-driven
table reasoning agent that (i) autonomously performs multi-turn tool
invocation, (ii) writes and executes data-analyzing code in a secure sandbox
environment for data analysis and precise numerical reasoning, and (iii)
exhibits high-level capabilities such as planning and self-reflection to adapt
strategies. To realize these capabilities, we adopt a two-stage fine-tuning
paradigm built on top of a powerful pre-trained language model: supervised
fine-tuning on high-quality reasoning trajectories to establish effective tool
usage patterns, followed by reinforcement fine-tuning to optimize
multi-objective strategies. In particular, we propose Rank-Aware Policy
Optimization (RAPO), which increases the update weight of high-quality
trajectories when their output probabilities are lower than those of
low-quality ones, thereby guiding the model more consistently toward better and
more accurate answers. Extensive experiments on several mainstream benchmarks
demonstrate that TableMind achieves superior performance compared to
competitive baselines, yielding substantial gains in both reasoning accuracy
and computational precision.

</details>


### [212] [SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents](https://arxiv.org/abs/2509.06283)
*Xuan-Phi Nguyen,Shrey Pandit,Revanth Gangi Reddy,Austin Xu,Silvio Savarese,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 通过续代强化学习技术，基于综合数据训练自主单代理模型SFR-DR-20B，在Humanity's Last Exam测试中达到28.7%的性能水平


<details>
  <summary>Details</summary>
Motivation: 开发能够进行深度研究（Deep Research）的自主单代理模型，充分利用大语言模型的复杂推理和工具使用能力，避免多代理系统的静态流程限制

Method: 采用续代强化学习（RL）技术，使用全部综合数据训练推理优化模型，集成最小化网络爬取和Python工具

Result: 最佳模型SFR-DR-20B在Humanity's Last Exam指标上达到28.7%的性能，显著提升了自主深度研究能力

Conclusion: 通过续代RL技术和综合数据训练，成功开发出了能够动态决策、自主执行深度研究任务的单代理模型，为代理智能领域提供了新的技术路径

Abstract: Equipping large language models (LLMs) with complex, interleaved reasoning
and tool-use capabilities has become a key focus in agentic AI research,
especially with recent advances in reasoning-oriented (``thinking'') models.
Such capabilities are key to unlocking a number of important applications. One
such application is Deep Research (DR), which requires extensive search and
reasoning over many sources. Our work in this paper focuses on the development
of native Autonomous Single-Agent models for DR featuring minimal web crawling
and Python tool integration. Unlike multi-agent systems, where agents take up
pre-defined roles and are told what to do at each step in a static workflow, an
autonomous single-agent determines its next action dynamically based on
context, without manual directive. While prior work has proposed training
recipes for base or instruction-tuned LLMs, we focus on continual reinforcement
learning (RL) of reasoning-optimized models to further enhance agentic skills
while preserving reasoning ability. Towards this end, we propose a simple RL
recipe with entirely synthetic data, which we apply to various open-source
LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam
benchmark. In addition, we conduct key analysis experiments to provide more
insights into our methodologies.

</details>


### [213] [From Implicit Exploration to Structured Reasoning: Leveraging Guideline and Refinement for LLMs](https://arxiv.org/abs/2509.06284)
*Jiaxiang Chen,Zhuo Wang,Mingxi Zou,Zhucong Li,Zhijian Zhou,Song Wang,Zenglin Xu*

Main category: cs.AI

TL;DR: 提出从隐式探索转向结构化推理的框架，通过从成功轨迹提取推理模式和从失败中提取反思信号来制定指导方针，在推理时逐步执行并每步进行精炼纠正错误，在多个推理基准上超越强基线


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理方法依赖隐式探索，导致推理路径不稳定、缺乏错误纠正和从经验中学习的能力有限

Method: 从成功轨迹提取结构化推理模式，从失败中提取反思信号形成指导方针；推理时逐步执行指导方针，每步后进行精炼以纠正错误和稳定推理过程

Result: 在BBH、GSM8K、MATH-500、MBPP、HumanEval等多个基准测试中一致超越强基线，结构化推理提高了稳定性和泛化能力

Conclusion: 结构化推理通过逐步执行和精炼改善了推理稳定性，指导方针在领域间迁移良好并支持跨模型协作，在效果和可扩展性上匹配或超越监督微调

Abstract: Large language models (LLMs) have advanced general-purpose reasoning, showing
strong performance across diverse tasks. However, existing methods often rely
on implicit exploration, where the model follows stochastic and unguided
reasoning paths-like walking without a map. This leads to unstable reasoning
paths, lack of error correction, and limited learning from past experience. To
address these issues, we propose a framework that shifts from implicit
exploration to structured reasoning through guideline and refinement. First, we
extract structured reasoning patterns from successful trajectories and
reflective signals from failures. During inference, the model follows these
guidelines step-by-step, with refinement applied after each step to correct
errors and stabilize the reasoning process. Experiments on BBH and four
additional benchmarks (GSM8K, MATH-500, MBPP, HumanEval) show that our method
consistently outperforms strong baselines across diverse reasoning tasks.
Structured reasoning with stepwise execution and refinement improves stability
and generalization, while guidelines transfer well across domains and flexibly
support cross-model collaboration, matching or surpassing supervised
fine-tuning in effectiveness and scalability.

</details>


### [214] [Can AI Make Energy Retrofit Decisions? An Evaluation of Large Language Models](https://arxiv.org/abs/2509.06307)
*Lei Shu,Dong Zhao*

Main category: cs.AI

TL;DR: 评估7种大型语言模型在住宅能源改造决策中的表现，发现LLM在技术目标上表现更好，但在准确性、一致性和上下文处理方面仍需改进


<details>
  <summary>Details</summary>
Motivation: 传统能源改造决策方法通用性有限且可解释性低，阻碍了在多样化住宅环境中的采用。随着智能社区和生成式AI的发展，LLM可能通过处理上下文信息来提供可读建议

Method: 评估7种LLM（ChatGPT、DeepSeek、Gemini、Grok、Llama和Claude）在两种目标下的住宅改造决策：最大化CO2减排（技术目标）和最小化投资回收期（社会技术目标）。使用包含49个州400个住宅的数据集，从准确性、一致性、敏感性和推理四个维度评估性能

Result: LLM在许多情况下能生成有效建议，未经微调即可达到54.5%的top 1匹配率和92.8%的top 5匹配率。技术目标表现更强，社会技术决策受经济权衡和本地背景限制。模型间一致性低，高性能模型往往与其他模型不同。对位置和建筑几何敏感，但对技术和居住者行为不太敏感

Conclusion: LLM在能源改造决策中是有前途的助手，但需要在准确性、一致性和上下文处理方面进行改进才能可靠应用

Abstract: Conventional approaches to building energy retrofit decision making suffer
from limited generalizability and low interpretability, hindering adoption in
diverse residential contexts. With the growth of Smart and Connected
Communities, generative AI, especially large language models (LLMs), may help
by processing contextual information and producing practitioner readable
recommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok,
Llama, and Claude) on residential retrofit decisions under two objectives:
maximizing CO2 reduction (technical) and minimizing payback period
(sociotechnical). Performance is assessed on four dimensions: accuracy,
consistency, sensitivity, and reasoning, using a dataset of 400 homes across 49
US states. LLMs generate effective recommendations in many cases, reaching up
to 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning.
Performance is stronger for the technical objective, while sociotechnical
decisions are limited by economic trade offs and local context. Agreement
across models is low, and higher performing models tend to diverge from others.
LLMs are sensitive to location and building geometry but less sensitive to
technology and occupant behavior. Most models show step by step, engineering
style reasoning, but it is often simplified and lacks deeper contextual
awareness. Overall, LLMs are promising assistants for energy retrofit decision
making, but improvements in accuracy, consistency, and context handling are
needed for reliable practice.

</details>


### [215] [Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation](https://arxiv.org/abs/2509.06337)
*Jianpeng Zhao,Chenyu Yuan,Weiming Luo,Haoling Xie,Guangwei Zhang,Steven Jige Quan,Zixuan Yuan,Pengyang Wang,Denghui Zhang*

Main category: cs.AI

TL;DR: 使用大语言模型模拟虚拟调查受访者，提出PAS和FAS两种模拟设置，构建LLM-S^3基准测试套件，评估主流LLM在生成准确人口统计响应方面的能力。


<details>
  <summary>Details</summary>
Motivation: 传统问卷调查方法成本高、耗时长且规模有限，需要探索利用LLM进行虚拟受访者模拟的新范式，为社会学研究和政策评估提供可扩展、成本效益高的工具。

Method: 提出两种模拟设置：PAS（基于部分受访者档案预测缺失属性）和FAS（在零上下文和上下文增强条件下生成完整合成数据集），构建包含11个真实世界公共数据集的LLM-S^3基准套件，评估GPT-3.5/4 Turbo、LLaMA 3.0/3.1-8B等主流模型。

Result: 研究揭示了预测性能的一致趋势，识别了失败模式，并证明了上下文和提示设计对模拟保真度的影响。

Conclusion: 这项工作为LLM驱动的调查模拟建立了严格基础，为社会学研究和政策评估提供了可扩展且成本效益高的工具。

Abstract: Questionnaire-based surveys are foundational to social science research and
public policymaking, yet traditional survey methods remain costly,
time-consuming, and often limited in scale. This paper explores a new paradigm:
simulating virtual survey respondents using Large Language Models (LLMs). We
introduce two novel simulation settings, namely Partial Attribute Simulation
(PAS) and Full Attribute Simulation (FAS), to systematically evaluate the
ability of LLMs to generate accurate and demographically coherent responses. In
PAS, the model predicts missing attributes based on partial respondent
profiles, whereas FAS involves generating complete synthetic datasets under
both zero-context and context-enhanced conditions. We curate a comprehensive
benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey
Simulation), that spans 11 real-world public datasets across four sociological
domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA
3.0/3.1-8B) reveals consistent trends in prediction performance, highlights
failure modes, and demonstrates how context and prompt design impact simulation
fidelity. This work establishes a rigorous foundation for LLM-driven survey
simulations, offering scalable and cost-effective tools for sociological
research and policy evaluation. Our code and dataset are available at:
https://github.com/dart-lab-research/LLM-S-Cube-Benchmark

</details>


### [216] [Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent](https://arxiv.org/abs/2509.06341)
*Issue Yishu Wang,Kakam Chong,Xiaofeng Wang,Xu Yan,DeXin Kong,Chen Ju,Ming Chen,Shuai Xiao,Shuguang Han,jufeng chen*

Main category: cs.AI

TL;DR: 这篇论文提出了一个用于评估二手市场中卖家语音助手语言模型讨价能力的多轮对话框架，重点考察模型跟踪和理解买家意图的能力。


<details>
  <summary>Details</summary>
Motivation: 在线二手市场中，多轮讨价是买卖双方交互的关键环节。语音助手需要在商业约束下为卖家进行协商，而准确跟踪和解释买家意图的能力直接影响讨价效果。

Method: 研究提出了一个多轮评估框架：(1)大规模电子商务讨价标准数据集，涵盖622个类别、9,892个产品和3,014个任务；(2)基于心理理论(ToM)的轮次评估框架，进行注释买家意图分析；(3)自动化流水线从大量对话数据中提取可靠意图信息。

Result: 该框架能够测试语音助手是否能够提取和跟踪买家意图，超越了仅仅考虑结果的评估指标。

Conclusion: 研究提供了一个维度丰富的评估框架，能够更全面地评估语音助手在多轮讨价对话中的意图跟踪能力，为二手市场语音助手的发展提供了重要的评测工具。

Abstract: In online second-hand marketplaces, multi-turn bargaining is a crucial part
of seller-buyer interactions. Large Language Models (LLMs) can act as seller
agents, negotiating with buyers on behalf of sellers under given business
constraints. A critical ability for such agents is to track and accurately
interpret cumulative buyer intents across long negotiations, which directly
impacts bargaining effectiveness. We introduce a multi-turn evaluation
framework for measuring the bargaining ability of seller agents in e-commerce
dialogues. The framework tests whether an agent can extract and track buyer
intents. Our contributions are: (1) a large-scale e-commerce bargaining
benchmark spanning 622 categories, 9,892 products, and 3,014 tasks; (2) a
turn-level evaluation framework grounded in Theory of Mind (ToM) with annotated
buyer intents, moving beyond outcome-only metrics; and (3) an automated
pipeline that extracts reliable intent from massive dialogue data.

</details>


### [217] [A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research](https://arxiv.org/abs/2509.06355)
*Yunzhe Wang,Volkan Ustun,Chris McGroarty*

Main category: cs.AI

TL;DR: DECOY是一个新颖的多智能体模拟器，将3D地形中的战略长期规划抽象为高级离散化模拟，同时保持低层次环境保真度。使用CS:GO作为测试平台，仅通过移动决策模拟游戏玩法，无需显式建模瞄准和射击等低层次机制。


<details>
  <summary>Details</summary>
Motivation: 现代复杂多智能体交互模拟环境需要在高度保真细节和计算效率之间取得平衡，需要一种既能保持环境真实性又能高效模拟战略决策的方法。

Method: 采用路径点系统简化和离散化连续状态和动作，配合基于真实CS:GO比赛数据训练的神经预测和生成模型来重建事件结果。

Result: 广泛评估表明，从人类数据生成的DECOY回放与原始游戏中观察到的回放高度匹配。

Conclusion: DECOY提供了一个有价值的工具，可用于推进战略多智能体规划和行为生成的研究，且模拟环境已公开可用。

Abstract: Modern simulation environments for complex multi-agent interactions must
balance high-fidelity detail with computational efficiency. We present DECOY, a
novel multi-agent simulator that abstracts strategic, long-horizon planning in
3D terrains into high-level discretized simulation while preserving low-level
environmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a
testbed, our framework accurately simulates gameplay using only movement
decisions as tactical positioning -- without explicitly modeling low-level
mechanics such as aiming and shooting. Central to our approach is a waypoint
system that simplifies and discretizes continuous states and actions, paired
with neural predictive and generative models trained on real CS:GO tournament
data to reconstruct event outcomes. Extensive evaluations show that replays
generated from human data in DECOY closely match those observed in the original
game. Our publicly available simulation environment provides a valuable tool
for advancing research in strategic multi-agent planning and behavior
generation.

</details>


### [218] [Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning](https://arxiv.org/abs/2509.06409)
*Yihong Luo,Wenwu He,Zhuo-Xu Cui,Dong Liang*

Main category: cs.AI

TL;DR: DiagCoT是一个多阶段框架，通过监督微调通用视觉语言模型，仅使用自由文本报告模拟放射科医生的逐步诊断推理，显著提升了疾病分类、病理定位和报告生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在医疗诊断中缺乏放射科医生的逐步推理能力，需要一种能够从非结构化临床报告中学习结构化推理的方法。

Method: 采用对比图像-报告调优进行领域对齐，使用思维链监督捕获推理逻辑，并通过临床奖励信号的强化调优提高事实准确性和流畅性。

Result: 在MIMIC-CXR基准测试中，零样本疾病分类AUC从0.52提升到0.76，病理定位mIoU从0.08提升到0.31，报告生成BLEU从0.11提升到0.33，在长尾疾病和外部数据集上优于最先进模型。

Conclusion: DiagCoT通过将非结构化临床叙述转化为结构化监督，为开发可解释且具备诊断能力的放射学AI系统提供了可扩展的方法。

Abstract: This study presents DiagCoT, a multi-stage framework that applies supervised
fine-tuning to general-purpose vision-language models (VLMs) to emulate
radiologists' stepwise diagnostic reasoning using only free-text reports.
DiagCoT combines contrastive image-report tuning for domain alignment,
chain-of-thought supervision to capture inferential logic, and reinforcement
tuning with clinical reward signals to enhance factual accuracy and fluency. On
the MIMIC-CXR benchmark, DiagCoT improved zero-shot disease classification AUC
from 0.52 to 0.76 (absolute gain of 0.24), pathology grounding mIoU from 0.08
to 0.31 (absolute gain of 0.23), and report generation BLEU from 0.11 to 0.33
(absolute gain of 0.22). It outperformed state-of-the-art models including
LLaVA-Med and CXR-LLAVA on long-tailed diseases and external datasets. By
converting unstructured clinical narratives into structured supervision,
DiagCoT offers a scalable approach for developing interpretable and
diagnostically competent AI systems for radiology.

</details>


### [219] [Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning](https://arxiv.org/abs/2509.06436)
*Song Yu,Xiaofei Xu,Ke Deng,Li Li,Lin Tian*

Main category: cs.AI

TL;DR: 提出了Tree of Agents (TOA)多智能体框架，通过将长输入分段处理、动态信息交换和树状结构协作，有效解决LLMs在长上下文任务中的'中间信息丢失'问题，在保持API开销相当的情况下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型处理长上下文任务时的'中间信息丢失'问题，现有方法要么可能丢弃关键信息，要么导致注意力分散，需要一种既能保持关键信息又能有效处理长上下文的方法。

Method: TOA多智能体推理框架：将输入分段由独立智能体处理，每个智能体生成局部认知，然后沿树状结构路径动态交换信息进行协作推理，结合前缀哈希缓存和自适应剪枝策略提高效率。

Result: 实验表明，基于紧凑LLaMA3.1-8B的TOA显著优于多个基线模型，在多种长上下文任务上达到与最新大型商业模型（如Gemini1.5-pro）相当的性能。

Conclusion: TOA框架通过多智能体协作有效缓解位置偏见和减少幻觉，为长上下文处理提供了一种高效且性能优异的解决方案。

Abstract: Large language models (LLMs) face persistent challenges when handling
long-context tasks, most notably the lost in the middle issue, where
information located in the middle of a long input tends to be underutilized.
Some existing methods that reduce input have the risk of discarding key
information, while others that extend context windows often lead to attention
dispersion. To address these limitations, we propose Tree of Agents (TOA), a
multi-agent reasoning framework that segments the input into chunks processed
by independent agents. Each agent generates its local cognition, then agents
dynamically exchange information for collaborative reasoning along
tree-structured paths. TOA enables agents to probe different reasoning orders
for multi-perspective understanding, effectively mitigating position bias and
reducing hallucinations. To improve processing efficiency, we incorporate
prefix-hash caching and adaptive pruning strategies, achieving significant
performance improvements with comparable API overhead. Experiments show that
TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple
baselines and demonstrates comparable performance to the latest and much larger
commercial models, such as Gemini1.5-pro, on various long-context tasks. Code
is available at https://github.com/Aireduce952/Tree-of-Agents.

</details>


### [220] [HyFedRAG: A Federated Retrieval-Augmented Generation Framework for Heterogeneous and Privacy-Sensitive Data](https://arxiv.org/abs/2509.06444)
*Cheng Qian,Hainan Zhang,Yongxin Tong,Hong-Wei Zheng,Zhiming Zheng*

Main category: cs.AI

TL;DR: HyFedRAG是一个联邦RAG框架，针对医疗领域的异构隐私数据，通过边缘-云协作机制支持SQL、知识图谱和文档等多种数据格式的隐私保护检索与生成。


<details>
  <summary>Details</summary>
Motivation: 集中式RAG系统在处理异构隐私敏感数据（特别是分布式医疗环境中的患者数据）时面临困难，传统云基RAG系统无法有效处理多样化格式和边缘设备的数据隐私问题。

Method: 基于Flower设计边缘-云协作RAG框架，边缘侧LLM将异构数据转换为标准化隐私保护表示，服务器侧LLM进行全局推理；集成轻量级本地检索器和隐私感知LLM，提供三种匿名化工具；设计三级缓存策略优化延迟。

Result: 在PMC-Patients数据集上的实验表明，HyFedRAG在检索质量、生成一致性和系统效率方面优于现有基线方法。

Conclusion: 该框架为结构化异构数据提供了可扩展且符合隐私要求的RAG解决方案，释放了LLM在敏感多样化数据环境中的潜力。

Abstract: Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive
data, especially in distributed healthcare settings where patient data spans
SQL, knowledge graphs, and clinical notes. Clinicians face difficulties
retrieving rare disease cases due to privacy constraints and the limitations of
traditional cloud-based RAG systems in handling diverse formats and edge
devices. To address this, we introduce HyFedRAG, a unified and efficient
Federated RAG framework tailored for Hybrid data modalities. By leveraging an
edge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across
diverse data sources while preserving data privacy. Our key contributions are:
(1) We design an edge-cloud collaborative RAG framework built on Flower, which
supports querying structured SQL data, semi-structured knowledge graphs, and
unstructured documents. The edge-side LLMs convert diverse data into
standardized privacy-preserving representations, and the server-side LLMs
integrates them for global reasoning and generation. (2) We integrate
lightweight local retrievers with privacy-aware LLMs and provide three
anonymization tools that enable each client to produce semantically rich,
de-identified summaries for global inference across devices. (3) To optimize
response latency and reduce redundant computation, we design a three-tier
caching strategy consisting of local cache, intermediate representation cache,
and cloud inference cache. Experimental results on PMC-Patients demonstrate
that HyFedRAG outperforms existing baselines in terms of retrieval quality,
generation consistency, and system efficiency. Our framework offers a scalable
and privacy-compliant solution for RAG over structural-heterogeneous data,
unlocking the potential of LLMs in sensitive and diverse data environments.

</details>


### [221] [Accelerate Scaling of LLM Alignment via Quantifying the Coverage and Depth of Instruction Set](https://arxiv.org/abs/2509.06463)
*Chengwei Wu,Li Du,Hanyu Zhao,Yiming Ju,Jiapu Wang,Tengfei Pan*

Main category: cs.AI

TL;DR: 提出了一种新的指令数据选择方法，通过最大化指令深度和语义覆盖率来持续提升大语言模型在下游任务中的对齐性能


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在下游任务中的应用需求增长，提升模型对齐性能和效率变得至关重要。当前指令集优化方法在指令池不断扩大时无法持续提升性能，需要探究影响对齐模型性能的关键因素

Method: 首先研究指令数据集分布与对齐模型性能关系的关键因素，发现指令深度和语义空间覆盖率是决定下游性能的关键因素。然后设计指令选择算法同时最大化所选指令的深度和语义覆盖率

Result: 该方法可以解释开发集上超过70%的模型损失，相比最先进的基线方法能够以更快的速度持续提升模型性能，实现"加速扩展"

Conclusion: 指令深度和语义覆盖率是影响对齐模型性能的关键因素，提出的指令选择方法能够有效提升大语言模型在下游任务中的对齐效率和性能

Abstract: With the growing demand for applying large language models to downstream
tasks, improving model alignment performance and efficiency has become crucial.
Such a process involves selecting informative instructions from a candidate
pool. However, due to the complexity of instruction set distributions, the key
factors driving the performance of aligned models remain unclear. As a result,
current instruction set refinement methods fail to improve performance as the
instruction pool expands continuously. To address this issue, we first
investigate the key factors that influence the relationship between instruction
dataset distribution and aligned model performance. Based on these insights, we
propose a novel instruction data selection method. We identify that the depth
of instructions and the coverage of the semantic space are the crucial factors
determining downstream performance, which could explain over 70\% of the model
loss on the development set. We then design an instruction selection algorithm
to simultaneously maximize the depth and semantic coverage of the selected
instructions. Experimental results demonstrate that, compared to
state-of-the-art baseline methods, it can sustainably improve model performance
at a faster pace and thus achieve \emph{``Accelerated Scaling''}.

</details>


### [222] [MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents](https://arxiv.org/abs/2509.06477)
*Pengxiang Zhao,Guangyi Liu,Yaozhen Liang,Weiqing He,Zhengxi Lu,Yuehao Huang,Yaxuan Guo,Kexin Zhang,Hao Wang,Liang Liu,Yong Liu*

Main category: cs.AI

TL;DR: MAS-Bench是首个专门评估GUI-快捷方式混合智能体的基准测试，专注于移动领域，包含139个复杂任务和88个预定义快捷方式，通过7个指标评估智能体自主生成快捷方式的能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估GUI操作与快捷方式（API、深度链接等）混合智能体的基准测试框架，需要填补这一空白以推动更高效GUI智能体的发展。

Method: 构建包含139个复杂任务、11个真实应用、88个预定义快捷方式知识库的基准测试平台，设计可通过GUI操作解决但能通过智能嵌入快捷方式显著加速的任务，使用7个评估指标来衡量智能体性能。

Result: 实验显示混合智能体相比纯GUI操作智能体在成功率和效率方面都有显著提升，证明了该方法在评估智能体快捷方式生成能力方面的有效性。

Conclusion: MAS-Bench填补了关键评估空白，为未来开发更高效、更鲁棒的智能体提供了基础平台，推动了GUI-快捷方式混合范式的发展。

Abstract: To enhance the efficiency of GUI agents on various platforms like smartphones
and computers, a hybrid paradigm that combines flexible GUI operations with
efficient shortcuts (e.g., API, deep links) is emerging as a promising
direction. However, a framework for systematically benchmarking these hybrid
agents is still underexplored. To take the first step in bridging this gap, we
introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut
hybrid agents with a specific focus on the mobile domain. Beyond merely using
predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously
generate shortcuts by discovering and creating reusable, low-cost workflows. It
features 139 complex tasks across 11 real-world applications, a knowledge base
of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation
metrics. The tasks are designed to be solvable via GUI-only operations, but can
be significantly accelerated by intelligently embedding shortcuts. Experiments
show that hybrid agents achieve significantly higher success rates and
efficiency than their GUI-only counterparts. This result also demonstrates the
effectiveness of our method for evaluating an agent's shortcut generation
capabilities. MAS-Bench fills a critical evaluation gap, providing a
foundational platform for future advancements in creating more efficient and
robust intelligent agents.

</details>


### [223] [MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization](https://arxiv.org/abs/2509.06490)
*Niki Kotecha,Ehecatl Antonio del Rio Chanona*

Main category: cs.AI

TL;DR: 采用强化学习与多目标进化算法相结合的方法，通过搜索策略神经网结构的参数空间生成帕索托前沿，并结合条件风险价值来提高供应链动态多目标优化的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 供应链管理中需要平衡多个冲突目标（成本、服务水平、环境可持续性），但传统多目标优化方法难以实时适应动态环境。

Method: 结合强化学习（RL）和多目标进化算法（MOEAs），利用MOEAs搜索策略神经网结构的参数空间生成帕索托前沿，并引入条件风险价值（CVaR）支持风险敏感决策。

Result: 通过案例研究验证了方法的有效性，在库存管理案例中超过了现有最优方法，能够响应供应链动态变化。

Conclusion: 该策略不仅提高了决策效率，还为管理不确定性和优化供应链性能提供了更鲁棒的框架。

Abstract: In supply chain management, decision-making often involves balancing multiple
conflicting objectives, such as cost reduction, service level improvement, and
environmental sustainability. Traditional multi-objective optimization methods,
such as linear programming and evolutionary algorithms, struggle to adapt in
real-time to the dynamic nature of supply chains. In this paper, we propose an
approach that combines Reinforcement Learning (RL) and Multi-Objective
Evolutionary Algorithms (MOEAs) to address these challenges for dynamic
multi-objective optimization under uncertainty. Our method leverages MOEAs to
search the parameter space of policy neural networks, generating a Pareto front
of policies. This provides decision-makers with a diverse population of
policies that can be dynamically switched based on the current system
objectives, ensuring flexibility and adaptability in real-time decision-making.
We also introduce Conditional Value-at-Risk (CVaR) to incorporate
risk-sensitive decision-making, enhancing resilience in uncertain environments.
We demonstrate the effectiveness of our approach through case studies,
showcasing its ability to respond to supply chain dynamics and outperforming
state-of-the-art methods in an inventory management case study. The proposed
strategy not only improves decision-making efficiency but also offers a more
robust framework for managing uncertainty and optimizing performance in supply
chains.

</details>


### [224] [Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers](https://arxiv.org/abs/2509.06493)
*Ran Xin,Zeyu Zheng,Yanchen Nie,Kun Yuan,Xia Xiao*

Main category: cs.AI

TL;DR: BFS-Prover-V2是一个针对LLM自动定理证明的双重扩展系统，通过多轮离线RL训练框架和规划增强的多智能体搜索架构，在MiniF2F和ProofNet基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在自动定理证明中训练时强化学习和推理时计算扩展的双重挑战，突破性能瓶颈。

Method: 1) 多轮离线RL框架，采用AlphaZero启发的专家迭代流程，包含自适应策略级数据过滤和定期重训练；2) 规划增强的多智能体搜索架构，使用通用推理模型作为高层规划器分解复杂定理，通过共享证明缓存实现并行证明智能体协作。

Result: 在MiniF2F测试集达到95.08%，在ProofNet测试集达到41.4%的state-of-the-art性能。

Conclusion: 该系统在形式数学领域展示了卓越性能，其RL和推理技术具有广泛适用性，可应用于其他需要长视野多轮推理和复杂搜索的领域。

Abstract: The integration of Large Language Models (LLMs) into automated theorem
proving has shown immense promise, yet is fundamentally constrained by
challenges in scaling up both training-time reinforcement learning (RL) and
inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system
designed to address this dual scaling problem. We present two primary
innovations. The first is a novel multi-turn off-policy RL framework for
continually improving the performance of LLM step-prover at training time. This
framework, inspired by the principles of AlphaZero, utilizes a multi-stage
expert iteration pipeline featuring adaptive tactic-level data filtering and
periodic retraining to surmount the performance plateaus that typically curtail
long-term RL in LLM-based agents. The second innovation is a planner-enhanced
multi-agent search architecture that scales reasoning capabilities at inference
time. This architecture employs a general reasoning model as a high-level
planner to iteratively decompose complex theorems into a sequence of simpler
subgoals. This hierarchical approach substantially reduces the search space,
enabling a team of parallel prover agents to collaborate efficiently by
leveraging a shared proof cache. We demonstrate that this dual approach to
scaling yields state-of-the-art results on established formal mathematics
benchmarks. \texttt{BFS-Prover-V2} achieves 95.08\% and 41.4\% on the MiniF2F
and ProofNet test sets respectively. While demonstrated in the domain of formal
mathematics, the RL and inference techniques presented in this work are of
broader interest and may be applied to other domains requiring long-horizon
multi-turn reasoning and complex search.

</details>


### [225] [An AI system to help scientists write expert-level empirical software](https://arxiv.org/abs/2509.06503)
*Eser Aygün,Anastasiya Belyaeva,Gheorghe Comanici,Marc Coram,Hao Cui,Jake Garrison,Renee Johnston Anton Kast,Cory Y. McLean,Peter Norgaard,Zahra Shamsi,David Smalling,James Thompson,Subhashini Venugopalan,Brian P. Williams,Chujun He,Sarah Martinson,Martyna Plomecka,Lai Wei,Yuchen Zhou,Qian-Ze Zhu,Matthew Abraham,Erica Brand,Anna Bulanova,Jeffrey A. Cardille,Chris Co,Scott Ellsworth,Grace Joseph,Malcolm Kane,Ryan Krueger,Johan Kartiwa,Dan Liebling,Jan-Matthis Lueckmann,Paul Raccuglia,Xuefei,Wang,Katherine Chou,James Manyika,Yossi Matias,John C. Platt,Lizzie Dorfman,Shibl Mourad,Michael P. Brenner*

Main category: cs.AI

TL;DR: AI系统使用大语言模型和树搜索自动生成专家级科学软件，在多个领域超越人类开发的方法


<details>
  <summary>Details</summary>
Motivation: 解决科学发现过程中手动创建计算实验软件的瓶颈问题，加速科学进步

Method: 结合大语言模型(LLM)和树搜索(TS)技术，系统性地改进质量指标并智能探索解决方案空间

Result: 在生物信息学中发现40种优于人类方法的新方法，流行病学中生成14个超越CDC模型的预测模型，在多个领域产生最先进软件

Conclusion: 该系统通过为不同任务设计新颖解决方案，代表了加速科学进步的重要一步

Abstract: The cycle of scientific discovery is frequently bottlenecked by the slow,
manual creation of software to support computational experiments. To address
this, we present an AI system that creates expert-level scientific software
whose goal is to maximize a quality metric. The system uses a Large Language
Model (LLM) and Tree Search (TS) to systematically improve the quality metric
and intelligently navigate the large space of possible solutions. The system
achieves expert-level results when it explores and integrates complex research
ideas from external sources. The effectiveness of tree search is demonstrated
across a wide range of benchmarks. In bioinformatics, it discovered 40 novel
methods for single-cell data analysis that outperformed the top human-developed
methods on a public leaderboard. In epidemiology, it generated 14 models that
outperformed the CDC ensemble and all other individual models for forecasting
COVID-19 hospitalizations. Our method also produced state-of-the-art software
for geospatial analysis, neural activity prediction in zebrafish, time series
forecasting and numerical solution of integrals. By devising and implementing
novel solutions to diverse tasks, the system represents a significant step
towards accelerating scientific progress.

</details>


### [226] [CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning](https://arxiv.org/abs/2509.06641)
*Zhou-Peng Shou,Zhi-Qiang You,Fang Wang,Hai-Bo Liu*

Main category: cs.AI

TL;DR: 这篇论文提出了一种零检测多模态推理组件，通过人类认知策略指导，解决多模态大模型复杂推理中的"短接"问题和上下文理解不足问题。


<details>
  <summary>Details</summary>
Motivation: 目标是解决多模态大模型在复杂跨模态推理中存在的"短接"问题（只利用表面特征而非深层逻辑）和上下文理解不充分的问题。

Method: 提出了一个插拔即用的三模块流水线：意图感知器(Intent Perceiver)、策略生成器(Strategy Generator)和策略选择器(Strategy Selector)，构建了"理解-规划-选择"的明确认知过程。通过生成和筛选"意图简笼"策略来指导最终推理，无需参数微调。

Result: 在IntentBench、WorldSense和Daily-Omni数据集上验证了方法的普适性和稳健收益。与各自基线相比，完整的"三模块"方案在不同推理引擎和流水线组合中都实现了一致收益，最高提升约9.51个百分点。

Conclusion: 信息论分析显示该过程能够降低条件熵和提高信息利用效率，从而压制意外的短接推理。"意图简笼"推理组件在零检测场景中具有实际价值和可移植性。

Abstract: Targeting the issues of "shortcuts" and insufficient contextual understanding
in complex cross-modal reasoning of multimodal large models, this paper
proposes a zero-shot multimodal reasoning component guided by human-like
cognitive strategies centered on an "intent sketch". The component comprises a
plug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and
Strategy Selector-that explicitly constructs a "understand-plan-select"
cognitive process. By generating and filtering "intent sketch" strategies to
guide the final reasoning, it requires no parameter fine-tuning and achieves
cross-model transfer solely through in-context engineering.
Information-theoretic analysis shows that this process can reduce conditional
entropy and improve information utilization efficiency, thereby suppressing
unintended shortcut reasoning. Experiments on IntentBench, WorldSense, and
Daily-Omni validate the method's generality and robust gains; compared with
their respective baselines, the complete "three-module" scheme yields
consistent improvements across different reasoning engines and pipeline
combinations, with gains up to approximately 9.51 percentage points,
demonstrating the practical value and portability of the "intent sketch"
reasoning component in zero-shot scenarios.

</details>


### [227] [Reinforcement Learning Foundations for Deep Research Systems: A Survey](https://arxiv.org/abs/2509.06733)
*Wenjun Li,Zhi Chen,Jingru Lin,Hannan Cao,Wei Han,Sheng Liang,Zhi Zhang,Kuicai Dong,Dexun Li,Chen Zhang,Yong Liu*

Main category: cs.AI

TL;DR: 本论文是首个专注于深度研究系统强化学习基础的综述，系统化了DeepSeek-R1之后的相关研究工作，包括数据合成、RL方法、训练系统等多个轴心领域，为培养健壮透明的深度研究代理提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究系统存在以下问题：SFT方法存在模仿偏差和暴露偏差，并且不充分利用环境反馈；DPO等偏好对齐方法依赖于代理指标、离策策略、弱于长期赋值和多目标优化；以及依赖于人工定义的决策点和子技能。强化学习能够更好地解决这些问题。

Method: 本综述从三个轴心系统化相关研究：(i)数据合成与管理；(ii)代理研究的RL方法，包括稳定性、样本效率、长上下文处理、奖励设计、赋值分配、多目标优化和多模态整合；(iii)代理RL训练系统和框架。同时涵盖代理架构、协调机制以及评估标准。

Result: 本综述提炼了重复出现的模式，持之以普通的实践指南，以支持培养健壮、透明的深度研究代理。

Conclusion: 强化学习在深度研究系统中具有重要价值，能够在闭环工具交互中优化轨迹策略，支持探索和恢复行为，并减少对人工偏好的依赖。本论文为这一领域的研究提供了系统化的基础和实用指南。

Abstract: Deep research systems, agentic AI that solve complex, multi-step tasks by
coordinating reasoning, search across the open web and user files, and tool
use, are moving toward hierarchical deployments with a Planner, Coordinator,
and Executors. In practice, training entire stacks end-to-end remains
impractical, so most work trains a single planner connected to core tools such
as search, browsing, and code. While SFT imparts protocol fidelity, it suffers
from imitation and exposure biases and underuses environment feedback.
Preference alignment methods such as DPO are schema and proxy-dependent,
off-policy, and weak for long-horizon credit assignment and multi-objective
trade-offs. A further limitation of SFT and DPO is their reliance on human
defined decision points and subskills through schema design and labeled
comparisons. Reinforcement learning aligns with closed-loop, tool-interaction
research by optimizing trajectory-level policies, enabling exploration,
recovery behaviors, and principled credit assignment, and it reduces dependence
on such human priors and rater biases.
  This survey is, to our knowledge, the first dedicated to the RL foundations
of deep research systems. It systematizes work after DeepSeek-R1 along three
axes: (i) data synthesis and curation; (ii) RL methods for agentic research
covering stability, sample efficiency, long context handling, reward and credit
design, multi-objective optimization, and multimodal integration; and (iii)
agentic RL training systems and frameworks. We also cover agent architecture
and coordination, as well as evaluation and benchmarks, including recent QA,
VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We
distill recurring patterns, surface infrastructure bottlenecks, and offer
practical guidance for training robust, transparent deep research agents with
RL.

</details>


### [228] [VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction](https://arxiv.org/abs/2509.06736)
*Jie Yang,Jiajun Chen,Zhangyue Yin,Shuo Chen,Yuxin Wang,Yiran Guo,Yuan Li,Yining Zheng,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的State-based Function Call (SFC)方法，通过维护显式系统状态意识和直接状态迁移，在车辆室内环境控制任务中显著超过传统Function Calling方法的性能。


<details>
  <summary>Details</summary>
Motivation: 智能车辆室对API代理提出了独特挑战，需要协调多个紧密耦合的子系统。传统Function Calling方法存在无状态操作、需多次探索调用建立环境意识、效率低下和错误恢复能力有限等问题。

Method: 研究者首先构建了VehicleWorld环境，包含30个模块、250个API和680个属性，提供完整的可执行实现。通过系统分析发现直接状态预测在环境控制中更优，从而提出State-based Function Call (SFC)方法，维护显式系统状态意识并实现直接状态迁移。

Result: 实验结果显示，SFC方法显著超过传统FC方法，实现了更高的执行准确性和更低的延迟。

Conclusion: 论文提出的State-based Function Call方法通过维护显式状态意识和直接状态迁移，有效解决了车辆室环境中API代理的效率和错误恢复问题，为汽车领域的智能控制提供了更有效的解决方案。所有实现代码已开源。

Abstract: Intelligent vehicle cockpits present unique challenges for API Agents,
requiring coordination across tightly-coupled subsystems that exceed typical
task environments' complexity. Traditional Function Calling (FC) approaches
operate statelessly, requiring multiple exploratory calls to build
environmental awareness before execution, leading to inefficiency and limited
error recovery. We introduce VehicleWorld, the first comprehensive environment
for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties
with fully executable implementations that provide real-time state information
during agent execution. This environment enables precise evaluation of vehicle
agent behaviors across diverse, challenging scenarios. Through systematic
analysis, we discovered that direct state prediction outperforms function
calling for environmental control. Building on this insight, we propose
State-based Function Call (SFC), a novel approach that maintains explicit
system state awareness and implements direct state transitions to achieve
target conditions. Experimental results demonstrate that SFC significantly
outperforms traditional FC approaches, achieving superior execution accuracy
and reduced latency. We have made all implementation code publicly available on
Github https://github.com/OpenMOSS/VehicleWorld.

</details>


### [229] [Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting](https://arxiv.org/abs/2509.06770)
*Shashidhar Reddy Javaji,Bhavul Gauri,Zining Zhu*

Main category: cs.AI

TL;DR: 这篇论文提出了一个评估迭代精细化的框架，用于测量大语言模型在多轮工作流中的表现，发现不同领域的迭代效果存在显著差异，并指出了有效的提示策略。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏明确的方法来评估迭代精细化在大语言模型中的效果，不知道什么时候迭代能够提高性能，什么时候会造成负面影响。

Method: 设计了一个评估协议，在思想点子生成、代码编写和数学问题解决三个领域进行控制的12轮对话。使用从模糊反馈到目标导向的多种提示，并记录每轮输出。通过领域适宜的检查来评分结果，并使用三组指标跟踪每轮行为。

Result: 评估结果显示：收益在不同领域存在差异：思想点子和代码在前几轮就有显著收益，而数学需要后期的详细迭代才能获得收益。模糊反馈往往在前几轮后就平台化或造成性能回落，而目标导向的提示能够可靠地改善指定质量指标。

Conclusion: 该框架和指标使得迭代过程可以进行量化测量和模型间比较，并为决定何时导向、停止或更换策略提供了信号。

Abstract: Large language models (LLMs) are now used in multi-turn workflows, but we
still lack a clear way to measure when iteration helps and when it hurts. We
present an evaluation framework for iterative refinement that spans ideation,
code, and math. Our protocol runs controlled 12-turn conversations per task,
utilizing a variety of prompts ranging from vague ``improve it'' feedback to
targeted steering, and logs per-turn outputs. We score outcomes with
domain-appropriate checks (unit tests for code; answer-equivalence plus
reasoning-soundness for math; originality and feasibility for ideation) and
track turn-level behavior with three families of metrics: semantic movement
across turns, turn-to-turn change, and output size growth. Across models and
tasks, gains are domain-dependent: they arrive early in ideas and code, but in
math late turns matter when guided by elaboration. After the first few turns,
vague feedback often plateaus or reverses correctness, while targeted prompts
reliably shift the intended quality axis (novelty vs. feasibility in ideation;
speed vs. readability in code; in math, elaboration outperforms exploration and
drives late-turn gains). We also observe consistent domain patterns: ideation
moves more in meaning across turns, code tends to grow in size with little
semantic change, and math starts fixed but can break that path with late,
elaborative iteration.Together, the framework and metrics make iteration
measurable and comparable across models, and signal when to steer, stop, or
switch strategies.

</details>


### [230] [RAFFLES: Reasoning-based Attribution of Faults for LLM Systems](https://arxiv.org/abs/2509.06822)
*Chenyang Zhu,Spencer Hong,Jingyu Wu,Kushal Chawla,Charlotte Tang,Youbing Yin,Nathan Wolfe,Erin Babinsky,Daben Liu*

Main category: cs.AI

TL;DR: RAFFLES是一个用于评估长时程多组件LLM代理系统的迭代式评估架构，通过推理和迭代精化来识别系统故障点和原因，在故障检测准确率上显著超越现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前的长时程多组件LLM代理系统评估方法存在局限，主要关注单一指标或端到端结果，难以识别系统在复杂长时程推理中的具体故障点和原因。需要能够推理、探测、迭代和理解系统复杂逻辑的评估框架。

Method: RAFFLES采用迭代式多组件流水线架构，包含一个中央Judge系统性地调查故障，以及一组专门Evaluators评估系统组件和Judge自身的推理质量，建立假设历史。

Result: 在Who&When基准测试中，RAFFLES在算法生成数据集上达到43%的代理-步骤故障对准确率（相比之前最佳16.6%大幅提升），在手工制作数据集上达到20%准确率（超越之前最佳8.8%）。

Conclusion: RAFFLES展示了为自主系统引入自动化故障检测的关键进展，相比劳动密集型人工审查具有显著优势，是长时程多组件LLM代理系统评估的重要进步。

Abstract: We have reached a critical roadblock in the development and enhancement of
long-horizon, multi-component LLM agentic systems: it is incredibly tricky to
identify where these systems break down and why. Evaluation capabilities that
currently exist today (e.g., single pass LLM-as-a-judge) are limited in that
they often focus on individual metrics or capabilities, end-to-end outcomes,
and are narrowly grounded on the preferences of humans. We argue that to match
the agentic capabilities, evaluation frameworks must also be able to reason,
probe, iterate, and understand the complex logic passing through these systems
over long horizons. In this paper, we present RAFFLES - an evaluation
architecture that incorporates reasoning and iterative refinement.
Specifically, RAFFLES operates as an iterative, multi-component pipeline, using
a central Judge to systematically investigate faults and a set of specialized
Evaluators to assess not only the system's components but also the quality of
the reasoning by the Judge itself, thereby building a history of hypotheses. We
tested RAFFLES against several baselines on the Who&When dataset, a benchmark
designed to diagnose the "who" (agent) and "when" (step) of a system's failure.
RAFFLES outperforms these baselines, achieving an agent-step fault pair
accuracy of over 43% on the Algorithmically-Generated dataset (a substantial
increase from the previously published best of 16.6%) and over 20% on the
Hand-Crafted dataset (surpassing the previously published best of 8.8%). These
results demonstrate a key step towards introducing automated fault detection
for autonomous systems over labor-intensive manual human review.

</details>


### [231] [Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet](https://arxiv.org/abs/2509.06861)
*James Xu Zhao,Bryan Hooi,See-Kiong Ng*

Main category: cs.AI

TL;DR: 这篇论文通过对12个推理模型的综合评估，发现测试时扩展（test-time scaling）在知识密集任务中并不肯定能提高准确性，反而可能导致更多幻觉，但但使用思考仍比不思考更好。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时扩展在多个领域表现强劲，但在需要高事实准确性和低幻觉率的知识密集任务中是否同样有效仍不确定，需要系统性评估。

Method: 使用12个推理模型在2个知识密集测试集上进行综合评估，分析测试时计算量增加对准确性和幻觉行为的影响，通过案例研究探索扩展推理如何引发确认偏见。

Result: 增加测试时计算并不肯定提高准确性，反而在许多情况下会导致更多幻觉；减少幻觉往往是模型选择放弃回答，而非改善事实记忆；某些模型会因长时间思考而尝试回答之前放弃的问题，导致幻觉。

Conclusion: 测试时扩展在知识密集任务中存在限制，可能引发确认偏见和过份自信的幻觉，但使用思考仍然比不思考更有益。

Abstract: Test-time scaling increases inference-time computation by allowing models to
generate long reasoning chains, and has shown strong performance across many
domains. However, in this work, we show that this approach is not yet effective
for knowledge-intensive tasks, where high factual accuracy and low
hallucination rates are essential. We conduct a comprehensive evaluation of
test-time scaling using 12 reasoning models on two knowledge-intensive
benchmarks. Our results reveal that increasing test-time computation does not
consistently improve accuracy and, in many cases, it even leads to more
hallucinations. We then analyze how extended reasoning affects hallucination
behavior. We find that reduced hallucinations often result from the model
choosing to abstain after thinking more, rather than from improved factual
recall. Conversely, for some models, longer reasoning encourages attempts on
previously unanswered questions, many of which result in hallucinations. Case
studies show that extended reasoning can induce confirmation bias, leading to
overconfident hallucinations. Despite these limitations, we observe that
compared to non-thinking, enabling thinking remains beneficial. Code and data
are available at https://github.com/XuZhao0/tts-knowledge

</details>


### [232] [Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents](https://arxiv.org/abs/2509.06917)
*Jiacheng Miao,Joe R. Davis,Jonathan K. Pritchard,James Zou*

Main category: cs.AI

TL;DR: Paper2Agent是一个自动化框架，可将研究论文转换为AI代理，使静态论文变为动态交互式研究助手，通过自然语言处理复杂科学查询。


<details>
  <summary>Details</summary>
Motivation: 传统研究论文需要读者投入大量精力理解和适应代码、数据和方法，阻碍了传播和重用。Paper2Agent旨在解决这一挑战，将被动研究成果转化为主动系统。

Method: 使用多代理系统分析论文和代码库，构建Model Context Protocol (MCP)服务器，通过迭代生成和运行测试来优化MCP。然后将论文MCP与聊天代理连接，通过自然语言执行复杂科学查询。

Result: 成功创建了基于AlphaGenome、ScanPy和TISSUE的论文代理，能够复现原始论文结果并正确执行新颖用户查询。

Conclusion: Paper2Agent通过将静态论文转化为动态AI代理，为知识传播引入了新范式，为AI协作科学生态系统奠定了基础。

Abstract: We introduce Paper2Agent, an automated framework that converts research
papers into AI agents. Paper2Agent transforms research output from passive
artifacts into active systems that can accelerate downstream use, adoption, and
discovery. Conventional research papers require readers to invest substantial
effort to understand and adapt a paper's code, data, and methods to their own
work, creating barriers to dissemination and reuse. Paper2Agent addresses this
challenge by automatically converting a paper into an AI agent that acts as a
knowledgeable research assistant. It systematically analyzes the paper and the
associated codebase using multiple agents to construct a Model Context Protocol
(MCP) server, then iteratively generates and runs tests to refine and robustify
the resulting MCP. These paper MCPs can then be flexibly connected to a chat
agent (e.g. Claude Code) to carry out complex scientific queries through
natural language while invoking tools and workflows from the original paper. We
demonstrate Paper2Agent's effectiveness in creating reliable and capable paper
agents through in-depth case studies. Paper2Agent created an agent that
leverages AlphaGenome to interpret genomic variants and agents based on ScanPy
and TISSUE to carry out single-cell and spatial transcriptomics analyses. We
validate that these paper agents can reproduce the original paper's results and
can correctly carry out novel user queries. By turning static papers into
dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for
knowledge dissemination and a foundation for the collaborative ecosystem of AI
co-scientists.

</details>


### [233] [Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference](https://arxiv.org/abs/2509.06942)
*Xiangwei Shen,Zhimin Li,Zhantao Yang,Shiyi Zhang,Yingfang Zhang,Donghao Li,Chunyu Wang,Qinglin Lu,Yansong Tang*

Main category: cs.AI

TL;DR: 通过提前定义噪声先验和提出语义相对偏好优化方法，解决了多步去噪计算费用高和需要离线调整奖励模型的问题，显著提升了模型的实代性和美学质量


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在对齐漏涼模型与人类偏好时遇到的两个主要挑战：(1)多步去噪计算费用高，限制优化步数；(2)需要离线连续调整奖励模型来达到得期的美学质量

Method: 提出Direct-Align方法，通过预定义噪声先验来恢复原始图像，避免后期时间步过度优化；提出语义相对偏好优化(SRPO)，将奖励形式化为文本条件信号，支持在线调整奖励

Result: 通过对FLUX.1.dev模型进行精细调整，在优化去噪和在线奖励调整的基础上，将人类评估的实代性和美学质量提升了3倍以上

Conclusion: 该方法有效解决了多步去噪计算费用高和离线奖励调整的限制，通过提前定义噪声先验和在线奖励调整机制，显著提升了漏涼模型的对齐效果和生成质量

Abstract: Recent studies have demonstrated the effectiveness of directly aligning
diffusion models with human preferences using differentiable reward. However,
they exhibit two primary challenges: (1) they rely on multistep denoising with
gradient computation for reward scoring, which is computationally expensive,
thus restricting optimization to only a few diffusion steps; (2) they often
need continuous offline adaptation of reward models in order to achieve desired
aesthetic quality, such as photorealism or precise lighting effects. To address
the limitation of multistep denoising, we propose Direct-Align, a method that
predefines a noise prior to effectively recover original images from any time
steps via interpolation, leveraging the equation that diffusion states are
interpolations between noise and target images, which effectively avoids
over-optimization in late timesteps. Furthermore, we introduce Semantic
Relative Preference Optimization (SRPO), in which rewards are formulated as
text-conditioned signals. This approach enables online adjustment of rewards in
response to positive and negative prompt augmentation, thereby reducing the
reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model
with optimized denoising and online reward adjustment, we improve its
human-evaluated realism and aesthetic quality by over 3x.

</details>
