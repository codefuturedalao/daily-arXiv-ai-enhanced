<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 38]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.AI](#cs.AI) [Total: 32]
- [cs.DC](#cs.DC) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations](https://arxiv.org/abs/2509.07135)
*Ruggero Marino Lazzaroni,Alessandro Angioi,Michelangelo Puliga,Davide Sanna,Roberto Marras*

Main category: cs.CL

TL;DR: MedBench-IT是首个针对意大利医学大学入学考试的全面基准测试，包含17,410个多选问题，涵盖6个科目和3个难度级别，用于评估LLM在意大利语医学教育领域的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育领域潜力巨大，但缺乏针对非英语语言专业领域的基准测试，特别是意大利语医学入学考试评估标准缺失。

Method: 从Edizioni Simone获取专家编写的17,410个多选问题，涵盖6个科目和3个难度级别，评估包括GPT-4o、Claude系列等专有模型和资源高效的开放模型（<30B参数），并进行可重复性测试、排序偏差分析和推理提示评估。

Result: 模型响应一致性达到88.86%（因科目而异），排序偏差影响最小，问题可读性与模型性能存在统计显著但较小的负相关关系。

Conclusion: MedBench-IT为意大利NLP社区、教育技术开发者和从业者提供了关键资源，揭示了当前模型能力，并为这一重要领域提供了标准化评估方法。

Abstract: Large language models (LLMs) show increasing potential in education, yet
benchmarks for non-English languages in specialized domains remain scarce. We
introduce MedBench-IT, the first comprehensive benchmark for evaluating LLMs on
Italian medical university entrance examinations. Sourced from Edizioni Simone,
a leading preparatory materials publisher, MedBench-IT comprises 17,410
expert-written multiple-choice questions across six subjects (Biology,
Chemistry, Logic, General Culture, Mathematics, Physics) and three difficulty
levels. We evaluated diverse models including proprietary LLMs (GPT-4o, Claude
series) and resource-efficient open-source alternatives (<30B parameters)
focusing on practical deployability.
  Beyond accuracy, we conducted rigorous reproducibility tests (88.86% response
consistency, varying by subject), ordering bias analysis (minimal impact), and
reasoning prompt evaluation. We also examined correlations between question
readability and model performance, finding a statistically significant but
small inverse relationship. MedBench-IT provides a crucial resource for Italian
NLP community, EdTech developers, and practitioners, offering insights into
current capabilities and standardized evaluation methodology for this critical
domain.

</details>


### [2] [The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties](https://arxiv.org/abs/2509.07139)
*William Chen,Chutong Meng,Jiatong Shi,Martijn Bartelds,Shih-Heng Wang,Hsiu-Hsuan Wang,Rafael Mosquera,Sara Hincapie,Dan Jurafsky,Antonis Anastasopoulos,Hung-yi Lee,Karen Livescu,Shinji Watanabe*

Main category: cs.CL

TL;DR: Interspeech 2025 ML-SUPERB 2.0挑战赛构建了包含200+语言、口音和方言的新测试套件，最佳提交在LID准确率上提升23%，CER降低18%，在口音和方言数据上CER降低30.2%，LID准确率提升15.7%


<details>
  <summary>Details</summary>
Motivation: 当前多语言ASR技术的改进在不同语言和语言变体间分布不均，需要推动最先进ASR模型的发展，使语音技术更具包容性

Method: 构建包含200+语言、口音和方言的新测试套件，基于DynaBench建立在线评估服务器，允许参与者灵活设计模型架构

Result: 收到3个团队的5份提交，所有提交均优于基线。最佳提交在通用多语言测试集上LID准确率提升23%，CER降低18%；在口音和方言数据上CER降低30.2%，LID准确率提升15.7%

Conclusion: 社区挑战赛对于使语音技术更具包容性具有重要意义，展示了在多语言、口音和方言ASR方面的显著改进

Abstract: Recent improvements in multilingual ASR have not been equally distributed
across languages and language varieties. To advance state-of-the-art (SOTA) ASR
models, we present the Interspeech 2025 ML-SUPERB 2.0 Challenge. We construct a
new test suite that consists of data from 200+ languages, accents, and dialects
to evaluate SOTA multilingual speech models. The challenge also introduces an
online evaluation server based on DynaBench, allowing for flexibility in model
design and architecture for participants. The challenge received 5 submissions
from 3 teams, all of which outperformed our baselines. The best-performing
submission achieved an absolute improvement in LID accuracy of 23% and a
reduction in CER of 18% when compared to the best baseline on a general
multilingual test set. On accented and dialectal data, the best submission
obtained 30.2% lower CER and 15.7% higher LID accuracy, showing the importance
of community challenges in making speech technologies more inclusive.

</details>


### [3] [Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models](https://arxiv.org/abs/2509.07142)
*Zhiyin Tan,Jennifer D'Souza*

Main category: cs.CL

TL;DR: 提出基于大语言模型的动态主题模型自动评估框架，通过9个LLM指标从四个维度评估主题质量，相比传统指标能更好地发现语义漂移和冗余等问题


<details>
  <summary>Details</summary>
Motivation: 传统主题模型评估指标（如连贯性和多样性）只能捕捉狭窄的统计模式，无法解释语义层面的失败，需要更智能的评估方法来处理动态演化的知识领域

Method: 开发了面向目标的评估框架，使用9个基于LLM的指标，涵盖词汇有效性、主题内语义合理性、主题间结构合理性和文档-主题对齐合理性四个关键维度，并通过对抗性和抽样协议进行验证

Result: LLM指标提供了可解释、稳健且任务相关的评估，能够发现传统指标经常遗漏的关键弱点，如冗余和语义漂移，支持开发可扩展的细粒度评估工具

Conclusion: 基于LLM的评估框架为动态数据集中保持主题相关性提供了有效的解决方案，支持数字图书馆系统中学术内容的组织和检索

Abstract: This study presents a framework for automated evaluation of dynamically
evolving topic models using Large Language Models (LLMs). Topic modeling is
essential for organizing and retrieving scholarly content in digital library
systems, helping users navigate complex and evolving knowledge domains.
However, widely used automated metrics, such as coherence and diversity, often
capture only narrow statistical patterns and fail to explain semantic failures
in practice. We introduce a purpose-oriented evaluation framework that employs
nine LLM-based metrics spanning four key dimensions of topic quality: lexical
validity, intra-topic semantic soundness, inter-topic structural soundness, and
document-topic alignment soundness. The framework is validated through
adversarial and sampling-based protocols, and is applied across datasets
spanning news articles, scholarly publications, and social media posts, as well
as multiple topic modeling methods and open-source LLMs. Our analysis shows
that LLM-based metrics provide interpretable, robust, and task-relevant
assessments, uncovering critical weaknesses in topic models such as redundancy
and semantic drift, which are often missed by traditional metrics. These
results support the development of scalable, fine-grained evaluation tools for
maintaining topic relevance in dynamic datasets. All code and data supporting
this work are accessible at
https://github.com/zhiyintan/topic-model-LLMjudgment.

</details>


### [4] [Towards EnergyGPT: A Large Language Model Specialized for the Energy Sector](https://arxiv.org/abs/2509.07177)
*Amal Chebbi,Babajide Kolade*

Main category: cs.CL

TL;DR: EnergyGPT是基于LLaMA 3.1-8B微调的能源领域专用语言模型，在能源相关任务上表现优于基础模型


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在能源等专业领域效果有限，需要深度技术知识和精确领域专业知识

Method: 使用监督微调方法，在高质量能源文本语料上微调LLaMA 3.1-8B模型，包括数据收集、模型微调、基准设计等完整流程

Result: EnergyGPT在大多数能源相关语言理解和生成任务上优于基础模型，证明了训练策略的有效性

Conclusion: 通过领域专业化微调，可以在不需要大规模基础设施的情况下提升模型在特定领域的相关性和性能

Abstract: Large Language Models have demonstrated impressive capabilities across
various domains. However, their general-purpose nature often limits their
effectiveness in specialized fields such as energy, where deep technical
expertise and precise domain knowledge are essential. In this paper, we
introduce EnergyGPT, a domain-specialized language model tailored for the
energy sector, developed by fine-tuning LLaMA 3.1-8B model using Supervised
Fine-Tuning on a high-quality, curated corpus of energy-related texts. We
present a complete development pipeline, including data collection and
curation, model fine-tuning, benchmark design and LLM-judge choice, evaluation
and deployment. Through this work, we demonstrate that our training strategy
enables improvements in domain relevance and performance without the need for
large-scale infrastructure. By evaluating the performance of the model using
domain-specific question-answering benchmarks, our results demonstrate that
EnergyGPT outperforms the base model in most of the energy-related language
understanding and generation tasks.

</details>


### [5] [DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge](https://arxiv.org/abs/2509.07188)
*Zonghai Yao,Michael Sun,Won Seok Jang,Sunjae Kwon,Soie Kwon,Hong Yu*

Main category: cs.CL

TL;DR: 这篇论文提出了DischargeSim标准化测试平台，用于评估大语言模型在出院教育中的个性化教育能力，发现模型在不同患者特征下表现差异显著，模型大小并非总能提升教育效果。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM测试重点在诊断理解，而忽视了出院后患者教育这一重要临床环节。需要一个专门评估LLM在后访期间个性化教育能力的标准化测试平台。

Method: 开发DischargeSim测试平台，通过DoctorAgent和PatientAgent模拟多轮对话。PatientAgent包含多样化的心理社会特征，涉及6个临床出院主题。评估包括对话质量、个性化文档生成和患者理解力三个维度。

Result: 在18个LLM上进行实验，发现出院教育能力存在显著差距，性能在不同患者特征下波动较大。模型大小与教育效果不正相关，反映了策略使用和内容优先级的变化。

Conclusion: DischargeSim为评测LLM在后访期临床教育中的表现提供了首个标准化平台，有助于推进公平、个性化的患者支持服务。

Abstract: Discharge communication is a critical yet underexplored component of patient
care, where the goal shifts from diagnosis to education. While recent large
language model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they
fail to evaluate models' ability to support patients after the visit. We
introduce DischargeSim, a novel benchmark that evaluates LLMs on their ability
to act as personalized discharge educators. DischargeSim simulates post-visit,
multi-turn conversations between LLM-driven DoctorAgents and PatientAgents with
diverse psychosocial profiles (e.g., health literacy, education, emotion).
Interactions are structured across six clinically grounded discharge topics and
assessed along three axes: (1) dialogue quality via automatic and LLM-as-judge
evaluation, (2) personalized document generation including free-text summaries
and structured AHRQ checklists, and (3) patient comprehension through a
downstream multiple-choice exam. Experiments across 18 LLMs reveal significant
gaps in discharge education capability, with performance varying widely across
patient profiles. Notably, model size does not always yield better education
outcomes, highlighting trade-offs in strategy use and content prioritization.
DischargeSim offers a first step toward benchmarking LLMs in post-visit
clinical education and promoting equitable, personalized patient support.

</details>


### [6] [Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation](https://arxiv.org/abs/2509.07190)
*Zahra Atf,Peter R Lewis*

Main category: cs.CL

TL;DR: 基于道德原则的规则系统用于处理LLM不确定性，通过预防、尊重、责任等道德规则指导不同不确定性水平下的响应，提供透明的理解和说明


<details>
  <summary>Details</summary>
Motivation: 在高风险场景中，现有的概率方法不透明且与透明性期望不一致，需要一种更透明、可解释的方法来处理LLM生成文本中的不确定性

Method: 基于道德心理学和美德伦理学的见解，定义预防、尊重、责任等道德规则，在轻量级Prolog引擎中编码，根据不确定性水平触发对应的系统动作和理由说明

Result: 通过场景模拟评估规则覆盖率、公平性和信任检定，在临床和法律领域的应用案例证明道德推理能够提高信任和可解释性

Conclusion: 该方法为社会责任性自然语言生成提供了一种透明、轻量级的替代方案，可以替代不透明的概率模型

Abstract: Large language models (LLMs) are increasingly used in high-stakes settings,
where explaining uncertainty is both technical and ethical. Probabilistic
methods are often opaque and misaligned with expectations of transparency. We
propose a framework based on rule-based moral principles for handling
uncertainty in LLM-generated text. Using insights from moral psychology and
virtue ethics, we define rules such as precaution, deference, and
responsibility to guide responses under epistemic or aleatoric uncertainty.
These rules are encoded in a lightweight Prolog engine, where uncertainty
levels (low, medium, high) trigger aligned system actions with plain-language
rationales. Scenario-based simulations benchmark rule coverage, fairness, and
trust calibration. Use cases in clinical and legal domains illustrate how moral
reasoning can improve trust and interpretability. Our approach offers a
transparent, lightweight alternative to probabilistic models for socially
responsible natural language generation.

</details>


### [7] [LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade](https://arxiv.org/abs/2509.07274)
*Aida Kostikova,Ole Pütz,Steffen Eger,Olga Sabelfeld,Benjamin Paassen*

Main category: cs.CL

TL;DR: 使用大型语言模型自动标注德国议会辩论中的（反）团结子类型，评估模型性能并分析二战后至今德国移民政策辩论中的团结趋势


<details>
  <summary>Details</summary>
Motivation: 传统上研究德国政治辩论中的移民话题需要大量人工标注，限制了分析范围。LLMs有潜力自动化复杂标注任务，从而深入分析德国议会辩论中的（反）团结表达模式

Method: 使用多种大型语言模型对德国议会辩论进行（反）团结子类型标注，与数千个人工参考标注对比，评估模型大小、提示差异、微调、历史与现代数据的影响，并分析系统错误

Result: 发现战后时期存在高度移民导向的团结表达，而自2015年以来德国议会中反团结趋势显著增强。LLMs在政治文本分析中表现出良好潜力

Conclusion: LLMs为政治文本分析提供了有前景的工具，德国移民辩论在人口下降和劳动力短缺与日益极化的背景下具有重要意义，这些发现为后续研究提供了动力

Abstract: Migration has been a core topic in German political debate, from millions of
expellees post World War II over labor migration to refugee movements in the
recent past. Studying political speech regarding such wide-ranging phenomena in
depth traditionally required extensive manual annotations, limiting the scope
of analysis to small subsets of the data. Large language models (LLMs) have the
potential to partially automate even complex annotation tasks. We provide an
extensive evaluation of a multiple LLMs in annotating (anti-)solidarity
subtypes in German parliamentary debates compared to a large set of thousands
of human reference annotations (gathered over a year). We evaluate the
influence of model size, prompting differences, fine-tuning, historical versus
contemporary data; and we investigate systematic errors. Beyond methodological
evaluation, we also interpret the resulting annotations from a social science
lense, gaining deeper insight into (anti-)solidarity trends towards migrants in
the German post-World War II period and recent past. Our data reveals a high
degree of migrant-directed solidarity in the postwar period, as well as a
strong trend towards anti-solidarity in the German parliament since 2015,
motivating further research. These findings highlight the promise of LLMs for
political text analysis and the importance of migration debates in Germany,
where demographic decline and labor shortages coexist with rising polarization.

</details>


### [8] [Causal Attention with Lookahead Keys](https://arxiv.org/abs/2509.07301)
*Zhuoqing Song,Peng Sun,Huizhuo Yuan,Quanquan Gu*

Main category: cs.CL

TL;DR: CASTLE是一种新的注意力机制，通过动态更新每个token的key来整合后续信息，同时保持自回归特性，在语言建模任务中表现优于标准因果注意力。


<details>
  <summary>Details</summary>
Motivation: 标准因果注意力中每个token的QKV是静态的，只能编码前文信息，限制了模型对完整上下文的理解能力。

Method: 提出CASTLE机制，持续更新每个token的key以整合后续token信息，通过数学等价性实现并行训练，避免显式存储lookahead keys。

Result: 在语言建模基准测试中，CASTLE在不同规模模型上都优于标准因果注意力，降低了验证困惑度，并在多个下游任务上表现更好。

Conclusion: 动态更新key的注意力机制能够有效提升语言模型性能，同时保持自回归特性，为注意力机制设计提供了新思路。

Abstract: In standard causal attention, each token's query, key, and value (QKV) are
static and encode only preceding context. We introduce CAuSal aTtention with
Lookahead kEys (CASTLE), an attention mechanism that continually updates each
token's keys as the context unfolds. We term these updated keys lookahead keys
because they belong to earlier positions yet integrate information from tokens
that appear later relative to those positions, while strictly preserving the
autoregressive property. Although the mechanism appears sequential, we derive a
mathematical equivalence that avoids explicitly materializing lookahead keys at
each position and enables efficient parallel training. On language modeling
benchmarks, CASTLE consistently outperforms standard causal attention across
model scales, reducing validation perplexity and improving performance on a
range of downstream tasks.

</details>


### [9] [Basis Vector Metric: A Method for Robust Open-Ended State Change Detection](https://arxiv.org/abs/2509.07308)
*David Oprea,Sam Powers*

Main category: cs.CL

TL;DR: 基础向量方法(BVM)在图片状态分类中表现最佳，但在形容词区分任务上不如逻辑回归模型


<details>
  <summary>Details</summary>
Motivation: 测试BVM方法在利用语言嵌入判断图片状态变化方面的效果

Method: 使用MIT-States数据集(53,000张图片)，对225个名词和115个形容词进行实验，对比BVM与余弦相似性、点积、量化算法、贝叶斯等多种方法的性能

Result: 在名词类别状态分类中BVM表现最好，但在形容词区分任务上未能超越逻辑回归模型

Conclusion: BVM在图片状态分析中有潜力，尽管在形容词区分上目前较弱，但通过方法调整有提升空间

Abstract: We test a new method, which we will abbreviate using the acronym BVM (Basis
Vectors Method), in its ability to judge the state changes in images through
using language embeddings. We used the MIT-States dataset, containing about
53,000 images, to gather all of our data, which has 225 nouns and 115
adjectives, with each noun having about 9 different adjectives, forming
approximately 1000 noun-adjective pairs. For our first experiment, we test our
method's ability to determine the state of each noun class separately against
other metrics for comparison. These metrics are cosine similarity, dot product,
product quantization, binary index, Naive Bayes, and a custom neural network.
Among these metrics, we found that our proposed BVM performs the best in
classifying the states for each noun. We then perform a second experiment where
we try using BVM to determine if it can differentiate adjectives from one
another for each adjective separately. We compared the abilities of BVM to
differentiate adjectives against the proposed method the MIT-States paper
suggests: using a logistic regression model. In the end, we did not find
conclusive evidence that our BVM metric could perform better than the logistic
regression model at discerning adjectives. Yet, we were able to find evidence
for possible improvements to our method; this leads to the chance of increasing
our method's accuracy through certain changes in our methodologies.

</details>


### [10] [Instance-level Performance Prediction for Long-form Generation Tasks](https://arxiv.org/abs/2509.07309)
*Chi-Yang Hsu,Alexander Braylan,Yiheng Su,Omar Alonso,Matthew Lease*

Main category: cs.CL

TL;DR: 这篇论文提出了一个新的长文本生成任务的实例级性能预测基准，支持多维度、细粒度评估指标的预测，包括点估计和预测区间。


<details>
  <summary>Details</summary>
Motivation: 为了解决长文本生成任务中多维度质量评估的性能预测问题，提供任务、模型和评估指标无关的黑盒预测方案。

Method: 使用黑盒模型的输入和输出来预测连续的评估指标得分，包括点估计和预测区间以量化不确定性。方法仅需要16个训练示例即可有效预测。

Result: 在11个长文本数据集/任务上进行了评估，涵盖多个大语言模型、基准方法和指标，证明了在各种长文本生成任务上都能有效预测性能。

Conclusion: 该研究提供了一个新题的、有用的任务定义，以及一个有价值的基准来推动进步，同时提供了可以立即应用的基准方法。

Abstract: We motivate and share a new benchmark for instance-level performance
prediction of long-form generation tasks having multi-faceted, fine-grained
quality metrics. Our task-, model- and metric-agnostic formulation predicts
continuous evaluation metric scores given only black-box model inputs and
outputs. Beyond predicting point estimates of metric scores, the benchmark also
requires inferring prediction intervals to quantify uncertainty around point
estimates. Evaluation spans 11 long-form datasets/tasks with multiple LLMs,
baselines, and metrics per task. We show that scores can be effectively
predicted across long-form generation tasks using as few as 16 training
examples. Overall, we introduce a novel and useful task, a valuable benchmark
to drive progress, and baselines ready for practical adoption today.

</details>


### [11] [Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations](https://arxiv.org/abs/2509.07311)
*Sihyun Park*

Main category: cs.CL

TL;DR: 提出了KAMIR方法，通过分析模型内部表示来选择监督微调数据，避免依赖提示工程，能应用于多种任务类型


<details>
  <summary>Details</summary>
Motivation: 现有的基于知识的数据选择方法依赖提示工程，对变化敏感且成本高，需要更有效的数据选择方法

Method: KAMIR通过计算每层隐藏状态与最终隐藏状态的相似度来评估数据，基于模型对输入的熟悉程度选择训练数据

Result: 实验表明，使用不太熟悉的数据进行训练能获得更好的泛化性能，且方法适用于机器阅读理解、摘要等多种任务

Conclusion: KAMIR提供了一种有效的数据选择方法，不依赖提示工程，能广泛应用于各种NLP任务，提高监督微调效率

Abstract: Recent advances in large language models (LLMs) have been driven by
pretraining, supervised fine tuning (SFT), and alignment tuning. Among these,
SFT plays a crucial role in transforming a model 's general knowledge into
structured responses tailored to specific tasks. However, there is no clearly
established methodology for effective training data selection. Simply
increasing the volume of data does not guarantee performance improvements,
while preprocessing, sampling, and validation require substantial time and
cost.
  To address this issue, a variety of data selection methods have been
proposed. Among them, knowledge based selection approaches identify suitable
training data by analyzing the model 's responses. Nevertheless, these methods
typically rely on prompt engineering, making them sensitive to variations and
incurring additional costs for prompt design.
  In this study, we propose Knowledge Analysis via Model Internal
Representations (KAMIR), a novel approach that overcomes these limitations by
analyzing data based on the model 's internal representations. KAMIR computes
similarities between the hidden states of each layer (block) and the final
hidden states for a given input to assess the data. Unlike prior methods that
were largely limited to multiple choice tasks, KAMIR can be applied to a wide
range of tasks such as machine reading comprehension and summarization.
Moreover, it selects data useful for training based on the model 's familiarity
with the input, even with a small dataset and a simple classifier architecture.
Experiments across diverse task datasets demonstrate that training with less
familiar data leads to better generalization performance.

</details>


### [12] [Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation](https://arxiv.org/abs/2509.07324)
*Nakyung Lee,Yeongoon Kim,Minhae Oh,Suhwan Kim,Jin Woo Koo,Hyewon Jo,Jungwoo Lee*

Main category: cs.CL

TL;DR: 提出了SAOBP框架，通过信念传播注入多跳关系来解决Transformer自注意力机制的局部化问题，防止注意力熵崩溃并提升模型性能


<details>
  <summary>Details</summary>
Motivation: Transformer自注意力机制存在局部化问题，注意力集中在有限token子集上，难以捕获长距离依赖关系

Method: 提出Self-Attention One-step Belief Propagation (SAOBP)框架，通过信念传播过程注入多跳关系，并引入Global Token Dependency (GTD)指标来量化和解释这些交互

Result: SAOBP能防止深层网络中的注意力熵崩溃，自适应地维持任务适当的GTD水平，在小规模模型上取得竞争性性能提升

Conclusion: SAOBP框架有效解决了注意力局部化问题，特别适合资源受限场景下的推理质量提升

Abstract: Transformer-based self-attention mechanism serves as the core of modern
language models, yet it often suffers from localization, where attentions
collapse onto a limited subset of tokens and fail to capture long-range
dependencies. To address this issue, we propose Self-Attention One-step Belief
Propagation (SAOBP), a refinement framework that injects multi-hop
relationships through a belief propagation process. To interpret and quantify
these interactions, we introduce Global Token Dependency (GTD) that captures
the relative contribution of multihop connections within the attention graph.
Empirical results indicate that SAOBP helps prevent entropy collapse in deeper
layers and adaptively maintains GTD at task-appropriate levels, thereby
supporting improvements in model performance. Importantly, we observe
competitive gains in small-scale models, highlighting its potential for
improving inference quality in resource-constrained scenarios.

</details>


### [13] [PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions](https://arxiv.org/abs/2509.07370)
*Yixuan Tang,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

TL;DR: PersonaFuse是一个基于特质激活理论和五大人格模型的LLM后训练框架，通过混合专家架构实现情境化人格表达，显著提升社交情感智能而不影响通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在现实对话中表现出情感感知和社交能力的局限性，无法根据不同社交和任务情境调整沟通风格和情感表达。

Method: 采用混合专家架构，结合人格适配器和动态路由网络，基于特质激活理论和五大人格模型实现情境化特质表达。

Result: 在社交情感智能多个维度上显著优于基线模型，在下游人本应用（如心理健康咨询和客服）中表现一致提升，人类偏好评估显示与大型模型竞争性表现。

Conclusion: PersonaFuse提供了一个理论基础扎实且实用的方法，用于开发社交情感增强的LLM，标志着向更人本AI系统的重要进展。

Abstract: Recent advancements in Large Language Models (LLMs) demonstrate remarkable
capabilities across various fields. These developments have led to more direct
communication between humans and LLMs in various situations, such as social
companionship and psychological support. However, LLMs often exhibit
limitations in emotional perception and social competence during real-world
conversations. These limitations partly originate from their inability to adapt
their communication style and emotional expression to different social and task
contexts. In this work, we introduce PersonaFuse, a novel LLM post-training
framework that enables LLMs to adapt and express different personalities for
varying situations. Inspired by Trait Activation Theory and the Big Five
personality model, PersonaFuse employs a Mixture-of-Expert architecture that
combines persona adapters with a dynamic routing network, enabling contextual
trait expression. Experimental results show that PersonaFuse substantially
outperforms baseline models across multiple dimensions of social-emotional
intelligence. Importantly, these gains are achieved without sacrificing general
reasoning ability or model safety, which remain common limitations of direct
prompting and supervised fine-tuning approaches. PersonaFuse also delivers
consistent improvements in downstream human-centered applications, such as
mental health counseling and review-based customer service. Finally, human
preference evaluations against leading LLMs, including GPT-4o and DeepSeek,
demonstrate that PersonaFuse achieves competitive response quality despite its
comparatively smaller model size. These findings demonstrate that
PersonaFuse~offers a theoretically grounded and practical approach for
developing social-emotional enhanced LLMs, marking a significant advancement
toward more human-centric AI systems.

</details>


### [14] [Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents](https://arxiv.org/abs/2509.07389)
*Sankalp Tattwadarshi Swain,Anshika Krishnatray,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.CL

TL;DR: 大语言模型在通过模式识别和交互反馈学习新语言方面的评估研究，发现LLM模型无法在100次回复内建立对话，但展现出类似人类语言学习的策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注词汇学习、语法概括等语言能力，缺乏对LLM通过模式识别和交互反馈学习语言的评估，而这正是人类语言获得的核心特征。

Method: 提出了一种新的实验框架，让LLM模型与只理解新构建语言Tinkatongue的机器人进行对话，评估其学习和使用新语言的能力。

Result: LLM模型在100次回复内无法成功建立对话，但它们采用了与人类语言学习类似的特定策略。

Conclusion: 结果为评估标准提供了新方向，并为更有效学习交互反馈的模型设计开启了新途径。

Abstract: Existing evaluation studies on linguistic competence of large language models
(LLM agents) have focused primarily on vocabulary learning, morphological rule
induction, syntactic generalization, pragmatic inference, and cross-linguistic
transfer. However, none assess whether LLM agents can acquire a language
through pattern recognition and interactive feedback, a central feature of
human language acquisition. We propose a novel experimental framework in which
an LLM agent is evaluated on its ability to acquire and use a newly constructed
language (Tinkatongue) in conversation with a bot that understands only
Tinkatongue. Our findings show that LLM agents fail to establish a conversation
within 100 responses, yet they adopt distinct strategies that mirror human
approaches to language learning. The results suggest a new direction for
evaluation benchmarks and open pathways to model designs that learn more
effectively from interactive feedback.

</details>


### [15] [The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering](https://arxiv.org/abs/2509.07399)
*Yi-Jie Cheng,Oscar Chew,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 通过简单高效的知识图遍历模块提升小型语言模型在知识固问答任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有知识图集成方法多依赖大型或专有模型，小型语言模型在知识图遍历和推理能力上有限，限制了可访问性和可扩展性

Method: 提出将知识图遍历任务交由简单高效的探索模块处理，而非由语言模型本身执行

Result: 这些轻量级模块有效提升了小型语言模型在知识图问答任务中的性能

Conclusion: 通过将知识图遍历任务分离出来由专门模块处理，可以充分发挥小型语言模型的潜力，提高知识图集成方案的可访问性和可扩展性

Abstract: Integrating knowledge graphs (KGs) into the reasoning processes of large
language models (LLMs) has emerged as a promising approach to mitigate
hallucination. However, existing work in this area often relies on proprietary
or extremely large models, limiting accessibility and scalability. In this
study, we investigate the capabilities of existing integration methods for
small language models (SLMs) in KG-based question answering and observe that
their performance is often constrained by their limited ability to traverse and
reason over knowledge graphs. To address this limitation, we propose leveraging
simple and efficient exploration modules to handle knowledge graph traversal in
place of the language model itself. Experiment results demonstrate that these
lightweight modules effectively improve the performance of small language
models on knowledge graph question answering tasks. Source code:
https://github.com/yijie-cheng/SLM-ToG/.

</details>


### [16] [LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction](https://arxiv.org/abs/2509.07403)
*Weichu Liu,Jing Xiong,Yuxuan Hu,Zixuan Li,Minghuan Tan,Ningning Mao,Chenyang Zhao,Zhongwei Wan,Chaofan Tao,Wendong Xu,Hui Shen,Chengming Li,Lingpeng Kong,Ngai Wong*

Main category: cs.CL

TL;DR: 提出了LongEmotion基准测试，专门针对长上下文情感智能任务，包含6类情感任务，平均输入长度8777个token。提出了RAG和CoEM方法提升性能，实验证明这些方法在大多数长上下文任务中都能增强LLMs的情感智能表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试忽略了长上下文场景下的情感智能评估，特别是在现实、实用的设置中，交互往往是冗长、多样且嘈杂的。需要向更现实的环境推进。

Method: 1) 构建LongEmotion基准，包含情感分类、检测、问答、对话、摘要和表达6类任务；2) 提出RAG方法，利用对话上下文和LLM本身作为检索源；3) 提出CoEM方法，将任务分解为5个阶段，整合检索增强和有限知识注入。

Result: 实验结果显示RAG和CoEM方法在大多数长上下文任务中都能持续提升情感智能相关性能，推动LLMs向更实用和真实世界的情感智能应用发展。

Conclusion: LongEmotion基准填补了长上下文情感智能评估的空白，提出的RAG和CoEM方法有效提升了LLMs在现实场景中的情感智能表现，为实际应用提供了有力支持。

Abstract: Large language models (LLMs) make significant progress in Emotional
Intelligence (EI) and long-context understanding. However, existing benchmarks
tend to overlook certain aspects of EI in long-context scenarios, especially
under realistic, practical settings where interactions are lengthy, diverse,
and often noisy. To move towards such realistic settings, we present
LongEmotion, a benchmark specifically designed for long-context EI tasks. It
covers a diverse set of tasks, including Emotion Classification, Emotion
Detection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion
Expression. On average, the input length for these tasks reaches 8,777 tokens,
with long-form generation required for Emotion Expression. To enhance
performance under realistic constraints, we incorporate Retrieval-Augmented
Generation (RAG) and Collaborative Emotional Modeling (CoEM), and compare them
with standard prompt-based methods. Unlike conventional approaches, our RAG
method leverages both the conversation context and the large language model
itself as retrieval sources, avoiding reliance on external knowledge bases. The
CoEM method further improves performance by decomposing the task into five
stages, integrating both retrieval augmentation and limited knowledge
injection. Experimental results show that both RAG and CoEM consistently
enhance EI-related performance across most long-context tasks, advancing LLMs
toward more practical and real-world EI applications. Furthermore, we conducted
a comparative case study experiment on the GPT series to demonstrate the
differences among various models in terms of EI. Code is available on GitHub at
https://github.com/LongEmotion/LongEmotion, and the project page can be found
at https://longemotion.github.io/.

</details>


### [17] [AIxcellent Vibes at GermEval 2025 Shared Task on Candy Speech Detection: Improving Model Performance by Span-Level Training](https://arxiv.org/abs/2509.07459)
*Christian Rene Thelen,Patrick Gustav Blaneck,Tobias Bornheim,Niklas Grieger,Stephan Bialonski*

Main category: cs.CL

TL;DR: 多语言XLM-RoBERTa-Large模型在德语YouTube评论中成功检测支持性语言，在二元分类和范围识别任务中都获得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 自动检测正面支持性在线沟通（糖果语言）的技术仍然不足，限制了对其影响的系统分析。

Method: 使用单语言和多语言语言模型（GBERT、Qwen3 Embedding、XLM-RoBERTa）在46k德语YouTube评论语料库中进行糖果语言检测，采用范围级训练方法。

Result: 多语言XLM-RoBERTa-Large模型表现最佳，在GermEval 2025共享任务中获得二元正样本F1分数0.8906，严格范围检测F1分数0.6307。

Conclusion: 范围基训练、多语言能力和表情符号识别技术提升了检测性能，多语言模型在识别正面支持性语言方面具有高效性。

Abstract: Positive, supportive online communication in social media (candy speech) has
the potential to foster civility, yet automated detection of such language
remains underexplored, limiting systematic analysis of its impact. We
investigate how candy speech can be reliably detected in a 46k-comment German
YouTube corpus by monolingual and multilingual language models, including
GBERT, Qwen3 Embedding, and XLM-RoBERTa. We find that a multilingual
XLM-RoBERTa-Large model trained to detect candy speech at the span level
outperforms other approaches, ranking first in both binary positive F1: 0.8906)
and categorized span-based detection (strict F1: 0.6307) subtasks at the
GermEval 2025 Shared Task on Candy Speech Detection. We speculate that
span-based training, multilingual capabilities, and emoji-aware tokenizers
improved detection performance. Our results demonstrate the effectiveness of
multilingual models in identifying positive, supportive language.

</details>


### [18] [Understanding Stigmatizing Language Lexicons: A Comparative Analysis in Clinical Contexts](https://arxiv.org/abs/2509.07462)
*Yiliang Zhou,Di Hu,Tianchu Lyu,Jasmine Dhillon,Alexandra L. Beck,Gelareh Sadigh,Kai Zheng*

Main category: cs.CL

TL;DR: 系统文献回顾发现4个污名化语言词典，分析显示它们具有中等语义相似度，大多数污名化术语与临床医生描述负面行为的评判性表达相关，情感分析以负面词汇为主，但各词典间存在差异。


<details>
  <summary>Details</summary>
Motivation: 污名化语言导致医疗不平等，但目前缺乏统一标准化的污名化语言词典来定义医疗环境中构成污名化的词汇、术语或短语。

Method: 通过系统文献检索识别现有污名化语言词典，进行对比分析：1）词典间的相似性和差异；2）基于既定情感数据集的积极、消极或中性术语分布。

Result: 识别出4个词典，分析显示它们具有中等语义相似度，大多数污名化术语涉及临床医生描述感知负面行为的评判性表达。情感分析显示负面分类术语占主导地位，但各词典间存在差异。

Conclusion: 研究结果强调了标准化词典的必要性，并凸显了在临床文本中定义污名化语言所面临的挑战。

Abstract: Stigmatizing language results in healthcare inequities, yet there is no
universally accepted or standardized lexicon defining which words, terms, or
phrases constitute stigmatizing language in healthcare. We conducted a
systematic search of the literature to identify existing stigmatizing language
lexicons and then analyzed them comparatively to examine: 1) similarities and
discrepancies between these lexicons, and 2) the distribution of positive,
negative, or neutral terms based on an established sentiment dataset. Our
search identified four lexicons. The analysis results revealed moderate
semantic similarity among them, and that most stigmatizing terms are related to
judgmental expressions by clinicians to describe perceived negative behaviors.
Sentiment analysis showed a predominant proportion of negatively classified
terms, though variations exist across lexicons. Our findings underscore the
need for a standardized lexicon and highlight challenges in defining
stigmatizing language in clinical texts.

</details>


### [19] [From Scarcity to Efficiency: Investigating the Effects of Data Augmentation on African Machine Translation](https://arxiv.org/abs/2509.07471)
*Mardiyyah Oduwole,Oluwatosin Olajide,Jamiu Suleiman,Faith Hunja,Busayo Awobade,Fatimo Adebanjo,Comfort Akanni,Chinonyelum Igwe,Peace Ododo,Promise Omoigui,Steven Kolawole,Abraham Owodunni*

Main category: cs.CL

TL;DR: 这篇论文研究了数据增帽技术在改善低资源非洲语言机器翻译中的效果，通过反向翻译和切换替代技术在6种语言上实验，平均提升BLEU指标25%


<details>
  <summary>Details</summary>
Motivation: 非洲大陆的语言多样性给机器翻译带来挑战和机遇，需要找到有效方法提升低资源语言的翻译系统性能

Method: 采用两种数据增帽技术：反向翻译的句子拼接和switch-out切换替代技术，在6种低资源非洲语言上进行实验

Result: 所有测试语言都显示出显著的翻译性能提升，BLEU指标最低提升25%，证明了数据增帽技术的有效性

Conclusion: 这些数据增帽技术具有强大潜力，能够有效改善低资源语言的机器翻译系统，为资源豌乏语言的翻译技术发展做出贡献

Abstract: The linguistic diversity across the African continent presents different
challenges and opportunities for machine translation. This study explores the
effects of data augmentation techniques in improving translation systems in
low-resource African languages. We focus on two data augmentation techniques:
sentence concatenation with back translation and switch-out, applying them
across six African languages. Our experiments show significant improvements in
machine translation performance, with a minimum increase of 25\% in BLEU score
across all six languages.We provide a comprehensive analysis and highlight the
potential of these techniques to improve machine translation systems for
low-resource languages, contributing to the development of more robust
translation systems for under-resourced languages.

</details>


### [20] [HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention](https://arxiv.org/abs/2509.07475)
*Saumya Goswami,Siddharth Kurra*

Main category: cs.CL

TL;DR: HALT-RAG是一个后处理验证系统，用于检测RAG管道输出中的幻觉内容，使用NLI模型和词汇特征训练元分类器，在HaluEval基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 检测生成语言模型输出中与源文本矛盾或不受支持的内容，对于安全部署生成模型至关重要。

Method: 使用两个冻结的现成NLI模型和轻量级词汇信号构建通用特征集，训练简单、校准且任务适应的元分类器，采用5折交叉验证防止数据泄漏。

Result: 在HaluEval基准测试中，摘要、问答和对话任务分别获得0.7756、0.9786和0.7391的F1分数，校准概率支持实用的弃权机制。

Conclusion: HALT-RAG提供了一个可靠的工具，通过平衡模型性能和安全要求，有效识别RAG输出中的幻觉内容。

Abstract: Detecting content that contradicts or is unsupported by a given source text
is a critical challenge for the safe deployment of generative language models.
We introduce HALT-RAG, a post-hoc verification system designed to identify
hallucinations in the outputs of Retrieval-Augmented Generation (RAG)
pipelines. Our flexible and task-adaptable framework uses a universal feature
set derived from an ensemble of two frozen, off-the-shelf Natural Language
Inference (NLI) models and lightweight lexical signals. These features are used
to train a simple, calibrated, and task-adapted meta-classifier. Using a
rigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and
produce unbiased estimates, we evaluate our system on the HaluEval benchmark.
By pairing our universal feature set with a lightweight, task-adapted
classifier and a precision-constrained decision policy, HALT-RAG achieves
strong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,
and dialogue tasks, respectively. The system's well-calibrated probabilities
enable a practical abstention mechanism, providing a reliable tool for
balancing model performance with safety requirements.

</details>


### [21] [ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval](https://arxiv.org/abs/2509.07512)
*Zihan Chen,Lei Shi,Weize Wu,Qiji Zhou,Yue Zhang*

Main category: cs.CL

TL;DR: ALLabel是一个三阶段主动学习框架，通过选择最具信息量和代表性的样本来优化LLM的实体识别性能，在相同标注预算下显著优于基线方法，仅需标注5%-10%数据即可达到全数据集标注的性能。


<details>
  <summary>Details</summary>
Motivation: 解决科学领域实体识别任务中LLM微调成本高昂的问题，寻求性能与成本的最佳平衡。

Method: 提出三阶段主动学习框架：1）使用不同策略选择信息样本 2）构建真实检索语料库 3）用于LLM上下文学习

Result: 在三个专业领域数据集上，ALLabel在相同标注预算下始终优于所有基线方法，仅标注5%-10%数据即可达到全数据集标注的同等性能。

Conclusion: ALLabel框架有效降低了LLM实体识别的标注成本，验证了其有效性和泛化能力，为科学领域数据驱动研究提供了高效解决方案。

Abstract: Many contemporary data-driven research efforts in the natural sciences, such
as chemistry and materials science, require large-scale, high-performance
entity recognition from scientific datasets. Large language models (LLMs) have
increasingly been adopted to solve the entity recognition task, with the same
trend being observed on all-spectrum NLP tasks. The prevailing entity
recognition LLMs rely on fine-tuned technology, yet the fine-tuning process
often incurs significant cost. To achieve a best performance-cost trade-off, we
propose ALLabel, a three-stage framework designed to select the most
informative and representative samples in preparing the demonstrations for LLM
modeling. The annotated examples are used to construct a ground-truth retrieval
corpus for LLM in-context learning. By sequentially employing three distinct
active learning strategies, ALLabel consistently outperforms all baselines
under the same annotation budget across three specialized domain datasets.
Experimental results also demonstrate that selectively annotating only 5\%-10\%
of the dataset with ALLabel can achieve performance comparable to the method
annotating the entire dataset. Further analyses and ablation studies verify the
effectiveness and generalizability of our proposal.

</details>


### [22] [VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents](https://arxiv.org/abs/2509.07553)
*Zheng Wu,Heyuan Huang,Xingyu Lou,Xiangmou Qu,Pengzhou Cheng,Zongru Wu,Weiwen Liu,Weinan Zhang,Jun Wang,Zhaoxiang Wang,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: 提出VeriOS-Agent，一个可信的操作系统代理，通过查询驱动的人机交互框架在不可信环境下主动向人类查询，提高任务执行安全性


<details>
  <summary>Details</summary>
Motivation: 现有操作系统代理主要针对理想化环境设计，而现实世界环境往往存在不可信条件，存在过度执行的风险

Method: 采用两阶段学习范式，构建查询驱动的人机-GUI交互框架，在正常条件下自主执行动作，在不可信场景中主动查询人类

Result: 在不可信场景中平均步骤成功率比最先进方法提高20.64%，且不影响正常性能表现

Conclusion: VeriOS-Agent展现出良好的合理性、泛化性和可扩展性，为可信操作系统代理的发展提供了有效解决方案

Abstract: With the rapid progress of multimodal large language models, operating system
(OS) agents become increasingly capable of automating tasks through on-device
graphical user interfaces (GUIs). However, most existing OS agents are designed
for idealized settings, whereas real-world environments often present
untrustworthy conditions. To mitigate risks of over-execution in such
scenarios, we propose a query-driven human-agent-GUI interaction framework that
enables OS agents to decide when to query humans for more reliable task
completion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy
OS agent trained with a two-stage learning paradigm that falicitate the
decoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent
autonomously executes actions in normal conditions while proactively querying
humans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves
the average step-wise success rate by 20.64\% in untrustworthy scenarios over
the state-of-the-art, without compromising normal performance. Analysis
highlights VeriOS-Agent's rationality, generalizability, and scalability. The
codes, datasets and models are available at
https://github.com/Wuzheng02/VeriOS.

</details>


### [23] [Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition](https://arxiv.org/abs/2509.07555)
*Yi Liu,Xiangrong Zhu,Xiangyu Liu,Wei Wei,Wei Hu*

Main category: cs.CL

TL;DR: 提出IRAKE方法解决多跳问答中知识编辑的跳过问题，通过引导分解和迭代检索增强编辑效果


<details>
  <summary>Details</summary>
Motivation: 大语言模型知识快速过时，重新训练成本高。现有基于检索增强的知识编辑方法在简单知识编辑表现良好，但在多跳问答中存在编辑跳过问题，无法有效利用编辑后的知识

Method: 提出迭代检索增强知识编辑方法(IRAKE)，通过单编辑事实和完整编辑案例的引导进行分解，解决粒度不匹配和自然语言表达多样性导致的编辑跳过问题

Result: 实验结果表明IRAKE有效缓解了编辑跳过导致的编辑失败，在多跳问答知识编辑任务上优于现有最先进方法

Conclusion: IRAKE通过引导分解和迭代检索机制，成功解决了多跳问答中知识编辑的挑战，为大规模语言模型的知识更新提供了有效解决方案

Abstract: In a rapidly evolving world where information updates swiftly, knowledge in
large language models (LLMs) becomes outdated quickly. Retraining LLMs is not a
cost-effective option, making knowledge editing (KE) without modifying
parameters particularly necessary. We find that although existing
retrieval-augmented generation (RAG)-based KE methods excel at editing simple
knowledge, they struggle with KE in multi-hop question answering due to the
issue of "edit skipping", which refers to skipping the relevant edited fact in
inference. In addition to the diversity of natural language expressions of
knowledge, edit skipping also arises from the mismatch between the granularity
of LLMs in problem-solving and the facts in the edited memory. To address this
issue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing
method with guided decomposition (IRAKE) through the guidance from single
edited facts and entire edited cases. Experimental results demonstrate that
IRAKE mitigates the failure of editing caused by edit skipping and outperforms
state-of-the-art methods for KE in multi-hop question answering.

</details>


### [24] [BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment](https://arxiv.org/abs/2509.07588)
*Andrey Sakhovskiy,Elena Tutubalina*

Main category: cs.CL

TL;DR: BALI是一种新颖的联合语言模型和知识图谱预训练方法，通过同时学习专用KG编码器并对齐LM和图表示，将外部知识增强到语言模型中，提升生物医学文本理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学LLM对复杂领域特定概念结构和生物医学知识图谱中编码的事实信息理解有限，需要更好的方法来整合外部知识。

Method: 提出BALI方法，将生物医学概念提及链接到UMLS知识图谱，利用局部KG子图作为跨模态正样本进行表示对齐，联合预训练LM和KG编码器。

Result: 在PubMedBERT和BioLinkBERT等生物医学LM上实施该方法，即使使用PubMed科学摘要的小型对齐数据集进行最小预训练，也能提高语言理解任务性能和实体表示质量。

Conclusion: BALI方法有效提升了生物医学语言模型对领域知识的理解和表示能力，为生物医学文本理解提供了新的知识增强解决方案。

Abstract: In recent years, there has been substantial progress in using pretrained
Language Models (LMs) on a range of tasks aimed at improving the understanding
of biomedical texts. Nonetheless, existing biomedical LLMs show limited
comprehension of complex, domain-specific concept structures and the factual
information encoded in biomedical Knowledge Graphs (KGs). In this work, we
propose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel
joint LM and KG pre-training method that augments an LM with external knowledge
by the simultaneous learning of a dedicated KG encoder and aligning the
representations of both the LM and the graph. For a given textual sequence, we
link biomedical concept mentions to the Unified Medical Language System (UMLS)
KG and utilize local KG subgraphs as cross-modal positive samples for these
mentions. Our empirical findings indicate that implementing our method on
several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves
their performance on a range of language understanding tasks and the quality of
entity representations, even with minimal pre-training on a small alignment
dataset sourced from PubMed scientific abstracts.

</details>


### [25] [MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs](https://arxiv.org/abs/2509.07622)
*Libo Ren,Yee Man Ng,Lifeng Han*

Main category: cs.CL

TL;DR: 这篇论文提出了一种迭代自我提示技术，利用大语言模型对临床案例文档进行摘要生成，在MultiClinSUM共享任务中获得了较好的评分。


<details>
  <summary>Details</summary>
Motivation: 临床报告通常长繁且包含专业术语，影响医生和患者之间的沟通效果。需要一种能够高效识别重要信息的摘要方法来支持共同决策。

Method: 采用迭代自我提示技术，让LLM生成任务特定提示并通过少样本学习精炼。使用ROUGE和BERT-score指标指导模型微调，在GPT-4和GPT-4o上实现视角感知自我提示方法。

Result: 在3,396份多娱科临床案例报告上，获得ROUGE分数(46.53, 24.68, 30.77)和BERTscore(87.84, 83.25, 85.46)。高BERTscore表明模型生成的摘要在语义上与参考摘要等价，虽然词汇层面的重叠度较低。

Conclusion: 视角感知自我提示方法可以有效应用于临床报告摘要，支持更好的医情沟通，为医生和患者之间的共同决策提供技术支持。

Abstract: Efficient communication between patients and clinicians plays an important
role in shared decision-making. However, clinical reports are often lengthy and
filled with clinical jargon, making it difficult for domain experts to identify
important aspects in the document efficiently. This paper presents the
methodology we applied in the MultiClinSUM shared task for summarising clinical
case documents. We used an Iterative Self-Prompting technique on large language
models (LLMs) by asking LLMs to generate task-specific prompts and refine them
via example-based few-shot learning. Furthermore, we used lexical and embedding
space metrics, ROUGE and BERT-score, to guide the model fine-tuning with
epochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved
ROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P,
R, F1) from the official evaluation on 3,396 clinical case reports from various
specialties extracted from open journals. The high BERTscore indicates that the
model produced semantically equivalent output summaries compared to the
references, even though the overlap at the exact lexicon level is lower, as
reflected in the lower ROUGE scores. This work sheds some light on how
perspective-aware ISP (PA-ISP) can be deployed for clinical report
summarisation and support better communication between patients and clinicians.

</details>


### [26] [MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval](https://arxiv.org/abs/2509.07666)
*Xixi Wu,Yanchao Tan,Nan Hou,Ruiyang Zhang,Hong Cheng*

Main category: cs.CL

TL;DR: MoLoRAG是一个逻辑感知的检索框架，通过构建页面图来捕捉页面间的逻辑关系，结合语义和逻辑相关性进行多模态多页文档检索，显著提升了文档问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法将文档转换为文本会丢失多模态信息，而大型视觉语言模型受限于输入长度无法处理多页文档。现有的检索增强生成方法仅依赖语义相关性，忽略了页面间的逻辑连接关系。

Method: 构建页面图捕捉页面间的上下文关系，使用轻量级视觉语言模型进行图遍历检索相关页面，结合语义和逻辑相关性。提供无需训练和微调两种变体，检索后使用任意大型视觉语言模型进行问答。

Result: 在四个文档问答数据集上，相比大型视觉语言模型直接推理平均准确率提升9.68%，相比基线方法检索精度提升7.44%。

Conclusion: MoLoRAG通过结合语义和逻辑相关性，有效解决了多模态多页文档理解中的检索问题，显著提升了文档问答性能。

Abstract: Document Understanding is a foundational AI capability with broad
applications, and Document Question Answering (DocQA) is a key evaluation task.
Traditional methods convert the document into text for processing by Large
Language Models (LLMs), but this process strips away critical multi-modal
information like figures. While Large Vision-Language Models (LVLMs) address
this limitation, their constrained input size makes multi-page document
comprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate
this by selecting relevant pages, but they rely solely on semantic relevance,
ignoring logical connections between pages and the query, which is essential
for reasoning.
  To this end, we propose MoLoRAG, a logic-aware retrieval framework for
multi-modal, multi-page document understanding. By constructing a page graph
that captures contextual relationships between pages, a lightweight VLM
performs graph traversal to retrieve relevant pages, including those with
logical connections often overlooked. This approach combines semantic and
logical relevance to deliver more accurate retrieval. After retrieval, the
top-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance
flexibility, MoLoRAG offers two variants: a training-free solution for easy
deployment and a fine-tuned version to improve logical relevance checking.
Experiments on four DocQA datasets demonstrate average improvements of 9.68% in
accuracy over LVLM direct inference and 7.44% in retrieval precision over
baselines. Codes and datasets are released at
https://github.com/WxxShirley/MoLoRAG.

</details>


### [27] [M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models](https://arxiv.org/abs/2509.07730)
*Zexuan Li,Hongliang Dai,Piji Li*

Main category: cs.CL

TL;DR: 提出了M-BRe框架，通过关系分组、关系提取和标签决策三个模块，结合多分类和二分类方法的优势，从未标注文本中自动提取高质量的关系抽取训练样本。


<details>
  <summary>Details</summary>
Motivation: 关系抽取任务中手动标注训练数据成本高昂，而包含目标关系的句子稀少且难以发现。现有LLM方法在多分类设置下难以全面捕捉关系语义，而二分类方法计算开销过大。

Method: 使用三个模块：关系分组（将相似关系聚类）、关系提取（对每个关系组进行多分类）、标签决策（对候选实例进行二分类验证），结合多分类和二分类的优势。

Result: 大量实验证实该框架能够从未标注文本中发现高质量的训练样本，在关系抽取任务上表现出优越性能。

Conclusion: M-BRe框架有效解决了LLM在关系抽取中的语义捕捉不全面和计算开销大的问题，为自动提取训练实例提供了实用解决方案。

Abstract: For Relation Extraction (RE), the manual annotation of training data may be
prohibitively expensive, since the sentences that contain the target relations
in texts can be very scarce and difficult to find. It is therefore beneficial
to develop an efficient method that can automatically extract training
instances from unlabeled texts for training RE models. Recently, large language
models (LLMs) have been adopted in various natural language processing tasks,
with RE also benefiting from their advances. However, when leveraging LLMs for
RE with predefined relation categories, two key challenges arise. First, in a
multi-class classification setting, LLMs often struggle to comprehensively
capture the semantics of every relation, leading to suboptimal results. Second,
although employing binary classification for each relation individually can
mitigate this issue, it introduces significant computational overhead,
resulting in impractical time complexity for real-world applications.
Therefore, this paper proposes a framework called M-BRe to extract training
instances from unlabeled texts for RE. It utilizes three modules to combine the
advantages of both of the above classification approaches: Relation Grouping,
Relation Extraction, and Label Decision. Extensive experiments confirm its
superior capability in discovering high-quality training samples from unlabeled
texts for RE.

</details>


### [28] [Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts](https://arxiv.org/abs/2509.07755)
*Rochana Prih Hastuti,Rian Adam Rajagede,Mansour Al Ghanim,Mengxin Zheng,Qian Lou*

Main category: cs.CL

TL;DR: 医疗领域水印技术在低熵配环境下存在事实性风险，现有方法会明显抵牢医学事实准确性，需要领域特定的水印方案来保护医疗内容整体性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在医疗等敏感领域的应用，模型流畅性带来了来源和负责性的安全风险。水印技术尽管能够洗洗这些风险，但其在医疗上下文中的可靠性仍未经过测试。

Method: 提出了一个以医疗为中心的评估流程，聚合评估事实准确性和连贯性。使用GPT-Judger和人工验证，引入了事实性加权分数（FWS），这是一个优先考虑事实准确性而非连贯性的复合指标。

Result: 评估结果显示当前的水印方法对医疗事实性造成了实质性的抑刻，熵配移动会降低医疗实体的表达能力。

Conclusion: 这些发现强调了需要具有领域意识的水印方法，以保持医疗内容的整体性。

Abstract: As large language models (LLMs) adapted to sensitive domains such as
medicine, their fluency raises safety risks, particularly regarding provenance
and accountability. Watermarking embeds detectable patterns to mitigate these
risks, yet its reliability in medical contexts remains untested. Existing
benchmarks focus on detection-quality tradeoffs, overlooking factual risks
under low-entropy settings often exploited by watermarking's reweighting
strategy. We propose a medical-focused evaluation workflow that jointly
assesses factual accuracy and coherence. Using GPT-Judger and further human
validation, we introduce the Factuality-Weighted Score (FWS), a composite
metric prioritizing factual accuracy beyond coherence to guide watermarking
deployment in medical domains. Our evaluation shows current watermarking
methods substantially compromise medical factuality, with entropy shifts
degrading medical entity representation. These findings underscore the need for
domain-aware watermarking approaches that preserve the integrity of medical
content.

</details>


### [29] [Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning](https://arxiv.org/abs/2509.07768)
*Michele Joshua Maggini,Dhia Merzougui,Rabiraj Bandyopadhyay,Gaël Dias,Fabrice Maurel,Pablo Gamallo*

Main category: cs.CL

TL;DR: 本研究全面评估了大语言模型在不同范式下检测虚假新闻、有害内容和政治偏见的性能，涵盖10个数据集和5种语言，发现微调方法通常优于上下文学习。


<details>
  <summary>Details</summary>
Motivation: 在线平台上虚假新闻、极化内容和有害信息的传播已成为严重问题，但目前缺乏对大语言模型在不同模型、使用方法和语言下性能的系统性基准测试。

Method: 使用10个数据集和5种语言（英语、西班牙语、葡萄牙语、阿拉伯语和保加利亚语），测试了参数高效微调和多种上下文学习策略，包括零样本提示、代码本、少样本学习（随机选择和多样性选择）以及思维链。

Result: 上下文学习通常表现不如微调方法，即使是较小的微调模型也能胜过最大的上下文学习模型（如LlaMA3.1-8b-Instruct、Mistral-Nemo-Instruct-2407和Qwen2.5-7B-Instruct）。

Conclusion: 研究强调了在特定任务设置中对模型进行微调的重要性，即使与最大的上下文学习模型相比，微调较小的模型也能获得更好的性能。

Abstract: The spread of fake news, polarizing, politically biased, and harmful content
on online platforms has been a serious concern. With large language models
becoming a promising approach, however, no study has properly benchmarked their
performance across different models, usage methods, and languages. This study
presents a comprehensive overview of different Large Language Models adaptation
paradigms for the detection of hyperpartisan and fake news, harmful tweets, and
political bias. Our experiments spanned 10 datasets and 5 different languages
(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and
multiclass classification scenarios. We tested different strategies ranging
from parameter efficient Fine-Tuning of language models to a variety of
different In-Context Learning strategies and prompts. These included zero-shot
prompts, codebooks, few-shot (with both randomly-selected and
diversely-selected examples using Determinantal Point Processes), and
Chain-of-Thought. We discovered that In-Context Learning often underperforms
when compared to Fine-Tuning a model. This main finding highlights the
importance of Fine-Tuning even smaller models on task-specific settings even
when compared to the largest models evaluated in an In-Context Learning setup -
in our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and
Qwen2.5-7B-Instruct.

</details>


### [30] [SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP](https://arxiv.org/abs/2509.07801)
*Decheng Duan,Yingyi Zhang,Jitong Peng,Chengzhi Zhang*

Main category: cs.CL

TL;DR: SciNLP是一个专门为NLP领域设计的全文本实体和关系抽取基准数据集，包含60篇人工标注的完整NLP论文，涵盖7072个实体和1826个关系，用于支持知识图谱构建和下游应用。


<details>
  <summary>Details</summary>
Motivation: 现有数据集大多只关注特定论文章节，由于领域复杂性和标注成本高，缺乏完整的全文本标注。为了解决这一限制，需要专门针对NLP领域的全文本实体关系抽取基准。

Method: 构建包含60篇NLP领域全文本论文的手工标注数据集，涵盖7072个实体和1826个关系。使用最先进的监督模型进行对比实验，并基于SciNLP训练模型自动构建细粒度NLP知识图谱。

Result: 实验显示现有模型在不同长度学术文本上的抽取能力存在差异。与现有数据集相比，SciNLP在某些基线模型上实现了显著性能提升。构建的知识图谱平均节点度为3.2，表明具有丰富的语义拓扑信息。

Conclusion: SciNLP是首个提供NLP领域全文本实体关系标注的数据集，有效支持了领域知识图谱的自动构建，并为下游应用提供了丰富的语义信息。数据集已公开可用。

Abstract: Structured information extraction from scientific literature is crucial for
capturing core concepts and emerging trends in specialized fields. While
existing datasets aid model development, most focus on specific publication
sections due to domain complexity and the high cost of annotating scientific
texts. To address this limitation, we introduce SciNLP - a specialized
benchmark for full-text entity and relation extraction in the Natural Language
Processing (NLP) domain. The dataset comprises 60 manually annotated full-text
NLP publications, covering 7,072 entities and 1,826 relations. Compared to
existing research, SciNLP is the first dataset providing full-text annotations
of entities and their relationships in the NLP domain. To validate the
effectiveness of SciNLP, we conducted comparative experiments with similar
datasets and evaluated the performance of state-of-the-art supervised models on
this dataset. Results reveal varying extraction capabilities of existing models
across academic texts of different lengths. Cross-comparisons with existing
datasets show that SciNLP achieves significant performance improvements on
certain baseline models. Using models trained on SciNLP, we implemented
automatic construction of a fine-grained knowledge graph for the NLP domain.
Our KG has an average node degree of 3.2 per entity, indicating rich semantic
topological information that enhances downstream applications. The dataset is
publicly available at https://github.com/AKADDC/SciNLP.

</details>


### [31] [Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems](https://arxiv.org/abs/2509.07817)
*Xiaolin Chen,Xuemeng Song,Haokun Wen,Weili Guan,Xiangyu Zhao,Liqiang Nie*

Main category: cs.CL

TL;DR: 本文提出DK2R方法，利用大语言模型整合结构化属性和非结构化评论知识，通过两阶段推理机制提升多模态任务导向对话系统的文本响应生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个主要局限：1）忽视非结构化评论知识；2）未充分利用大语言模型。作者希望充分利用双重知识（结构化属性和非结构化评论知识）结合LLMs来提升多模态任务导向对话系统中的文本响应生成。

Method: 提出DK2R方法：1）从外部知识库提取结构化属性和非结构化评论知识；2）使用LLM评估每种知识类型的效用；3）通过专用推理分别总结意图导向的关键线索；4）将这些线索作为辅助信号增强基于LLM的文本响应生成。

Result: 在公共数据集上的大量实验验证了DK2R的优越性。

Conclusion: DK2R通过有效整合双重知识和两阶段推理机制，显著提升了多模态任务导向对话系统的文本响应生成性能。

Abstract: Textual response generation is pivotal for multimodal \mbox{task-oriented}
dialog systems, which aims to generate proper textual responses based on the
multimodal context. While existing efforts have demonstrated remarkable
progress, there still exist the following limitations: 1) \textit{neglect of
unstructured review knowledge} and 2) \textit{underutilization of large
language models (LLMs)}. Inspired by this, we aim to fully utilize dual
knowledge (\textit{i.e., } structured attribute and unstructured review
knowledge) with LLMs to promote textual response generation in multimodal
task-oriented dialog systems. However, this task is non-trivial due to two key
challenges: 1) \textit{dynamic knowledge type selection} and 2)
\textit{intention-response decoupling}. To address these challenges, we propose
a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for
multimodal dialog systems (named DK2R). To be specific, DK2R first extracts
both structured attribute and unstructured review knowledge from external
knowledge base given the dialog context. Thereafter, DK2R uses an LLM to
evaluate each knowledge type's utility by analyzing LLM-generated provisional
probe responses. Moreover, DK2R separately summarizes the intention-oriented
key clues via dedicated reasoning, which are further used as auxiliary signals
to enhance LLM-based textual response generation. Extensive experiments
conducted on a public dataset verify the superiority of DK2R. We have released
the codes and parameters.

</details>


### [32] [Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost](https://arxiv.org/abs/2509.07829)
*Mihai Nadas,Laura Diosan,Andreea Tomescu,Andrei Piscoran*

Main category: cs.CL

TL;DR: TF2是一个用于英罗文学翻译的统一框架，包含数据集创建、微调和评估，发布了12B参数的开源模型和300万句的大规模合成平行语料库，在保持竞争力的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决小规模开源模型在文学翻译这一复杂任务中的性能问题，为低资源语言（如罗马尼亚语）提供高质量文学数据集和高效的翻译解决方案。

Method: 采用两阶段微调流程：先进行指令调优以捕捉特定体裁的叙事风格，然后使用适配器压缩实现高效部署；结合BLEU和基于LLM的五维度评估标准（准确性、流畅性、连贯性、风格、文化适应性）。

Result: 微调后的模型在流畅性和充分性方面与顶级专有大模型竞争，同时具有开源、易获取和显著成本效益的优势。

Conclusion: TF2提供了一个端到端、可复现的流程，用于研究成本效益高的翻译、跨语言叙事生成，以及在低资源环境中广泛采用开源模型处理具有文化意义的文学内容。

Abstract: Literary translation has recently gained attention as a distinct and complex
task in machine translation research. However, the translation by small open
models remains an open problem. We contribute to this ongoing research by
introducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for
dataset creation, fine tuning, and evaluation in English-Romanian literary
translations, centred on the creation and open release of both a compact, fine
tuned language model (TF2-12B) and large scale synthetic parallel datasets
(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the
largest collection of synthetic English fables to date, we address the need for
rich, high quality literary datasets in low resource languages such as
Romanian. Our pipeline first generates 15k high quality Romanian references
from the TF1 pool using a high performing LLM. We then apply a two stage fine
tuning process to a 12B parameter open weight model: (i) instruction tuning to
capture genre specific narrative style, and (ii) adapter compression for
efficient deployment. Evaluation combines corpus level BLEU and a five
dimension LLM based rubric (accuracy, fluency, coherence, style, cultural
adaptation) to provide a nuanced assessment of translation quality. Results
show that our fine tuned model achieves fluency and adequacy competitive with
top performing large proprietary models, while being open, accessible, and
significantly more cost effective. Alongside the fine tuned model and both
datasets, we publicly release all scripts and evaluation prompts. TF2 thus
provides an end-to-end, reproducible pipeline for research on cost efficient
translation, cross lingual narrative generation, and the broad adoption of open
models for culturally significant literary content in low resource settings.

</details>


### [33] [Are Humans as Brittle as Large Language Models?](https://arxiv.org/abs/2509.07869)
*Jiahui Li,Sean Papay,Roman Klinger*

Main category: cs.CL

TL;DR: 本文比较了大型语言模型和人类标注者在提示修改下的不稳定性，发现两者都对特定类型的提示变化敏感，但人类对拼写错误和标签顺序反转的影响较小。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型输出不稳定性的根源，探索提示脆弱性是否是人类标注固有的特性，还是LLM特有的问题。

Method: 通过文本分类任务，对LLM和人类标注者使用相同的指令修改，系统比较两者对提示变化的敏感性。

Result: 人类和LLM都对标签集替换和标签格式变化表现出脆弱性，但人类对拼写错误和标签顺序反转的敏感性低于LLM。

Conclusion: 提示脆弱性部分反映了人类标注的固有方差，但LLM在某些方面的敏感性超出了人类水平，需要针对性改进。

Abstract: The output of large language models (LLM) is unstable, due to both
non-determinism of the decoding process as well as to prompt brittleness. While
the intrinsic non-determinism of LLM generation may mimic existing uncertainty
in human annotations through distributional shifts in outputs, it is largely
assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.
This raises the question: do human annotators show similar sensitivity to
instruction changes? If so, should prompt brittleness in LLMs be considered
problematic? One may alternatively hypothesize that prompt brittleness
correctly reflects human annotation variances. To fill this research gap, we
systematically compare the effects of prompt modifications on LLMs and
identical instruction modifications for human annotators, focusing on the
question of whether humans are similarly sensitive to prompt perturbations. To
study this, we prompt both humans and LLMs for a set of text classification
tasks conditioned on prompt variations. Our findings indicate that both humans
and LLMs exhibit increased brittleness in response to specific types of prompt
modifications, particularly those involving the substitution of alternative
label sets or label formats. However, the distribution of human judgments is
less affected by typographical errors and reversed label order than that of
LLMs.

</details>


### [34] [From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing](https://arxiv.org/abs/2509.07889)
*Chengyan Wu,Yiqiang Cai,Yufei Cheng,Yun Xue*

Main category: cs.CL

TL;DR: 本文介绍了NLPCC-2025共享任务7的解决方案，使用LoRA微调大语言模型进行中文句子级性别偏见检测和缓解，通过数据平衡、多模型集成和多温度采样机制提升性能，最终获得47.90%的平均分排名第四。


<details>
  <summary>Details</summary>
Motivation: 促进自然语言生成的公平性和可控性，自动检测、分类和缓解中文文本中的性别偏见问题。

Method: 采用基于大语言模型的LoRA微调方法，构建平衡训练集缓解类别不平衡，引入多源异构样本增强泛化能力，使用多数投票策略集成多个专家模型，设计多温度采样机制捕捉偏见表达风格变化。

Result: 实验结果表明该方法在偏见检测、分类和缓解方面有效，最终获得47.90%的平均分，在共享任务中排名第四。

Conclusion: 提出的基于LLM的LoRA微调方法结合数据平衡、模型集成和采样机制，能够有效处理中文性别偏见检测和缓解任务，为促进NLP公平性提供了可行方案。

Abstract: This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which
focuses on sentence-level gender bias detection and mitigation in Chinese. The
task aims to promote fairness and controllability in natural language
generation by automatically detecting, classifying, and mitigating gender bias.
To address this challenge, we adopt a fine-tuning approach based on large
language models (LLMs), efficiently adapt to the bias detection task via
Low-Rank Adaptation (LoRA). In terms of data processing, we construct a more
balanced training set to alleviate class imbalance and introduce heterogeneous
samples from multiple sources to enhance model generalization. For the
detection and classification sub-tasks, we employ a majority voting strategy
that integrates outputs from multiple expert models to boost performance.
Additionally, to improve bias generation detection and mitigation, we design a
multi-temperature sampling mechanism to capture potential variations in bias
expression styles. Experimental results demonstrate the effectiveness of our
approach in bias detection, classification, and mitigation. Our method
ultimately achieves an average score of 47.90%, ranking fourth in the shared
task.

</details>


### [35] [Biased Tales: Cultural and Topic Bias in Generating Children's Stories](https://arxiv.org/abs/2509.07908)
*Donya Rooein,Vilém Zouhar,Debora Nozza,Dirk Hovy*

Main category: cs.CL

TL;DR: Biased Tales数据集分析发现LLM生成的故事中存在显著的性别和文化偏见，女孩主角的外貌相关属性比男孩多55.26%，非西方儿童故事过度强调文化遗产和家庭主题


<details>
  <summary>Details</summary>
Motivation: 随着家长越来越多地使用大型语言模型生成睡前故事，这些叙事中存在的文化和性别刻板印象引发了严重担忧，需要系统分析这些偏见

Method: 创建Biased Tales数据集，分析LLM生成故事中主角属性和故事元素如何受到偏见影响，比较不同性别和文化背景主角的故事特征

Result: 发现显著差异：女孩主角外貌相关属性比男孩多55.26%；非西方儿童故事过度强调文化遗产、传统和家庭主题，远超西方儿童故事

Conclusion: 研究结果强调了社会文化偏见在使创意AI使用更加公平和多样化方面的重要作用，需要关注和解决LLM生成内容中的偏见问题

Abstract: Stories play a pivotal role in human communication, shaping beliefs and
morals, particularly in children. As parents increasingly rely on large
language models (LLMs) to craft bedtime stories, the presence of cultural and
gender stereotypes in these narratives raises significant concerns. To address
this issue, we present Biased Tales, a comprehensive dataset designed to
analyze how biases influence protagonists' attributes and story elements in
LLM-generated stories. Our analysis uncovers striking disparities. When the
protagonist is described as a girl (as compared to a boy), appearance-related
attributes increase by 55.26%. Stories featuring non-Western children
disproportionately emphasize cultural heritage, tradition, and family themes
far more than those for Western children. Our findings highlight the role of
sociocultural bias in making creative AI use more equitable and diverse.

</details>


### [36] [GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models](https://arxiv.org/abs/2509.07925)
*Tuo Wang,Adithya Kulkarni,Tyler Cody,Peter A. Beling,Yujun Yan,Dawei Zhou*

Main category: cs.CL

TL;DR: GENUINE是一个基于图结构的LLM不确定性估计框架，通过依赖解析树和分层图池化来改进不确定性量化，相比语义熵方法AUROC提升29%，校准误差降低15%


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视语义依赖关系，仅依赖词级概率度量，无法捕捉生成文本中的结构关系，需要更可靠的不确定性估计方法

Method: 利用依赖解析树和分层图池化构建结构感知框架，结合监督学习建模语义和结构关系

Result: 在NLP任务中，AUROC比语义熵方法高29%，校准误差降低超过15%

Conclusion: 图基不确定性建模有效提升LLM可靠性，特别是在高风险应用中

Abstract: Uncertainty estimation is essential for enhancing the reliability of Large
Language Models (LLMs), particularly in high-stakes applications. Existing
methods often overlook semantic dependencies, relying on token-level
probability measures that fail to capture structural relationships within the
generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty
Estimation for Large Language Models, a structure-aware framework that
leverages dependency parse trees and hierarchical graph pooling to refine
uncertainty quantification. By incorporating supervised learning, GENUINE
effectively models semantic and structural relationships, improving confidence
assessments. Extensive experiments across NLP tasks show that GENUINE achieves
up to 29% higher AUROC than semantic entropy-based approaches and reduces
calibration errors by over 15%, demonstrating the effectiveness of graph-based
uncertainty modeling. The code is available at
https://github.com/ODYSSEYWT/GUQ.

</details>


### [37] [SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge](https://arxiv.org/abs/2509.07968)
*Lukas Haas,Gal Yona,Giovanni D'Antonio,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: SimpleQA Verified是一个包含1000个提示的基准测试，用于评估大语言模型的短文本事实性，改进了OpenAI SimpleQA基准的标签噪声、主题偏见和问题冗余等问题。


<details>
  <summary>Details</summary>
Motivation: 解决OpenAI SimpleQA基准测试中存在的标签错误、主题偏见和问题冗余等关键限制，为研究社区提供更可靠的事实性评估工具。

Method: 通过多阶段过滤流程创建基准，包括去重、主题平衡和来源核对，同时改进了自动评分提示。

Result: Gemini 2.5 Pro在新基准上达到55.6的F1分数，优于包括GPT-5在内的其他前沿模型。

Conclusion: 该工作为研究社区提供了更高保真度的工具来跟踪参数模型事实性的真实进展并减少幻觉问题。

Abstract: We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large
Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It
addresses critical limitations in OpenAI's benchmark, including noisy and
incorrect labels, topical biases, and question redundancy. SimpleQA Verified
was created through a rigorous multi-stage filtering process involving
de-duplication, topic balancing, and source reconciliation to produce a more
reliable and challenging evaluation set, alongside improvements in the
autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a
state-of-the-art F1-score of 55.6, outperforming other frontier models,
including GPT-5. This work provides the research community with a
higher-fidelity tool to track genuine progress in parametric model factuality
and to mitigate hallucinations. The benchmark dataset, evaluation code, and
leaderboard are available at:
https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.

</details>


### [38] [Parallel-R1: Towards Parallel Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.07980)
*Tong Zheng,Hongming Zhang,Wenhao Yu,Xiaoyang Wang,Xinyu Yang,Runpeng Dai,Rui Liu,Huiwen Bao,Chengsong Huang,Heng Huang,Dong Yu*

Main category: cs.CL

TL;DR: Parallel-R1是一个强化学习框架，通过渐进式课程训练使大语言模型具备并行思考能力，在数学推理任务上相比顺序思考模型提升8.4%准确率，并可作为中期探索支架带来42.9%的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖监督微调合成数据，鼓励教师强制模仿而非探索和泛化，难以有效训练并行思考能力

Method: 提出首个强化学习框架Parallel-R1，采用渐进式课程：先用SFT在简单任务上培养并行思考能力，然后转向RL在困难问题上探索和泛化该技能

Result: 在MATH、AMC23、AIME等数学基准测试中，相比直接在挑战性任务上用RL训练的序列思考模型，准确率提升8.4%。分析显示模型早期使用并行思考进行探索，后期用于多视角验证

Conclusion: 并行思考可作为中期探索支架，临时探索阶段能在RL后解锁更高性能上限，在AIME25上比基线提升42.9%，证明该方法的有效性

Abstract: Parallel thinking has emerged as a novel approach for enhancing the reasoning
capabilities of large language models (LLMs) by exploring multiple reasoning
paths concurrently. However, activating such capabilities through training
remains challenging, as existing methods predominantly rely on supervised
fine-tuning (SFT) over synthetic data, which encourages teacher-forced
imitation rather than exploration and generalization. Different from them, we
propose \textbf{Parallel-R1}, the first reinforcement learning (RL) framework
that enables parallel thinking behaviors for complex real-world reasoning
tasks. Our framework employs a progressive curriculum that explicitly addresses
the cold-start problem in training parallel thinking with RL. We first use SFT
on prompt-generated trajectories from easier tasks to instill the parallel
thinking ability, then transition to RL to explore and generalize this skill on
harder problems. Experiments on various math benchmarks, including MATH, AMC23,
and AIME, show that Parallel-R1 successfully instills parallel thinking,
leading to 8.4% accuracy improvements over the sequential thinking model
trained directly on challenging tasks with RL. Further analysis reveals a clear
shift in the model's thinking behavior: at an early stage, it uses parallel
thinking as an exploration strategy, while in a later stage, it uses the same
capability for multi-perspective verification. Most significantly, we validate
parallel thinking as a \textbf{mid-training exploration scaffold}, where this
temporary exploratory phase unlocks a higher performance ceiling after RL,
yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and
code will be open-source at https://github.com/zhengkid/Parallel-R1.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Individualized and Interpretable Sleep Forecasting via a Two-Stage Adaptive Spatial-Temporal Model](https://arxiv.org/abs/2509.06974)
*Xueyi Wang,Elisabeth Wilhelm*

Main category: cs.LG

TL;DR: 这篇论文提出了一种可解释的个人化两阶段适配空间-时间模型，用于预测睡眠质量。模型结合多尺度卷积、递归网络、注意力机制和两阶段域适配策略，在多种预测窗口下都超越了现有时间序列预测方法，最佳结果达到RMSE 0.216。


<details>
  <summary>Details</summary>
Motivation: 睡眠质量对健康致关重要，需要可访问和可靠的预测工具来支持预防性干预。目前缺乏能够处理商用可穿戴设备稀疏数据的个性化、适配性强的睡眠预测模型。

Method: 提出了一种可解释的两阶段适配空间-时间模型。等级包括：1) 多尺度卷积层模型多重输入变量的空间相互作用；2) 递归层和注意力机制捕捉长期时间依赖；3) 两阶段域适配策略（训练时适配和港源无需标签的测试时适配）来提升模型的普适性。

Result: 在5种不同输入窗口大小（3-11天）和5种预测窗口大小（1-9天）的实验中，模型均超过了LSTM、Informer、PatchTST和TimesNet等基线方法。最佳性能为3天输入窗口和1天预测窗口（RMSE=0.216），甚至在更长预测期限下也表现良好（3天预测RMSE=0.257）。解释性分析证明了不同特征对睡眠质量的影响。

Conclusion: 该框架为使用商业可穿戴设备的稀疏数据进行个性化睡眠预测提供了一种稳健、适配性强且可解释的解决方案，具有实际应用价值。

Abstract: Sleep quality significantly impacts well-being. Therefore, healthcare
providers and individuals need accessible and reliable forecasting tools for
preventive interventions. This paper introduces an interpretable,
individualized two-stage adaptive spatial-temporal model for predicting sleep
quality scores. Our proposed framework combines multi-scale convolutional
layers to model spatial interactions across multiple input variables, recurrent
layers and attention mechanisms to capture long-term temporal dependencies, and
a two-stage domain adaptation strategy to enhance generalization. The first
adaptation stage is applied during training to mitigate overfitting on the
training set. In the second stage, a source-free test-time adaptation mechanism
is employed to adapt the model to new users without requiring labels. We
conducted various experiments with five input window sizes (3, 5, 7, 9, and 11
days) and five prediction window sizes (1, 3, 5, 7, and 9 days). Our model
consistently outperformed time series forecasting baseline approaches,
including Long Short-Term Memory (LSTM), Informer, PatchTST, and TimesNet. The
best performance was achieved with a three-day input window and a one-day
prediction window, yielding a root mean square error (RMSE) of 0.216.
Furthermore, the model demonstrated good predictive performance even for longer
forecasting horizons (e.g, with a 0.257 RMSE for a three-day prediction
window), highlighting its practical utility for real-world applications. We
also conducted an explainability analysis to examine how different features
influence sleep quality. These findings proved that the proposed framework
offers a robust, adaptive, and explainable solution for personalized sleep
forecasting using sparse data from commercial wearable devices.

</details>


### [40] [GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised Learning](https://arxiv.org/abs/2509.06975)
*Yu Song,Zhigang Hua,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: GSTBench是首个系统评估图自监督学习方法跨数据集迁移能力的基准测试，发现大多数图SSL方法迁移效果不佳，而GraphMAE方法表现最佳


<details>
  <summary>Details</summary>
Motivation: 现有图自监督学习方法主要在单数据集上开发和评估，缺乏对跨数据集迁移性的系统研究，限制了知识迁移和大规模预训练的发展

Method: 在ogbn-papers100M上进行大规模预训练，评估5种代表性SSL方法在多样化目标图上的表现，通过标准化实验设置分离混淆因素

Result: 大多数图SSL方法迁移效果差，有些甚至比随机初始化更差；GraphMAE（掩码自编码器方法）表现稳定且能提升迁移性能

Conclusion: 研究揭示了图SSL方法迁移性的关键因素，为图学习的"预训练-迁移"范式奠定了坚实基础，并指导未来可迁移图SSL研究

Abstract: Self-supervised learning (SSL) has shown great promise in graph
representation learning. However, most existing graph SSL methods are developed
and evaluated under a single-dataset setting, leaving their cross-dataset
transferability largely unexplored and limiting their ability to leverage
knowledge transfer and large-scale pretraining, factors that are critical for
developing generalized intelligence beyond fitting training data. To address
this gap and advance foundation model research for graphs, we present GSTBench,
the first systematic benchmark for evaluating the transferability of graph SSL
methods. We conduct large-scale pretraining on ogbn-papers100M and evaluate
five representative SSL methods across a diverse set of target graphs. Our
standardized experimental setup decouples confounding factors such as model
architecture, dataset characteristics, and adaptation protocols, enabling
rigorous comparisons focused solely on pretraining objectives. Surprisingly, we
observe that most graph SSL methods struggle to generalize, with some
performing worse than random initialization. In contrast, GraphMAE, a masked
autoencoder approach, consistently improves transfer performance. We analyze
the underlying factors that drive these differences and offer insights to guide
future research on transferable graph SSL, laying a solid foundation for the
"pretrain-then-transfer" paradigm in graph learning. Our code is available at
https://github.com/SongYYYY/GSTBench.

</details>


### [41] [A Knowledge-Guided Cross-Modal Feature Fusion Model for Local Traffic Demand Prediction](https://arxiv.org/abs/2509.06976)
*Lingyu Zhang,Pengfei Xu,Guobin Wu,Jian Liang,Ruiyang Dong,Yunhai Wang,Xuan Song*

Main category: cs.LG

TL;DR: 提出了一种知识引导的跨模态特征表示学习模型(KGCM)，通过结合结构化交通数据和文本知识数据来提升交通需求预测精度


<details>
  <summary>Details</summary>
Motivation: 现有交通预测模型主要依赖时序数据，缺乏对人类知识和经验的利用。现实中交通知识和经验对精确预测有重要影响，可以指导模型发现数据中的潜在模式

Method: 构建先验知识数据集，使用大语言模型结合人工编写修订；设计局部和全局自适应图网络学习多模态特征；提出基于推理的动态更新策略优化图模型参数

Result: 在多个交通数据集上的实验表明，该模型能准确预测未来交通需求，性能优于现有最先进模型

Conclusion: KGCM模型成功整合了结构化交通数据和人类知识经验，通过跨模态特征学习和动态优化策略，显著提升了交通需求预测的准确性和鲁棒性

Abstract: Traffic demand prediction plays a critical role in intelligent transportation
systems. Existing traffic prediction models primarily rely on temporal traffic
data, with limited efforts incorporating human knowledge and experience for
urban traffic demand forecasting. However, in real-world scenarios, traffic
knowledge and experience derived from human daily life significantly influence
precise traffic prediction. Such knowledge and experiences can guide the model
in uncovering latent patterns within traffic data, thereby enhancing the
accuracy and robustness of predictions. To this end, this paper proposes
integrating structured temporal traffic data with textual data representing
human knowledge and experience, resulting in a novel knowledge-guided
cross-modal feature representation learning (KGCM) model for traffic demand
prediction. Based on regional transportation characteristics, we construct a
prior knowledge dataset using a large language model combined with manual
authoring and revision, covering both regional and global knowledge and
experiences. The KGCM model then learns multimodal data features through
designed local and global adaptive graph networks, as well as a cross-modal
feature fusion mechanism. A proposed reasoning-based dynamic update strategy
enables dynamic optimization of the graph model's parameters, achieving optimal
performance. Experiments on multiple traffic datasets demonstrate that our
model accurately predicts future traffic demand and outperforms existing
state-of-the-art (SOTA) models.

</details>


### [42] [Toward Reproducible Cross-Backend Compatibility for Deep Learning: A Configuration-First Framework with Three-Tier Verification](https://arxiv.org/abs/2509.06977)
*Zehua Li*

Main category: cs.LG

TL;DR: 这篇论文提出了一个配置先行框架，用于评估深度学习系统在CPU、GPU和编译运行时之间的跨后端兼容性问题，通过系统化方法量化和缓解跨平台偏移。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在不同运行时环境（CPU、GPU、编译后端）下存在输出不一致问题，影响部署的可靠性和可复现性，需要系统化的评估方法。

Method: 采用YAML配置文件解耦实验与代码，支持库和仓库模型，使用三层验证协议（张量级接近度、激活对齐、任务级指标）进行评估。

Result: 通过672次检查发现，72.0%的运行通过检测，大多数差异出现在更严格的容差阈值下。检测模型和编译后端更容易出现偏移，主要原因是非确定性后处理。

Conclusion: 确定性适配器和选择性回退机制可以显著提高一致性而不伤害性能。该框架是首个统一的系统化方法，能够量化和缓解深度学习中的跨后端偏移问题，为异构运行时环境提供可靠的部署方案。

Abstract: This paper presents a configuration-first framework for evaluating
cross-backend compatibility in deep learning systems deployed on CPU, GPU, and
compiled runtimes. The framework decouples experiments from code using YAML,
supports both library and repository models, and employs a three-tier
verification protocol covering tensor-level closeness, activation alignment,
and task-level metrics. Through 672 checks across multiple models and tolerance
settings, we observe that 72.0% of runs pass, with most discrepancies occurring
under stricter thresholds. Our results show that detection models and compiled
backends are particularly prone to drift, often due to nondeterministic
post-processing. We further demonstrate that deterministic adapters and
selective fallbacks can substantially improve agreement without significant
performance loss. To our knowledge, this is the first unified framework that
systematically quantifies and mitigates cross-backend drift in deep learning,
providing a reproducible methodology for dependable deployment across
heterogeneous runtimes.

</details>


### [43] [A Kriging-HDMR-based surrogate model with sample pool-free active learning strategy for reliability analysis](https://arxiv.org/abs/2509.06978)
*Wenxiong Li,Hanyu Liao,Suiyin Chen*

Main category: cs.LG

TL;DR: 提出了一种基于Kriging-HDMR建模的主动学习代理模型方法，用于解决高维可靠性分析问题，通过构建多个低维子代理模型的复合表示来近似高维极限状态函数。


<details>
  <summary>Details</summary>
Motivation: 传统代理模型在高维随机变量下遭遇"维度灾难"，而现有的高维模型表示方法主要关注优化问题，缺乏专门针对可靠性分析的研究，可靠性分析更关注关键区域的预测精度而非整个域的均匀精度。

Method: 采用三阶段框架：1)为所有随机变量开发单变量子代理模型；2)识别耦合变量子代理模型的需求；3)构建耦合变量子代理模型。基于各阶段特点制定实验设计样本选择的优化数学模型，采用无候选样本池方法选择信息丰富的样本。

Result: 数值实验表明，所提方法在解决高维可靠性问题时实现了高计算效率，同时保持了强大的预测准确性。

Conclusion: 该方法有效解决了高维可靠性分析中的维度灾难问题，通过分阶段构建复合代理模型的方式，在保证精度的同时显著提高了计算效率。

Abstract: In reliability engineering, conventional surrogate models encounter the
"curse of dimensionality" as the number of random variables increases. While
the active learning Kriging surrogate approaches with high-dimensional model
representation (HDMR) enable effective approximation of high-dimensional
functions and are widely applied to optimization problems, there are rare
studies specifically focused on reliability analysis, which prioritizes
prediction accuracy in critical regions over uniform accuracy across the entire
domain. This study develops an active learning surrogate model method based on
the Kriging-HDMR modeling for reliability analysis. The proposed approach
facilitates the approximation of high-dimensional limit state functions through
a composite representation constructed from multiple low-dimensional
sub-surrogate models. The architecture of the surrogate modeling framework
comprises three distinct stages: developing single-variable sub-surrogate
models for all random variables, identifying the requirements for
coupling-variable sub-surrogate models, and constructing the coupling-variable
sub-surrogate models. Optimization mathematical models for selection of design
of experiment samples are formulated based on each stage's characteristics,
with objectives incorporating uncertainty variance, predicted mean, sample
location and inter-sample distances. A candidate sample pool-free approach is
adopted to achieve the selection of informative samples. Numerical experiments
demonstrate that the proposed method achieves high computational efficiency
while maintaining strong predictive accuracy in solving high-dimensional
reliability problems.

</details>


### [44] [Exploring Over-stationarization in Deep Learning-based Bus/Tram Arrival Time Prediction: Analysis and Non-stationary Effect Recovery](https://arxiv.org/abs/2509.06979)
*Zirui Li,Bin Yang,Meng Wang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的公共交通到站时间多步预测方法NSATP，通过平衡预测性和非稳态性来避免过度稳态化问题，在德納斯顿公交数据上取得了更好的预测精度。


<details>
  <summary>Details</summary>
Motivation: 多步到站时间预测中，非稳态数据会降低模型性能，而传统的标准化方法可能导致过度稳态化，隐藏了非稳态性中的有用特征。

Method: 提出NSATP方法，包含两个阶段：时间序列稳态化和非稳态性效应恢复。第一阶段提高预测性，第二阶段将一维模型扩展到二维来捕捉隐藏周期性，并设计补偿模块学习缩放和偏移因子。

Result: 在德納斯顿125天公交运营数据上验证，NSATP方法与基线方法相比，在有轨电车和公交车上分别降低了RMSE、MAE和MAPE指标，最大改善达2.37%。

Conclusion: NSATP方法能够有效平衡预测性和非稳态性的关系，避免了过度稳态化问题，在公共交通到站时间预测任务中表现出更好的性能。

Abstract: Arrival time prediction (ATP) of public transport vehicles is essential in
improving passenger experience and supporting traffic management. Deep learning
has demonstrated outstanding performance in ATP due to its ability to model
non-linear and temporal dynamics. In the multi-step ATP, non-stationary data
will degrade the model performance due to the variation in variables' joint
distribution along the temporal direction. Previous studies mainly applied
normalization to eliminate the non-stationarity in time series, thereby
achieving better predictability. However, the normalization may obscure useful
characteristics inherent in non-stationarity, which is known as the
over-stationarization. In this work, to trade off predictability and
non-stationarity, a new approach for multi-step ATP, named non-stationary ATP (
NSATP), is proposed. The method consists of two stages: series stationarization
and non-stationarity effect recovery. The first stage aims at improving the
predictability. As for the latter, NSATP extends a state-of-the-art method from
one-dimensional to two dimensional based models to capture the hidden
periodicity in time series and designs a compensation module of
over-stationarization by learning scaling and shifting factors from raw data.
125 days' public transport operational data of Dresden is collected for
validation. Experimental results show that compared to baseline methods, the
proposed NSATP can reduce RMSE, MAE, and MAPE by 2.37%, 1.22%, and 2.26% for
trams and by 1.72%, 0.60%, and 1.17% for buses, respectively.

</details>


### [45] [RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use](https://arxiv.org/abs/2509.06980)
*Jiajun Chai,Guojun Yin,Zekun Xu,Chuhuai Yue,Yi Jia,Siyu Xia,Xiaohan Wang,Jiwen Jiang,Xiaoguang Li,Chengqi Dong,Hang He,Wei Lin*

Main category: cs.LG

TL;DR: RLFactory是一个即插即用的强化学习后训练框架，通过异步调用器和解耦架构解决多轮工具使用的稳定性和适应性，在Search-R1基准上超越更大模型表现


<details>
  <summary>Details</summary>
Motivation: 大语言模型擅长基础推理但在需要与外部工具交互的任务上表现不佳，需要解决工具调用的稳定性和异构工具接口问题

Method: 使用基于asyncio的异步调用器和解耦工具/训练架构，引入观察标记重构MDP，实现生成-解析-调用-更新的动态策略优化工作流

Result: 在Qwen3-4B模型上，Natural Questions数据集测试得分0.486，超越类似技术训练的更大模型，训练吞吐量提升6.8倍

Conclusion: RLFactory提供了一个低门槛、高适应性的框架，可有效增强LLM在真实场景中的多轮工具使用能力

Abstract: Large language models excel at basic reasoning but struggle with tasks that
require interaction with external tools. We present RLFactory, a plug-and-play
reinforcement learning post-training framework for multi-round tool use.
RLFactory tackles (i) tool-call stability and adaptability amid tool
heterogeneity and interface issues via an asyncio-based asynchronous caller and
a decoupled tool/training architecture, and (ii) diverse evaluation needs via a
reward layer supporting rule-based, model-judgment, and tool-verification
signals. It reconstructs the MDP by introducing observation markers from tool
feedback, closing the loop among model, tools, and environment, and implements
a generate-parse-invoke-update workflow for dynamic policy optimization. On
Search-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural
Questions (NQ) dataset, surpassing larger models trained with similar
techniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training
throughput by 6.8x. RLFactory provides a low-barrier, highly adaptable
framework for strengthening multi-round tool use of LLMs in real-world
scenarios. Code: https://github.com/Simple-Efficient/RL-Factory.

</details>


### [46] [CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention](https://arxiv.org/abs/2509.06982)
*Xiaomeng Hu,Fei Huang,Chenhan Yuan,Junyang Lin,Tsung-Yi Ho*

Main category: cs.LG

TL;DR: CARE框架通过实时安全监控、回滚机制和自省干预策略，在解码阶段实现安全对齐，在安全性和响应质量之间取得更好平衡


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中部署增多，确保解码时输出安全成为关键挑战，现有方法往往在安全性和响应质量之间需要严重权衡

Method: 提出CARE框架，包含三个核心组件：(1)实时安全监控的守卫模型；(2)带令牌缓冲区的回滚机制；(3)基于自省的干预策略，模型生成对先前输出的自我反思来指导后续解码

Result: 实验结果表明该框架在安全性、质量和效率方面达到优越平衡，有害响应率低，对用户体验干扰最小，同时保持高响应质量

Conclusion: CARE框架通过精确干预、及时纠正和有效自校正，实现了解码时安全对齐的优越安全-质量权衡

Abstract: As large language models (LLMs) are increasingly deployed in real-world
applications, ensuring the safety of their outputs during decoding has become a
critical challenge. However, existing decoding-time interventions, such as
Contrastive Decoding, often force a severe trade-off between safety and
response quality. In this work, we propose CARE, a novel framework for
decoding-time safety alignment that integrates three key components: (1) a
guard model for real-time safety monitoring, enabling detection of potentially
unsafe content; (2) a rollback mechanism with a token buffer to correct unsafe
outputs efficiently at an earlier stage without disrupting the user experience;
and (3) a novel introspection-based intervention strategy, where the model
generates self-reflective critiques of its previous outputs and incorporates
these reflections into the context to guide subsequent decoding steps. The
framework achieves a superior safety-quality trade-off by using its guard model
for precise interventions, its rollback mechanism for timely corrections, and
our novel introspection method for effective self-correction. Experimental
results demonstrate that our framework achieves a superior balance of safety,
quality, and efficiency, attaining a low harmful response rate and minimal
disruption to the user experience while maintaining high response quality.

</details>


### [47] [FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under Missing Modalities](https://arxiv.org/abs/2509.06984)
*Lishan Yang,Nam Kha Nguygen,Po Hu,Wei Emma Zhang,Yanjun Shu,Mong Yuan Sim,Weitong Chen*

Main category: cs.LG

TL;DR: FediLoRA是一个联邦多模态微调框架，解决异构LoRA秩和缺失模态问题，通过维度聚合和模型编辑提升性能


<details>
  <summary>Details</summary>
Motivation: 基础模型参数量大，部署困难。现有联邦LoRA方法假设统一秩配置和单模态输入，忽略了客户端资源异构性和多模态数据缺失的现实挑战

Method: 提出维度聚合策略重新加权LoRA更新，避免信息稀释；采用轻量级分层模型编辑方法选择性整合全局参数修复本地组件

Result: 在三个多模态基准数据集上实验表明，FediLoRA在全局和个性化设置中均优于基线方法，特别是在模态不完整情况下表现优异

Conclusion: FediLoRA有效解决了联邦学习中异构LoRA秩和缺失模态问题，为实际部署提供了简单而有效的解决方案

Abstract: Foundation models have demonstrated remarkable performance across a wide
range of tasks, yet their large parameter sizes pose challenges for practical
deployment, especially in decentralized environments. Parameter-efficient
fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), reduces local computing
and memory overhead, making it attractive for federated learning. However,
existing federated LoRA methods typically assume uniform rank configurations
and unimodal inputs, overlooking two key real-world challenges: (1)
heterogeneous client resources have different LoRA ranks, and (2) multimodal
data settings with potentially missing modalities. In this work, we propose
FediLoRA, a simple yet effective framework for federated multimodal fine-tuning
under heterogeneous LoRA ranks and missing modalities. FediLoRA introduces a
dimension-wise aggregation strategy that reweights LoRA updates without
information dilution during aggregation. It also includes a lightweight
layer-wise model editing method that selectively incorporates global parameters
to repair local components which improves both client and global model
performances. Experimental results on three multimodal benchmark datasets
demonstrate that FediLoRA achieves superior performance over competitive
baselines in both global and personalized settings, particularly in the
presence of modality incompleteness.

</details>


### [48] [Machine Generalize Learning in Agent-Based Models: Going Beyond Surrogate Models for Calibration in ABMs](https://arxiv.org/abs/2509.07013)
*Sima Najafzadehkhoei,George Vega Yon,Bernardo Modenesi,Derek S. Meyer*

Main category: cs.LG

TL;DR: 使用双向LSTM网络学习逆向映射来快速检定代理基于模型，比传统方法更准确更快速


<details>
  <summary>Details</summary>
Motivation: 代理基于流行病模型的检定过程计算成本高，需要更高效的方法

Method: 使用三层双向LSTM网络，输入60天发病率、人口规模和恢复率，输出传播概率、接触率和R0，加入流行病学一致性惩罚

Result: 在1000个场景模拟中，方法比ABC方法锐减错误（MAE：R0 0.0616 vs 0.275），提供更紧凑的预测区间，并将检定时间从77.4秒缩短到2.35秒

Conclusion: 该机器学习检定器虽然面临参数识别性问题，但能更准确地重现流行曲线，实现快速实用的模型检定

Abstract: Calibrating agent-based epidemic models is computationally demanding. We
present a supervised machine learning calibrator that learns the inverse
mapping from epidemic time series to SIR parameters. A three-layer
bidirectional LSTM ingests 60-day incidence together with population size and
recovery rate, and outputs transmission probability, contact rate, and R0.
Training uses a composite loss with an epidemiology-motivated consistency
penalty that encourages R0 \* recovery rate to equal transmission probability
\* contact rate.
  In a 1000-scenario simulation study, we compare the calibrator with
Approximate Bayesian Computation (likelihood-free MCMC). The method achieves
lower error across all targets (MAE: R0 0.0616 vs 0.275; transmission 0.0715 vs
0.128; contact 1.02 vs 4.24), produces tighter predictive intervals with near
nominal coverage, and reduces wall clock time from 77.4 s to 2.35 s per
calibration. Although contact rate and transmission probability are partially
nonidentifiable, the approach reproduces epidemic curves more faithfully than
ABC, enabling fast and practical calibration. We evaluate it on SIR agent based
epidemics generated with epiworldR and provide an implementation in R.

</details>


### [49] [An efficient deep reinforcement learning environment for flexible job-shop scheduling](https://arxiv.org/abs/2509.07019)
*Xinquan Wu,Xuefeng Yan,Mingqiang Wei,Donghai Guan*

Main category: cs.LG

TL;DR: 本文提出了一个基于离散事件模拟的简单时序DRL环境用于柔性作业车间调度问题，并基于PPO算法构建端到端DRL调度模型，通过新的状态表示和奖励函数设计，在公开基准测试中取得了有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度强化学习方法主要关注DRL调度智能体的设计，而忽视了DRL环境的建模。为了解决这个问题，需要为柔性作业车间调度问题构建一个合适的DRL环境。

Method: 1) 基于离散事件模拟构建简单时序DRL环境；2) 使用近端策略优化(PPO)算法构建端到端DRL调度模型；3) 提出基于两个状态变量的短状态表示方法；4) 设计基于机器调度区域的可理解奖励函数。

Result: 实验结果表明：在提出的调度环境中，简单优先级调度规则的性能得到提升；所提出的DRL调度模型与OR-Tools、元启发式、DRL和PDR调度方法相比，获得了具有竞争力的性能。

Conclusion: 本文成功构建了一个有效的DRL环境用于柔性作业车间调度问题，提出的端到端DRL调度模型在多个基准测试中表现出色，证明了环境建模在DRL调度中的重要性。

Abstract: The Flexible Job-shop Scheduling Problem (FJSP) is a classical combinatorial
optimization problem that has a wide-range of applications in the real world.
In order to generate fast and accurate scheduling solutions for FJSP, various
deep reinforcement learning (DRL) scheduling methods have been developed.
However, these methods are mainly focused on the design of DRL scheduling
Agent, overlooking the modeling of DRL environment. This paper presents a
simple chronological DRL environment for FJSP based on discrete event
simulation and an end-to-end DRL scheduling model is proposed based on the
proximal policy optimization (PPO). Furthermore, a short novel state
representation of FJSP is proposed based on two state variables in the
scheduling environment and a novel comprehensible reward function is designed
based on the scheduling area of machines. Experimental results on public
benchmark instances show that the performance of simple priority dispatching
rules (PDR) is improved in our scheduling environment and our DRL scheduling
model obtains competing performance compared with OR-Tools, meta-heuristic, DRL
and PDR scheduling methods.

</details>


### [50] [Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data](https://arxiv.org/abs/2509.07198)
*Yiyue Chen,Usman Akram,Chianing Wang,Haris Vikalo*

Main category: cs.LG

TL;DR: Fed-REACT是一个针对异构和演化客户端数据的联邦学习框架，通过表示学习和进化聚类的两阶段方法提升性能


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端数据分布随时间演化和跨客户端显著差异导致的性能下降问题

Method: 两阶段方法：第一阶段客户端学习本地模型提取特征表示；第二阶段服务器基于表示动态聚类客户端，协调集群级任务特定模型训练

Result: 理论分析表示学习阶段，实证证明在真实数据集上获得优越准确性和鲁棒性

Conclusion: Fed-REACT框架有效解决了联邦学习中的异构性和演化数据问题，提升了模型性能

Abstract: Motivated by the high resource costs and privacy concerns associated with
centralized machine learning, federated learning (FL) has emerged as an
efficient alternative that enables clients to collaboratively train a global
model while keeping their data local. However, in real-world deployments,
client data distributions often evolve over time and differ significantly
across clients, introducing heterogeneity that degrades the performance of
standard FL algorithms. In this work, we introduce Fed-REACT, a federated
learning framework designed for heterogeneous and evolving client data.
Fed-REACT combines representation learning with evolutionary clustering in a
two-stage process: (1) in the first stage, each client learns a local model to
extracts feature representations from its data; (2) in the second stage, the
server dynamically groups clients into clusters based on these representations
and coordinates cluster-wise training of task-specific models for downstream
objectives such as classification or regression. We provide a theoretical
analysis of the representation learning stage, and empirically demonstrate that
Fed-REACT achieves superior accuracy and robustness on real-world datasets.

</details>


### [51] [1 bit is all we need: binary normalized neural networks](https://arxiv.org/abs/2509.07025)
*Eduardo Lobo Lustoda Cabral,Paulo Pirozelli,Larissa Driemeier*

Main category: cs.LG

TL;DR: 一种全部参数仅使用1位(0/1)的新型神经网络层，可将内存需求降低32倍但保持类似性能


<details>
  <summary>Details</summary>
Motivation: 解决大型神经网络模型(如语言模型和图像模型)部署时的内存需求和计算效率挑战

Method: 发明二进制归一化层(binary normalized layers)，所有参数(包括核权重和偏置)均为0或1，支持全连接、卷积、注意力等多种层类型

Result: 在图像分类和语言模型任务上，二进制归一化模型表现几乎与32位实数参数模型相等，内存占用降低32倍

Conclusion: 该新型层为大型神经网络模型提供了内存效率极高的解决方案，可在普通CPU或移动设备上部署，无需专用硬件

Abstract: The increasing size of large neural network models, specifically language
models and foundational image models, poses deployment challenges, prompting
efforts to reduce memory requirements and enhance computational efficiency.
These efforts are critical to ensure practical deployment and effective
utilization of these models across various applications. In this work, a novel
type of neural network layers and models is developed that uses only single-bit
parameters. In this novel type of models all parameters of all layers,
including kernel weights and biases, only have values equal to zero or one.
This novel type of models uses layers named as binary normalized layer. These
binary normalized layers can be of any type, such as fully connected,
convolutional, attention, etc., and they consist of slight variations of the
corresponding conventional layers. To show the effectiveness of the binary
normalized layers, two different models are configured to solve a multiclass
image classification problem and a language decoder to predict the next token
of a sequence. The model to solve the image classification has convolutional
and fully connected layers, and the language model is composed of transformer
blocks with multi-head attention. The results show that models with binary
normalized layers present almost the same results obtained by equivalent models
with real 32-bit parameters. The binary normalized layers allow to develop
models that use 32 times less memory than current models and have equivalent
performance. Besides, the binary normalized layers can be easily implemented on
current computers using 1-bit arrays, and do not require the development of
dedicated electronic hardware. This novel type of layers opens a new era for
large neural network models with reduced memory requirements that can be
deployed using simple and cheap hardware, such as mobile devices or only cpus.

</details>


### [52] [FedTeddi: Temporal Drift and Divergence Aware Scheduling for Timely Federated Edge Learning](https://arxiv.org/abs/2509.07342)
*Yuxuan Bai,Yuxuan Sun,Tan Chen,Wei Chen,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: FedTeddi是一种针对联邦边缘学习中动态非IID数据的时间漂移和散度感知调度算法，通过联合调度和带宽分配实现快速收敛


<details>
  <summary>Details</summary>
Motivation: 现实场景中客户端数据具有时变和非独立同分布特性，现有研究大多假设静态数据集，需要适应动态数据演化的高效模型更新方法

Method: 使用时间漂移和集体散度量化数据动态特性，表示为类别分布的Earth Mover's Distance，提出优化目标并开发联合调度和带宽分配算法

Result: 在CIFAR-10和CIFAR-100上相比随机调度分别提升58.4%和49.2%的收敛速度，获得更高的测试精度

Conclusion: FedTeddi算法能有效处理动态数据演化，实现快速学习新数据同时不遗忘先前知识，在资源受限环境下表现优异

Abstract: Federated edge learning (FEEL) enables collaborative model training across
distributed clients over wireless networks without exposing raw data. While
most existing studies assume static datasets, in real-world scenarios clients
may continuously collect data with time-varying and non-independent and
identically distributed (non-i.i.d.) characteristics. A critical challenge is
how to adapt models in a timely yet efficient manner to such evolving data. In
this paper, we propose FedTeddi, a temporal-drift-and-divergence-aware
scheduling algorithm that facilitates fast convergence of FEEL under dynamic
data evolution and communication resource limits. We first quantify the
temporal dynamics and non-i.i.d. characteristics of data using temporal drift
and collective divergence, respectively, and represent them as the Earth
Mover's Distance (EMD) of class distributions for classification tasks. We then
propose a novel optimization objective and develop a joint scheduling and
bandwidth allocation algorithm, enabling the FEEL system to learn from new data
quickly without forgetting previous knowledge. Experimental results show that
our algorithm achieves higher test accuracy and faster convergence compared to
benchmark methods, improving the rate of convergence by 58.4% on CIFAR-10 and
49.2% on CIFAR-100 compared to random scheduling.

</details>


### [53] [Recursive State Inference for Linear PASFA](https://arxiv.org/abs/2509.07028)
*Vishal Rishi*

Main category: cs.LG

TL;DR: 提出了一种递归扩展的线性PASFA方法，用于从观测数据和模型中进行状态（慢特征）的MMSE估计，解决了现有方法在恢复原始状态方面的困难。


<details>
  <summary>Details</summary>
Motivation: 现有的概率自适应慢特征分析（PASFA）方法将慢特征建模为ARMA过程的状态，但缺乏从观测数据和模型高效推断这些状态的方法。当前方法使用卡尔曼滤波需要将ARMA过程转换为状态空间模型，但难以恢复原始的有用表示状态。

Method: 提出了线性PASFA的递归扩展算法，该算法在给定观测数据和模型的情况下，对按照ARMA过程演化的状态进行最小均方误差（MMSE）估计。

Result: 在合成数据集上评估了所提技术的正确性，验证了方法的有效性。

Conclusion: 该递归扩展方法能够有效解决从观测数据和PASFA模型中推断慢特征状态的问题，为慢特征分析提供了更实用的状态估计工具。

Abstract: Slow feature analysis (SFA), as a method for learning slowly varying features
in classification and signal analysis, has attracted increasing attention in
recent years. Recent probabilistic extensions to SFA learn effective
representations for classification tasks. Notably, the Probabilistic Adaptive
Slow Feature Analysis models the slow features as states in an ARMA process and
estimate the model from the observations. However, there is a need to develop
efficient methods to infer the states (slow features) from the observations and
the model. In this paper, a recursive extension to the linear PASFA has been
proposed. The proposed algorithm performs MMSE estimation of states evolving
according to an ARMA process, given the observations and the model. Although
current methods tackle this problem using Kalman filters after transforming the
ARMA process into a state space model, the original states (or slow features)
that form useful representations cannot be easily recovered. The proposed
technique is evaluated on a synthetic dataset to demonstrate its correctness.

</details>


### [54] [MoE-Compression: How the Compression Error of Experts Affects the Inference Accuracy of MoE Model?](https://arxiv.org/abs/2509.07727)
*Songkai Ma,Zhaorui Zhang,Sheng Di,Benben Liu,Xiaodong Yu,Xiaoyi Lu,Dan Wang*

Main category: cs.LG

TL;DR: 使用误差有界压缩算法压缩MoE模型中的非激活专家，减少GPU内存与主存间的数据传输开销，并分析不同层级专家压缩误差对推理精度的影响。


<details>
  <summary>Details</summary>
Motivation: MoE模型在有限GPU内存下的高效服务面临挑战，需要探索压缩专家参数的方法来减少数据传输开销，同时分析压缩误差对推理性能的影响。

Method: 采用误差有界损失压缩算法（如SZ3和CuSZp）压缩非激活专家，通过大量实验分析不同层级专家压缩误差对整体推理精度的影响。

Result: 浅层专家（负责注意力机制和输入转换）对压缩误差容忍度高；中层专家（负责模型推理）对误差敏感；深层专家（负责指令跟随和输出整合）的误差有时反而能提升推理精度。

Conclusion: 误差有界压缩是减少MoE模型数据传输开销的有效方法，不同层级专家对压缩误差的敏感性不同，需要分层制定压缩策略。

Abstract: With the widespread application of Mixture of Experts (MoE) reasoning models
in the field of LLM learning, efficiently serving MoE models under limited GPU
memory constraints has emerged as a significant challenge. Offloading the
non-activated experts to main memory has been identified as an efficient
approach to address such a problem, while it brings the challenges of
transferring the expert between the GPU memory and main memory. We need to
explore an efficient approach to compress the expert and analyze how the
compression error affects the inference performance.
  To bridge this gap, we propose employing error-bounded lossy compression
algorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby
reducing data transfer overhead during MoE inference. We conduct extensive
experiments across various benchmarks and present a comprehensive analysis of
how compression-induced errors in different experts affect overall inference
accuracy. The results indicate that experts in the shallow layers, which are
primarily responsible for the attention mechanism and the transformation of
input tokens into vector representations, exhibit minimal degradation in
inference accuracy when subjected to bounded errors. In contrast, errors in the
middle-layer experts, which are central to model reasoning, significantly
impair inference accuracy. Interestingly, introducing bounded errors in the
deep-layer experts, which are mainly responsible for instruction following and
output integration, can sometimes lead to improvements in inference accuracy.

</details>


### [55] [A Minimalist Bayesian Framework for Stochastic Optimization](https://arxiv.org/abs/2509.07030)
*Kaizheng Wang*

Main category: cs.LG

TL;DR: 提出了一种简约贝叶斯框架，仅对感兴趣参数（如最优位置）设置先验，通过轮廓似然消除冗余参数，可处理复杂约束。基于此开发了MINTS算法，适用于结构化问题并建立了接近最优的遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯方法需要对所有参数建立概率模型，这阻碍了复杂结构约束的融入。需要一种更简约的方法来处理具有约束的序列决策问题。

Method: 采用简约贝叶斯框架，仅对目标参数设置先验，通过轮廓似然消除冗余参数。开发了MINimalist Thompson Sampling (MINTS)算法，可处理连续臂Lipschitz赌博机和动态定价等结构化问题。

Result: 该框架为经典凸优化算法（如重心法和椭球法）提供了概率视角。对多臂赌博机问题的MINTS算法分析显示其具有接近最优的遗憾保证。

Conclusion: 提出的简约贝叶斯框架成功解决了传统方法难以处理复杂约束的问题，MINTS算法在多个应用场景中表现出色，为约束优化问题提供了新的概率视角和理论保证。

Abstract: The Bayesian paradigm offers principled tools for sequential decision-making
under uncertainty, but its reliance on a probabilistic model for all parameters
can hinder the incorporation of complex structural constraints. We introduce a
minimalist Bayesian framework that places a prior only on the component of
interest, such as the location of the optimum. Nuisance parameters are
eliminated via profile likelihood, which naturally handles constraints. As a
direct instantiation, we develop a MINimalist Thompson Sampling (MINTS)
algorithm. Our framework accommodates structured problems, including
continuum-armed Lipschitz bandits and dynamic pricing. It also provides a
probabilistic lens on classical convex optimization algorithms such as the
center of gravity and ellipsoid methods. We further analyze MINTS for
multi-armed bandits and establish near-optimal regret guarantees.

</details>


### [56] [Methodological Insights into Structural Causal Modelling and Uncertainty-Aware Forecasting for Economic Indicators](https://arxiv.org/abs/2509.07036)
*Federico Cerutti*

Main category: cs.LG

TL;DR: 结合因果发现和不确定性感知预测的金融时间序列分析方法，应用于美国宏观经济指标分析，发现经济增长对GDP的单向因果联系，并利用Chronos框架进行失业率的零样本概率预测。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过结合因果发现和不确定性感知预测来增强金融时间序列分析的稳健性，为经济政策提供更可靠的信息支持。

Method: 使用LPCMCI框架和Gaussian Process Distance Correlation (GPDC)方法分析1970-2021年季度数据，发现动态因果关系；利用Chronos大型语言模型进行零样本概率预测。

Result: 发现经济增长对GDP存在稳健的单向因果联系，通胀连通性有限；失业率预测在1-2个季度前瞻中表现准确，提供90%置信区间，支持异常检测。

Conclusion: 研究表明将因果结构学习与概率语言模型相结合，能够为经济政策提供有价值的信息并增强预测的稳健性。

Abstract: This paper presents a methodological approach to financial time series
analysis by combining causal discovery and uncertainty-aware forecasting. As a
case study, we focus on four key U.S. macroeconomic indicators -- GDP, economic
growth, inflation, and unemployment -- and we apply the LPCMCI framework with
Gaussian Process Distance Correlation (GPDC) to uncover dynamic causal
relationships in quarterly data from 1970 to 2021. Our results reveal a robust
unidirectional causal link from economic growth to GDP and highlight the
limited connectivity of inflation, suggesting the influence of latent factors.
Unemployment exhibits strong autoregressive dependence, motivating its use as a
case study for probabilistic forecasting. Leveraging the Chronos framework, a
large language model trained for time series, we perform zero-shot predictions
on unemployment. This approach delivers accurate forecasts one and two quarters
ahead, without requiring task-specific training. Crucially, the model's
uncertainty-aware predictions yield 90\% confidence intervals, enabling
effective anomaly detection through statistically principled deviation
analysis. This study demonstrates the value of combining causal structure
learning with probabilistic language models to inform economic policy and
enhance forecasting robustness.

</details>


### [57] [Benchmarking Vision Transformers and CNNs for Thermal Photovoltaic Fault Detection with Explainable AI Validation](https://arxiv.org/abs/2509.07039)
*Serra Aksoy*

Main category: cs.LG

TL;DR: 本研究系统比较了CNN和Vision Transformer在光伏热故障检测中的性能，使用XRAI显著性分析验证模型决策与热物理原理的一致性，Swin Transformer表现最佳，为能源监测AI决策提供了物理验证方法。


<details>
  <summary>Details</summary>
Motivation: 人工智能在光伏监测部署中面临可解释性障碍，虽然深度学习在热故障检测中达到高精度，但缺乏模型决策与热物理原理一致性的验证，限制了在关键能源基础设施中的应用。

Method: 系统比较卷积神经网络（ResNet-18、EfficientNet-B0）和视觉变换器（ViT-Tiny、Swin-Tiny），使用XRAI显著性分析评估模型决策与热物理原理的对齐程度。

Result: 在20,000张红外图像上的评估显示，Swin Transformer性能最佳（二元准确率94%；多类准确率73%）。XRAI分析表明模型学习了物理有意义的特征，但不同故障类型性能差异显著。

Conclusion: 热物理指导的可解释性方法为验证能源监测应用中的AI决策提供了方法论，解决了可再生能源基础设施部署障碍，但热成像分辨率限制了某些环境因素的检测性能。

Abstract: Artificial intelligence deployment for automated photovoltaic (PV) monitoring
faces interpretability barriers that limit adoption in energy infrastructure
applications. While deep learning achieves high accuracy in thermal fault
detection, validation that model decisions align with thermal physics
principles remains lacking, creating deployment hesitancy where understanding
model reasoning is critical. This study provides a systematic comparison of
convolutional neural networks (ResNet-18, EfficientNet-B0) and vision
transformers (ViT-Tiny, Swin-Tiny) for thermal PV fault detection, using XRAI
saliency analysis to assess alignment with thermal physics principles. This
represents the first systematic comparison of CNNs and vision transformers for
thermal PV fault detection with physics-validated interpretability. Evaluation
on 20,000 infrared images spanning normal operation and 11 fault categories
shows that Swin Transformer achieves the highest performance (94% binary
accuracy; 73% multiclass accuracy) compared to CNN approaches. XRAI analysis
reveals that models learn physically meaningful features, such as localized
hotspots for cell defects, linear thermal paths for diode failures, and thermal
boundaries for vegetation shading, consistent with expected thermal signatures.
However, performance varies significantly across fault types: electrical faults
achieve strong detection (F1-scores >0.90) while environmental factors like
soiling remain challenging (F1-scores 0.20-0.33), indicating limitations
imposed by thermal imaging resolution. The thermal physics-guided
interpretability approach provides methodology for validating AI
decision-making in energy monitoring applications, addressing deployment
barriers in renewable energy infrastructure.

</details>


### [58] [Lookup multivariate Kolmogorov-Arnold Networks](https://arxiv.org/abs/2509.07103)
*Sergey Pozdnyakov,Philippe Schwaller*

Main category: cs.LG

TL;DR: 提出lmKANs作为线性层的替代方案，通过可训练低维多元函数实现高维映射，显著提升推理效率与容量的平衡


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型中，高维线性映射（线性层）占据了大部分参数数量和计算成本，需要更高效的替代方案

Method: 使用查找表实现的多变量样条函数来表达高维映射，每个函数包含数十到数百个可训练参数，通过少量乘法计算完成

Result: 推理FLOPs减少高达6.0倍，在甲烷配置数据集上实现10倍以上H100吞吐量提升，CNN中推理FLOPs减少1.6-2.1倍

Conclusion: lmKANs在保持MLP灵活性的同时显著降低了推理成本，为深度学习模型提供了高效的线性层替代方案

Abstract: High-dimensional linear mappings, or linear layers, dominate both the
parameter count and the computational cost of most modern deep-learning models.
We introduce a general drop-in replacement, lookup multivariate
Kolmogorov-Arnold Networks (lmKANs), which deliver a substantially better
trade-off between capacity and inference cost. Our construction expresses a
general high-dimensional mapping through trainable low-dimensional multivariate
functions. These functions can carry dozens or hundreds of trainable parameters
each, and yet it takes only a few multiplications to compute them because they
are implemented as spline lookup tables. Empirically, lmKANs reduce inference
FLOPs by up to 6.0x while matching the flexibility of MLPs in general
high-dimensional function approximation. In another feedforward fully connected
benchmark, on the tabular-like dataset of randomly displaced methane
configurations, lmKANs enable more than 10x higher H100 throughput at equal
accuracy. Within frameworks of Convolutional Neural Networks, lmKAN-based CNNs
cut inference FLOPs at matched accuracy by 1.6-2.1x and by 1.7x on the CIFAR-10
and ImageNet-1k datasets, respectively. Our code, including dedicated CUDA
kernels, is available online at https://github.com/schwallergroup/lmkan.

</details>


### [59] [Riemannian Batch Normalization: A Gyro Approach](https://arxiv.org/abs/2509.07115)
*Ziheng Chen,Xiao-Jun Wu,Nicu Sebe*

Main category: cs.LG

TL;DR: GyroBN是一个基于陀螺群结构的黎曼批量归一化框架，将欧几里得批量归一化扩展到非欧几里得流形数据上


<details>
  <summary>Details</summary>
Motivation: 传统的欧几里得归一化层不适用于流形数据，而许多机器学习中的黎曼流形具有陀螺结构，需要一种原则性的归一化方法

Method: 提出GyroBN框架，建立伪约简和陀螺等距陀螺两个必要条件来保证理论控制，并在七个代表性几何结构上实例化

Result: 在所有已知的机器学习陀螺群上都满足条件，实验证明了GyroBN的有效性

Conclusion: GyroBN提供了一个统一的黎曼批量归一化框架，能够处理多种非欧几里得几何结构，代码已开源

Abstract: Normalization layers are crucial for deep learning, but their Euclidean
formulations are inadequate for data on manifolds. On the other hand, many
Riemannian manifolds in machine learning admit gyro-structures, enabling
principled extensions of Euclidean neural networks to non-Euclidean domains.
Inspired by this, we introduce GyroBN, a principled Riemannian batch
normalization framework for gyrogroups. We establish two necessary conditions,
namely \emph{pseudo-reduction} and \emph{gyroisometric gyrations}, that
guarantee GyroBN with theoretical control over sample statistics, and show that
these conditions hold for all known gyrogroups in machine learning. Our
framework also incorporates several existing Riemannian normalization methods
as special cases. We further instantiate GyroBN on seven representative
geometries, including the Grassmannian, five constant curvature spaces, and the
correlation manifold, and derive novel gyro and Riemannian structures to enable
these instantiations. Experiments across these geometries demonstrate the
effectiveness of GyroBN. The code is available at
https://github.com/GitZH-Chen/GyroBN.git.

</details>


### [60] [Of Graphs and Tables: Zero-Shot Node Classification with Tabular Foundation Models](https://arxiv.org/abs/2509.07143)
*Adrian Hayler,Xingyue Huang,İsmail İlkan Ceylan,Michael Bronstein,Ben Finkelshtein*

Main category: cs.LG

TL;DR: TabGFM将图学习问题重新表述为表格问题，通过特征和结构编码器将图转换为表格，使用多个表格基础模型进行零样本节点分类，并通过集成选择聚合结果，在28个真实数据集上取得了优于专用GNN和最先进图基础模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图基础模型(GFMs)在代表性不足的数据集上训练，泛化性能有限。而表格基础模型(TFMs)不仅在表格预测任务中表现出色，还在时间序列、NLP和CV等领域展现出强大适用性，因此考虑将节点分类重新表述为表格问题。

Method: 1) 通过特征和结构编码器将图转换为表格（节点作为行，特征、结构和标签信息作为列）
2) 使用多个TFMs对多样化采样的表格进行零样本节点分类
3) 通过集成选择聚合不同TFMs的输出结果

Result: 在28个真实世界数据集上的实验表明，TabGFM相比任务专用GNNs和最先进的GFMs取得了持续的性能提升

Conclusion: 表格化重构为可扩展和可泛化的图学习提供了有前景的新途径，证明了TFMs在图学习领域的潜在价值

Abstract: Graph foundation models (GFMs) have recently emerged as a promising paradigm
for achieving broad generalization across various graph data. However, existing
GFMs are often trained on datasets that were shown to poorly represent
real-world graphs, limiting their generalization performance. In contrast,
tabular foundation models (TFMs) not only excel at classical tabular prediction
tasks but have also shown strong applicability in other domains such as time
series forecasting, natural language processing, and computer vision. Motivated
by this, we take an alternative view to the standard perspective of GFMs and
reformulate node classification as a tabular problem. Each node can be
represented as a row with feature, structure, and label information as columns,
enabling TFMs to directly perform zero-shot node classification via in-context
learning. In this work, we introduce TabGFM, a graph foundation model framework
that first converts a graph into a table via feature and structural encoders,
applies multiple TFMs to diversely subsampled tables, and then aggregates their
outputs through ensemble selection. Through experiments on 28 real-world
datasets, TabGFM achieves consistent improvements over task-specific GNNs and
state-of-the-art GFMs, highlighting the potential of tabular reformulation for
scalable and generalizable graph learning.

</details>


### [61] [Measuring Uncertainty in Transformer Circuits with Effective Information Consistency](https://arxiv.org/abs/2509.07149)
*Anatoly A. Krasnovsky*

Main category: cs.LG

TL;DR: 提出了EICS（有效信息一致性评分）方法，通过结合层间雅可比矩阵的sheaf不一致性和高斯有效信息代理，来量化Transformer电路中功能子图的连贯性和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏形式化的单次通过方法来量化Transformer电路中活跃电路的行为连贯性和可信度，需要一种白盒、单次通过的评估方法。

Method: 基于系统理论和sheaf/上同调理论，结合局部雅可比矩阵和激活值计算归一化的sheaf不一致性，以及从相同前向状态推导出的电路级因果涌现的高斯有效信息代理。

Result: 提出了EICS评分框架，提供了评分解释、计算开销（快速和精确模式）的实践指导，以及玩具验证分析。

Conclusion: EICS为Transformer电路提供了一种白盒、单次通过的连贯性量化方法，但实证验证仍需在LLM任务上进行。

Abstract: Mechanistic interpretability has identified functional subgraphs within large
language models (LLMs), known as Transformer Circuits (TCs), that appear to
implement specific algorithms. Yet we lack a formal, single-pass way to
quantify when an active circuit is behaving coherently and thus likely
trustworthy. Building on prior systems-theoretic proposals, we specialize a
sheaf/cohomology and causal emergence perspective to TCs and introduce the
Effective-Information Consistency Score (EICS). EICS combines (i) a normalized
sheaf inconsistency computed from local Jacobians and activations, with (ii) a
Gaussian EI proxy for circuit-level causal emergence derived from the same
forward state. The construction is white-box, single-pass, and makes units
explicit so that the score is dimensionless. We further provide practical
guidance on score interpretation, computational overhead (with fast and exact
modes), and a toy sanity-check analysis. Empirical validation on LLM tasks is
deferred.

</details>


### [62] [PLaID++: A Preference Aligned Language Model for Targeted Inorganic Materials Design](https://arxiv.org/abs/2509.07150)
*Andy Xu,Rohan Desai,Larry Wang,Gabriel Hope,Ethan Ritz*

Main category: cs.LG

TL;DR: PLaID++是一个基于大语言模型的晶体结构生成方法，通过Wyckoff文本表示和强化学习优化，显著提高了稳定、新颖晶体的生成效率


<details>
  <summary>Details</summary>
Motivation: 传统材料发现过程缓慢且昂贵，需要加速新材料开发流程，特别是用于太阳能电池、电池和碳捕获等技术的材料

Method: 使用Qwen-2.5 7B模型进行微调，采用Wyckoff-based文本表示生成晶体结构，并通过基于Direct Preference Optimization的强化学习技术指导生成过程

Result: PLaID++生成的热力学稳定、独特且新颖结构的比率比现有方法提高约50%，在无条件和空间群条件生成方面分别实现约115%和50%的改进

Conclusion: 该工作展示了将自然语言处理的后训练技术应用于材料设计的潜力，为靶向高效发现新材料开辟了新途径

Abstract: Discovering novel materials is critical for technological advancements such
as solar cells, batteries, and carbon capture. However, the development of new
materials is constrained by a slow and expensive trial-and-error process. To
accelerate this pipeline, we introduce PLaID++, a Large Language Model (LLM)
fine-tuned for stable and property-guided crystal generation. We fine-tune
Qwen-2.5 7B to generate crystal structures using a novel Wyckoff-based text
representation. We show that generation can be effectively guided with a
reinforcement learning technique based on Direct Preference Optimization (DPO),
with sampled structures categorized by their stability, novelty, and space
group. By encoding symmetry constraints directly into text and guiding model
outputs towards desirable chemical space, PLaID++ generates structures that are
thermodynamically stable, unique, and novel at a $\sim$50\% greater rate than
prior methods and conditionally generates structures with desired space group
properties. Our experiments highlight the effectiveness of iterative DPO,
achieving $\sim$115\% and $\sim$50\% improvements in unconditional and space
group conditioned generation, respectively, compared to fine-tuning alone. Our
work demonstrates the potential of adapting post-training techniques from
natural language processing to materials design, paving the way for targeted
and efficient discovery of novel materials.

</details>


### [63] [Predicting effect of novel treatments using molecular pathways and real-world data](https://arxiv.org/abs/2509.07204)
*Adrien Couetoux,Thomas Devenyns,Lise Diagne,David Champagne,Pierre-Yves Mousset,Chris Anagnostopoulos*

Main category: cs.LG

TL;DR: 提出基于机器学习的模块化方法，利用药物-通路权重影响评分和患者数据预测未测试药物的疗效


<details>
  <summary>Details</summary>
Motivation: 在药物研发中，临床测试前预测药物疗效具有挑战性，需要开发新方法来利用真实世界数据和药物嵌入信息

Method: 训练机器学习模型，使用药物-通路权重影响评分和患者特征/临床结果数据，分析未测试药物在生物分子-蛋白质通路上的加权影响评分

Result: 在真实世界数据集上验证了方法的有效性，使用了两种不同的权重影响评分算法，并展示了在未见治疗上的泛化性能

Conclusion: 该方法提供了一个可迭代的初始框架，支持未来利用真实世界临床数据和药物嵌入来预测未测试药物效果的研究

Abstract: In pharmaceutical R&D, predicting the efficacy of a pharmaceutical in
treating a particular disease prior to clinical testing or any real-world use
has been challenging. In this paper, we propose a flexible and modular machine
learning-based approach for predicting the efficacy of an untested
pharmaceutical for treating a disease. We train a machine learning model using
sets of pharmaceutical-pathway weight impact scores and patient data, which can
include patient characteristics and observed clinical outcomes. The resulting
model then analyses weighted impact scores of an untested pharmaceutical across
human biological molecule-protein pathways to generate a predicted efficacy
value. We demonstrate how the method works on a real-world dataset with patient
treatments and outcomes, with two different weight impact score algorithms We
include methods for evaluating the generalisation performance on unseen
treatments, and to characterise conditions under which the approach can be
expected to be most predictive. We discuss specific ways in which our approach
can be iterated on, making it an initial framework to support future work on
predicting the effect of untested drugs, leveraging RWD clinical data and drug
embeddings.

</details>


### [64] [Explaining How Quantization Disparately Skews a Model](https://arxiv.org/abs/2509.07222)
*Abhimanyu Bellam,Jung-Eun Kim*

Main category: cs.LG

TL;DR: 量化加剧了模型对不同群体的不公平影响，作者提出结合混合精度量化感知训练和数据集采样方法来缓解这一问题


<details>
  <summary>Details</summary>
Motivation: 发现后训练量化(PTQ)虽然压缩效果好，但会加剧对少数群体的不公平影响，需要研究量化过程中的不公平机制并找到解决方案

Method: 分析量化过程中权重和激活值变化对网络的影响，研究梯度范数和Hessian矩阵特征值，提出混合精度QAT结合数据集采样和加权损失函数的方法

Result: 量化导致logits方差降低、损失增加、群体准确率下降，通过提出的方法可以实现公平的量化神经网络部署

Conclusion: 量化会放大模型的不公平性，需要专门设计公平量化方法来确保所有群体都能从模型压缩中受益

Abstract: Post Training Quantization (PTQ) is widely adopted due to its high
compression capacity and speed with minimal impact on accuracy. However, we
observed that disparate impacts are exacerbated by quantization, especially for
minority groups. Our analysis explains that in the course of quantization there
is a chain of factors attributed to a disparate impact across groups during
forward and backward passes. We explore how the changes in weights and
activations induced by quantization cause cascaded impacts in the network,
resulting in logits with lower variance, increased loss, and compromised group
accuracies. We extend our study to verify the influence of these impacts on
group gradient norms and eigenvalues of the Hessian matrix, providing insights
into the state of the network from an optimization point of view. To mitigate
these effects, we propose integrating mixed precision Quantization Aware
Training (QAT) with dataset sampling methods and weighted loss functions,
therefore providing fair deployment of quantized neural networks.

</details>


### [65] [Systematic Optimization of Open Source Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2509.07238)
*Pranav Pawar,Dhwaj Jain,Varun Gupta,Kaustav Dedhia,Dashrath Kale,Sudhir Dhekane*

Main category: cs.LG

TL;DR: 这篇论文通过系统化的参数优化框架，在5款最先进的大模型上实现了数学推理任务的显著性能提升，平均节省29.4%计算成本和23.9%推理速度


<details>
  <summary>Details</summary>
Motivation: 解决大模型在数学推理任务中参数配置优化缺乏系统研究的问题，通过系统化的参数调优来提升效率和性能

Method: 建立了一个生产导向的参数优化框架，系统地搜索温度(0.1-0.5)、推理步骤(4-12)、规划周期(1-4)、核取样(0.85-0.98)等参数空间，在Qwen2.5-72B、Llama-3.1-70B等5款SOTA模型上进行实验

Result: 实现100%优化成功率，平均节省29.4%计算成本和23.9%推理速度提升。DeepSeek-V3达到98%准确率，Mixtral-8x22B实现每个准确响应361.5个标记的成本效益。发现低温度(0.1-0.4)和减少推理步骤(4-6)能在保持准确性的同时提升效率

Conclusion: 该研究为大模型数学推理任务提供了系统化的参数优化框架，发现了跨模型架构通用的优化规律，并提供了生产环境可直接使用的最优配置，对提升大模型在数学推理任务中的实际应用价值具有重要意义

Abstract: This paper presents a practical investigation into fine-tuning model
parameters for mathematical reasoning tasks through experimenting with various
configurations including randomness control, reasoning depth, and sampling
strategies, careful tuning demonstrates substantial improvements in efficiency
as well as performance. A holistically optimized framework is introduced for
five state-of-the-art models on mathematical reasoning tasks, exhibiting
significant performance boosts while maintaining solution correctness. Through
systematic parameter optimization across Qwen2.5-72B, Llama-3.1-70B,
DeepSeek-V3, Mixtral-8x22B, and Yi-Lightning, consistent efficiency gains are
demonstrated with 100% optimization success rate. The methodology achieves an
average 29.4% reduction in computational cost and 23.9% improvement in
inference speed across all tested models. This framework systematically
searches parameter spaces including temperature (0.1-0.5), reasoning steps
(4-12), planning periods (1-4), and nucleus sampling (0.85-0.98), determining
optimal configurations through testing on mathematical reasoning benchmarks.
Critical findings show that lower temperature regimes (0.1-0.4) and reduced
reasoning steps (4-6) consistently enhance efficiency without compromising
accuracy. DeepSeek-V3 achieves the highest accuracy at 98%, while Mixtral-8x22B
delivers the most cost-effective performance at 361.5 tokens per accurate
response. Key contributions include: (1) the first comprehensive optimization
study for five diverse SOTA models in mathematical reasoning, (2) a
standardized production-oriented parameter optimization framework, (3)
discovery of universal optimization trends applicable across model
architectures, and (4) production-ready configurations with extensive
performance characterization.

</details>


### [66] [IP-Basis PINNs: Efficient Multi-Query Inverse Parameter Estimation](https://arxiv.org/abs/2509.07245)
*Shalev Manor,Mohammad Kohandel*

Main category: cs.LG

TL;DR: IP-Basis PINNs是一种元学习框架，通过离线-在线分解解决物理信息神经网络在多查询逆问题中的计算效率问题，显著提升推理速度


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在多查询逆问题场景中计算成本高昂，每个新的观测数据集都需要重新进行昂贵的训练过程

Method: 采用离线-在线分解：离线训练深度网络生成基函数，在线冻结网络仅训练轻量级线性输出层，结合前向模式自动微分和新型在线损失函数

Result: 在三个不同基准测试中表现优异，包括扩展到未知函数项的通用PINNs，在常数和函数参数估计中表现一致，相比标准PINNs显著提速，在稀缺和噪声数据下鲁棒运行

Conclusion: IP-Basis PINNs为多查询逆问题提供了一种高效、快速的解决方案，在保持性能的同时大幅降低计算成本

Abstract: Solving inverse problems with Physics-Informed Neural Networks (PINNs) is
computationally expensive for multi-query scenarios, as each new set of
observed data requires a new, expensive training procedure. We present
Inverse-Parameter Basis PINNs (IP-Basis PINNs), a meta-learning framework that
extends the foundational work of Desai et al. (2022) to enable rapid and
efficient inference for inverse problems. Our method employs an offline-online
decomposition: a deep network is first trained offline to produce a rich set of
basis functions that span the solution space of a parametric differential
equation. For each new inverse problem online, this network is frozen, and
solutions and parameters are inferred by training only a lightweight linear
output layer against observed data. Key innovations that make our approach
effective for inverse problems include: (1) a novel online loss formulation for
simultaneous solution reconstruction and parameter identification, (2) a
significant reduction in computational overhead via forward-mode automatic
differentiation for PDE loss evaluation, and (3) a non-trivial validation and
early-stopping mechanism for robust offline training. We demonstrate the
efficacy of IP-Basis PINNs on three diverse benchmarks, including an extension
to universal PINNs for unknown functional terms-showing consistent performance
across constant and functional parameter estimation, a significant speedup per
query over standard PINNs, and robust operation with scarce and noisy data.

</details>


### [67] [GCond: Gradient Conflict Resolution via Accumulation-based Stabilization for Large-Scale Multi-Task Learning](https://arxiv.org/abs/2509.07252)
*Evgeny Alves Limarenko,Anastasiia Alexandrovna Studenikina*

Main category: cs.LG

TL;DR: GCond是一个基于PCGrad原理的高效梯度冲突解决方法，通过梯度累积和自适应仲裁机制实现两倍计算加速，在保持优化质量的同时在多个数据集和模型架构上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 多任务学习中梯度冲突是一个重要挑战，现有方法如PCGrad、CAGrad和GradNorm在原始实现中计算成本高昂，限制了它们在现代大型模型和transformer中的应用。

Method: 提出Gradient Conductor (GCond)方法，结合PCGrad原理、梯度累积和自适应仲裁机制，提供随机模式和确定性模式两种变体。

Result: 在ImageNet 1K和头颈部CT扫描数据集上，GCond的随机模式实现了两倍计算加速，在所有评估指标上表现优越，L1和SSIM损失均低于其他方法，并展示了在MobileNetV3-Small和ConvNeXt等不同规模模型上的高可扩展性。

Conclusion: GCond为多任务学习中的梯度冲突问题提供了一个可扩展且高效的解决方案，与现代优化器如AdamW和Lion/LARS兼容。

Abstract: In multi-task learning (MTL), gradient conflict poses a significant
challenge. Effective methods for addressing this problem, including PCGrad,
CAGrad, and GradNorm, in their original implementations are computationally
demanding, which significantly limits their application in modern large models
and transformers. We propose Gradient Conductor (GCond), a method that builds
upon PCGrad principles by combining them with gradient accumulation and an
adaptive arbitration mechanism. We evaluated GCond on self-supervised learning
tasks using MobileNetV3-Small and ConvNeXt architectures on the ImageNet 1K
dataset and a combined head and neck CT scan dataset, comparing the proposed
method against baseline linear combinations and state-of-the-art gradient
conflict resolution methods. The stochastic mode of GCond achieved a two-fold
computational speedup while maintaining optimization quality, and demonstrated
superior performance across all evaluated metrics, achieving lower L1 and SSIM
losses compared to other methods on both datasets. GCond exhibited high
scalability, being successfully applied to both compact models
(MobileNetV3-Small) and large architectures (ConvNeXt-tiny and ConvNeXt-Base).
It also showed compatibility with modern optimizers such as AdamW and
Lion/LARS. Therefore, GCond offers a scalable and efficient solution to the
problem of gradient conflicts in multi-task learning.

</details>


### [68] [Learning Generalized Hamiltonian Dynamics with Stability from Noisy Trajectory Data](https://arxiv.org/abs/2509.07280)
*Luke McLennan,Yi Wang,Ryan Farell,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: 提出一个基于变分贝叶斯推理的鲁棒框架，从噪声稀疏的相空间数据中无监督学习广义哈密顿动力学，能够同时处理保守、耗散和端口哈密顿系统。


<details>
  <summary>Details</summary>
Motivation: 虽然保守、耗散和端口哈密顿系统可能具有相同的初始总能量，但单个哈密顿网络模型难以从采样的相空间轨迹中捕捉不同的运动动力学和物理特性。

Method: 扩展稀疏辛随机傅里叶高斯过程学习，结合哈密顿景观的预测性连续数值估计，使用广义的状态和共轭动量哈密顿动力学形式。除了核化ELBO损失外，还加入稳定性和守恒约束作为正则化项。

Result: 该方法能够处理不同类别的物理系统，提高了预测精度并提供了有界不确定性估计。

Conclusion: 该框架为学习复杂哈密顿流形提供了一个有效的解决方案，通过物理约束正则化确保了模型的物理正确性。

Abstract: We introduce a robust framework for learning various generalized Hamiltonian
dynamics from noisy, sparse phase-space data and in an unsupervised manner
based on variational Bayesian inference. Although conservative, dissipative,
and port-Hamiltonian systems might share the same initial total energy of a
closed system, it is challenging for a single Hamiltonian network model to
capture the distinctive and varying motion dynamics and physics of a phase
space, from sampled observational phase space trajectories. To address this
complicated Hamiltonian manifold learning challenge, we extend sparse
symplectic, random Fourier Gaussian processes learning with predictive
successive numerical estimations of the Hamiltonian landscape, using a
generalized form of state and conjugate momentum Hamiltonian dynamics,
appropriate to different classes of conservative, dissipative and
port-Hamiltonian physical systems. In addition to the kernelized evidence lower
bound (ELBO) loss for data fidelity, we incorporate stability and conservation
constraints as additional hyper-parameter balanced loss terms to regularize the
model's multi-gradients, enforcing physics correctness for improved prediction
accuracy with bounded uncertainty.

</details>


### [69] [ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers](https://arxiv.org/abs/2509.07282)
*Jeff Shen,Lindsay Smith*

Main category: cs.LG

TL;DR: 提出ALICE模型解决密码破解问题，使用Transformer架构在极小训练样本下实现泛化，并通过可解释性分析揭示其工作机理


<details>
  <summary>Details</summary>
Motivation: 密码破解任务具有组合复杂性（26!种可能映射），是研究神经网络泛化能力的理想测试平台

Method: 开发ALICE架构：基于编码器Transformer，使用Gumbel-Sinkhorn方法构建双射解码头来显式建模排列

Result: 仅用约1500个密码样本训练（占可能空间的极小部分3.7×10^{-24}），就能泛化到未见过的密码，在准确性和速度上都达到新SOTA

Conclusion: 该架构和分析方法可扩展到任何具有双射映射和组合结构的领域，为神经网络泛化和可解释性提供了新见解

Abstract: We present cryptogram solving as an ideal testbed for studying neural network
generalization in combinatorially complex domains. In this task, models must
decrypt text encoded with substitution ciphers, choosing from 26! possible
mappings without explicit access to the cipher. We develop ALICE (an
Architecture for Learning Interpretable Cryptogram dEcipherment): a simple
encoder-only Transformer that sets a new state-of-the-art for both accuracy and
speed on this decryption problem. Surprisingly, ALICE generalizes to unseen
ciphers after training on only ${\sim}1500$ unique ciphers, a minute fraction
($3.7 \times 10^{-24}$) of the possible cipher space. To enhance
interpretability, we introduce a novel bijective decoding head that explicitly
models permutations via the Gumbel-Sinkhorn method, enabling direct extraction
of learned cipher mappings. Through early exit analysis, we reveal how ALICE
progressively refines its predictions in a way that appears to mirror common
human strategies for this task: early layers employ frequency-based heuristics,
middle layers form word structures, and final layers correct individual
characters. Our architectural innovations and analysis methods extend beyond
cryptograms to any domain with bijective mappings and combinatorial structure,
offering new insights into neural network generalization and interpretability.

</details>


### [70] [CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation](https://arxiv.org/abs/2509.07325)
*Alyssa Unell,Noel C. F. Codella,Sam Preston,Peniel Argaw,Wen-wai Yim,Zelalem Gero,Cliff Wong,Rajesh Jena,Eric Horvitz,Amanda K. Hall,Ruican Rachel Zhong,Jiachen Li,Shrey Jain,Mu Wei,Matthew Lungren,Hoifung Poon*

Main category: cs.LG

TL;DR: 使用LLM机器人自动生成非小细肺癌病人的NCCN指南一致治疗方案，通过混合人工注释和模型一致性信息降低成本并提高准确性


<details>
  <summary>Details</summary>
Motivation: 将复杂的病人情况转换为指南一致的治疗建议耗时且容易出错，需要专业知识，LLM技术有望提高效率和准确性

Method: 构建121例NSCLC病人纵向数据集，专家注解NCCN指南路径；使用LLM生成高质量代理评测标准；开发混合方法，结合人工注释和模型一致性信息构建预测框架和验证分类器

Result: 与专家注释标准强相关性（Spearman r=0.88，RMSE=0.08），治疗建议预测准确性高（AUROC=0.800），能够生成具有检验信心度的准确输出

Conclusion: 这个框架在准确性、可解释性和满足监管要求之间取得平衡，降低注释成本，为自动化临床决策支持提供了可扩展的路径

Abstract: The National Comprehensive Cancer Network (NCCN) provides evidence-based
guidelines for cancer treatment. Translating complex patient presentations into
guideline-compliant treatment recommendations is time-intensive, requires
specialized expertise, and is prone to error. Advances in large language model
(LLM) capabilities promise to reduce the time required to generate treatment
recommendations and improve accuracy. We present an LLM agent-based approach to
automatically generate guideline-concordant treatment trajectories for patients
with non-small cell lung cancer (NSCLC). Our contributions are threefold.
First, we construct a novel longitudinal dataset of 121 cases of NSCLC patients
that includes clinical encounters, diagnostic results, and medical histories,
each expertly annotated with the corresponding NCCN guideline trajectories by
board-certified oncologists. Second, we demonstrate that existing LLMs possess
domain-specific knowledge that enables high-quality proxy benchmark generation
for both model development and evaluation, achieving strong correlation
(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.
Third, we develop a hybrid approach combining expensive human annotations with
model consistency information to create both the agent framework that predicts
the relevant guidelines for a patient, as well as a meta-classifier that
verifies prediction accuracy with calibrated confidence scores for treatment
recommendations (AUROC=0.800), a critical capability for communicating the
accuracy of outputs, custom-tailoring tradeoffs in performance, and supporting
regulatory compliance. This work establishes a framework for clinically viable
LLM-based guideline adherence systems that balance accuracy, interpretability,
and regulatory requirements while reducing annotation costs, providing a
scalable pathway toward automated clinical decision support.

</details>


### [71] [General Demographic Foundation Models for Enhancing Predictive Performance Across Diseases](https://arxiv.org/abs/2509.07330)
*Li-Chin Chen,Ji-Tian Sheu,Yuh-Jue Chuang*

Main category: cs.LG

TL;DR: 该研究提出了一个通用的通用人口统计预训练模型（GDP），专门针对年龄和性别特征进行表示学习，通过顺序排序策略和编码方法将表格型人口统计数据转换为潜在嵌入，显著提升了医疗风险预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 人口统计属性在电子健康记录中普遍存在，是临床风险分层和治疗决策的重要预测因子，但现有模型设计中这些属性往往被置于辅助角色，缺乏专门的表示学习方法。

Method: 提出GDP预训练模型，探索不同的排序策略和编码方法组合，将表格型人口统计输入转换为潜在嵌入表示，并在不同地理区域、疾病类型和人群组成的数据集上进行预训练和评估。

Result: 实验结果表明，顺序排序策略显著提升了模型在判别、校准和决策树分裂信息增益方面的性能，特别是在年龄和性别对风险分层贡献显著的疾病中。即使人口统计属性预测价值较低的数据集，GDP也能增强其表示重要性。

Conclusion: 针对表格型人口统计属性的基础模型能够跨任务和人群泛化，为改善医疗健康应用的预测性能提供了有前景的方向。

Abstract: Demographic attributes are universally present in electronic health records
and serve as vital predictors in clinical risk stratification and treatment
decisions. Despite their significance, these attributes are often relegated to
auxiliary roles in model design, with limited attention has been given to
learning their representations. This study proposes a General Demographic
Pre-trained (GDP) model as a foundational representation framework tailored to
age and gender. The model is pre-trained and evaluated using datasets with
diverse diseases and population compositions from different geographic regions.
The GDP architecture explores combinations of ordering strategies and encoding
methods to transform tabular demographic inputs into latent embeddings.
Experimental results demonstrate that sequential ordering substantially
improves model performance in discrimination, calibration, and the
corresponding information gain at each decision tree split, particularly in
diseases where age and gender contribute significantly to risk stratification.
Even in datasets where demographic attributes hold relatively low predictive
value, GDP enhances the representational importance, increasing their influence
in downstream gradient boosting models. The findings suggest that foundational
models for tabular demographic attributes can generalize across tasks and
populations, offering a promising direction for improving predictive
performance in healthcare applications.

</details>


### [72] [SBS: Enhancing Parameter-Efficiency of Neural Representations for Neural Networks via Spectral Bias Suppression](https://arxiv.org/abs/2509.07373)
*Qihu Xie,Yuan Li,Yi Kang*

Main category: cs.LG

TL;DR: SBS是一种针对神经网络的神经表示方法的参数高效增强技术，通过抑制频谱偏差来改善卷积神经网络权重的重建精度。


<details>
  <summary>Details</summary>
Motivation: 标准的神经网络神经表示方法存在明显的频谱偏差，限制了其有效重建高频细节的能力，需要改进参数压缩效果。

Method: 提出SBS方法，采用两种技术：1）基于单向排序的平滑技术改善输出空间的核平滑度；2）基于单向排序平滑感知的随机傅里叶特征，根据层间参数数量自适应调节输入编码的频率带宽。

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上的多种ResNet模型评估表明，SBS相比现有最佳方法以更少的参数实现了显著更好的重建精度。

Conclusion: SBS通过有效抑制频谱偏差，在神经网络权重的神经表示中实现了更好的参数压缩和重建性能。

Abstract: Implicit neural representations have recently been extended to represent
convolutional neural network weights via neural representation for neural
networks, offering promising parameter compression benefits. However, standard
multi-layer perceptrons used in neural representation for neural networks
exhibit a pronounced spectral bias, hampering their ability to reconstruct
high-frequency details effectively. In this paper, we propose SBS, a
parameter-efficient enhancement to neural representation for neural networks
that suppresses spectral bias using two techniques: (1) a unidirectional
ordering-based smoothing that improves kernel smoothness in the output space,
and (2) unidirectional ordering-based smoothing aware random fourier features
that adaptively modulate the frequency bandwidth of input encodings based on
layer-wise parameter count. Extensive evaluations on various ResNet models with
datasets CIFAR-10, CIFAR-100, and ImageNet, demonstrate that SBS achieves
significantly better reconstruction accuracy with less parameters compared to
SOTA.

</details>


### [73] [EfficientNet in Digital Twin-based Cardiac Arrest Prediction and Analysis](https://arxiv.org/abs/2509.07388)
*Qasim Zia,Avais Jan,Zafar Iqbal,Muhammad Mumtaz Ali,Mukarram Ali,Murray Patterson*

Main category: cs.LG

TL;DR: 结合EfficientNet深度学习模型和数字孪生系统的新型框架，用于心脏骤停的早期检测和分析


<details>
  <summary>Details</summary>
Motivation: 心脏骤停是全球重大健康问题，早期识别和管理对改善患者预后至关重要

Method: 使用EfficientNet深度学习模型进行心血管图像特征学习，同时通过IoT设备数据构建个性化的数字孪生心血管系统模型

Result: 实验表明系统具有高预测准确性和效率

Conclusion: 深度学习与数字孪生技术的结合为心脏疾病预测提供了主动和个性化的方法

Abstract: Cardiac arrest is one of the biggest global health problems, and early
identification and management are key to enhancing the patient's prognosis. In
this paper, we propose a novel framework that combines an EfficientNet-based
deep learning model with a digital twin system to improve the early detection
and analysis of cardiac arrest. We use compound scaling and EfficientNet to
learn the features of cardiovascular images. In parallel, the digital twin
creates a realistic and individualized cardiovascular system model of the
patient based on data received from the Internet of Things (IoT) devices
attached to the patient, which can help in the constant assessment of the
patient and the impact of possible treatment plans. As shown by our
experiments, the proposed system is highly accurate in its prediction abilities
and, at the same time, efficient. Combining highly advanced techniques such as
deep learning and digital twin (DT) technology presents the possibility of
using an active and individual approach to predicting cardiac disease.

</details>


### [74] [Hybrid GCN-GRU Model for Anomaly Detection in Cryptocurrency Transactions](https://arxiv.org/abs/2509.07392)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Soyoun Kim,Sunyoung Moon,Sua Lee,Jaeyoung Choi,Hyemin Lee,Sangmi Chai*

Main category: cs.LG

TL;DR: 提出混合GCN-GRU模型检测区块链非法交易，结合图卷积网络和门控循环单元，在比特币数据上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 区块链交易网络具有复杂的时序模式和节点间关系，需要同时捕捉结构特征和序列特征来有效检测非法活动

Method: 使用混合GCN-GRU模型，GCN处理图结构信息捕捉节点间关系，GRU处理时序信息捕捉交易模式变化

Result: 在2020-2024年真实比特币交易数据上，模型达到0.9470准确率和0.9807 AUC-ROC，优于所有基线方法

Conclusion: 混合GCN-GRU模型能有效结合结构性和时序性特征，在区块链非法交易检测任务中表现优异

Abstract: Blockchain transaction networks are complex, with evolving temporal patterns
and inter-node relationships. To detect illicit activities, we propose a hybrid
GCN-GRU model that captures both structural and sequential features. Using real
Bitcoin transaction data (2020-2024), our model achieved 0.9470 Accuracy and
0.9807 AUC-ROC, outperforming all baselines.

</details>


### [75] [EMORF-II: Adaptive EM-based Outlier-Robust Filtering with Correlated Measurement Noise](https://arxiv.org/abs/2509.07415)
*Arslan Majal,Aamir Hussain Chughtai,Muhammad Tahir*

Main category: cs.LG

TL;DR: 提出EMORF-II，一种基于学习的异常值鲁棒滤波器，能够处理相关测量噪声并学习异常特征


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理相关测量噪声和异常值检测方面存在局限，需要更强大的异常值缓解能力

Method: 基于EM算法的增强版本，在推理过程中同时进行异常检测和学习异常特征

Result: 数值实验显示相比现有方法在精度上有性能提升，但计算开销增加

Conclusion: EMORF-II是一种实用的选择，计算复杂度与其他实用方法相当，适用于多样化应用

Abstract: We present a learning-based outlier-robust filter for a general setup where
the measurement noise can be correlated. Since it is an enhanced version of
EM-based outlier robust filter (EMORF), we call it as EMORF-II. As it is
equipped with an additional powerful feature to learn the outlier
characteristics during inference along with outlier-detection, EMORF-II has
improved outlier-mitigation capability. Numerical experiments confirm
performance gains as compared to the state-of-the-art methods in terms of
accuracy with an increased computational overhead. However, thankfully the
computational complexity order remains at par with other practical methods
making it a useful choice for diverse applications.

</details>


### [76] [The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2509.07430)
*Long Li,Jiaran Hao,Jason Klein Liu,Zhijian Zhou,Xiaoyu Tan,Wei Chu,Zhe Wang,Shirui Pan,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: 提出了DPH-RL框架，通过使用mass-covering f-divergences（如前向KL和JS散度）作为排练机制，解决RLVR微调中多尝试性能下降和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在RLVR微调中出现的多尝试性能（Pass@k）下降和灾难性遗忘问题，现有方法在散度项选择和功能方面缺乏系统性研究。

Method: 提出DPH-RL框架，使用mass-covering f-divergences（如前向KL散度和JS散度）作为排练机制，持续参考初始策略以保持广泛的解决方案覆盖范围。

Result: 在数学和SQL生成任务上的实验表明，DPH-RL不仅解决了Pass@k性能下降问题，还提高了域内外的Pass@1和Pass@k性能，且训练效率更高。

Conclusion: 散度度量的正确选择是改进RLVR的关键因素，DPH-RL通过保持多样性构建了更通用和多样化的推理模型。

Abstract: A central paradox in fine-tuning Large Language Models (LLMs) with
Reinforcement Learning with Verifiable Reward (RLVR) is the frequent
degradation of multi-attempt performance (Pass@k) despite improvements in
single-attempt accuracy (Pass@1). This is often accompanied by catastrophic
forgetting, where models lose previously acquired skills. While various methods
have been proposed, the choice and function of the divergence term have been
surprisingly unexamined as a proactive solution. We argue that standard RLVR
objectives -- both those using the mode-seeking reverse KL-divergence and those
forgoing a divergence term entirely -- lack a crucial mechanism for knowledge
retention. The reverse-KL actively accelerates this decay by narrowing the
policy, while its absence provides no safeguard against the model drifting from
its diverse knowledge base. We propose a fundamental shift in perspective:
using the divergence term itself as the solution. Our framework,
Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences
(like forward-KL and JS-divergence) to function as a rehearsal mechanism. By
continuously referencing the initial policy, this approach forces the model to
maintain broad solution coverage. Extensive experiments on math and SQL
generation demonstrate that DPH-RL not only resolves the Pass@k degradation but
improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is
more training-efficient because it computes f-divergence using generator
functions, requiring only sampling from the initial policy and no online
reference model. Our work highlights a crucial, overlooked axis for improving
RLVR, demonstrating that the proper selection of a divergence measure is a
powerful tool for building more general and diverse reasoning models.

</details>


### [77] [Conv4Rec: A 1-by-1 Convolutional AutoEncoder for User Profiling through Joint Analysis of Implicit and Explicit Feedbacks](https://arxiv.org/abs/2509.07499)
*Antoine Ledent,Petr Kasalický,Rodrigo Alves,Hady W. Lauw*

Main category: cs.LG

TL;DR: 提出新型卷积自编码器架构，用于用户建模和推荐任务，能同时处理显式评分和隐式反馈，在多个真实数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统模型在处理不同类型交互（显式评分和隐式反馈）时存在局限性，需要更灵活的架构来学习用户-物品间的复杂关联，并提供更丰富的信息预测能力。

Method: 设计卷积自编码器架构，能够学习不同交互类型之间的关联组合，同时从显式评分和隐式采样模式中联合学习，分别预测内容消费概率和高评分可能性。

Result: 在多个真实数据集上，该模型在隐式和显式反馈预测任务中都达到了最先进的性能，同时提供了额外的可解释性（各评分概率的单独预测）。

Conclusion: 该模型不仅实现了优异的预测性能，还能识别用户可能不会自然消费但会喜欢的物品，为推荐系统提供了更强的解释能力和实用性。

Abstract: We introduce a new convolutional AutoEncoder architecture for user modelling
and recommendation tasks with several improvements over the state of the art.
Firstly, our model has the flexibility to learn a set of associations and
combinations between different interaction types in a way that carries over to
each user and item. Secondly, our model is able to learn jointly from both the
explicit ratings and the implicit information in the sampling pattern (which we
refer to as `implicit feedback'). It can also make separate predictions for the
probability of consuming content and the likelihood of granting it a high
rating if observed. This not only allows the model to make predictions for both
the implicit and explicit feedback, but also increases the informativeness of
the predictions: in particular, our model can identify items which users would
not have been likely to consume naturally, but would be likely to enjoy if
exposed to them. Finally, we provide several generalization bounds for our
model, which to the best of our knowledge, are among the first generalization
bounds for auto-encoders in a Recommender Systems setting; we also show that
optimizing our loss function guarantees the recovery of the exact sampling
distribution over interactions up to a small error in total variation. In
experiments on several real-life datasets, we achieve state-of-the-art
performance on both the implicit and explicit feedback prediction tasks despite
relying on a single model for both, and benefiting from additional
interpretability in the form of individual predictions for the probabilities of
each possible rating.

</details>


### [78] [Water Demand Forecasting of District Metered Areas through Learned Consumer Representations](https://arxiv.org/abs/2509.07515)
*Adithya Ramachandran,Thorkil Flensmark B. Neergaard,Tomás Arias-Vergara,Andreas Maier,Siming Bayer*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于对比学习和波小变换卷积网络的新题水需求预测方法，通过分析用户消费行为来提高预测精度，最大提升4.9%


<details>
  <summary>Details</summary>
Motivation: 水资源管理面临气候变化带来的不确定性挑战，尤其是气象等非确定性因素影响水需求预测精度

Method: 采用无监督对比学习将用户按消费行为分类，然后使用波小变换卷积网络结合交叉注意机制进行需求预测

Result: 在真实DMA区域进行六个月的评估，MAPE指标最大提升4.9%，同时识别出受社会经济因素影响的用户行为

Conclusion: 该方法能够有效提高短期水需求预测精度，并为水资源管理提供了更深入的用户行为洞察

Abstract: Advancements in smart metering technologies have significantly improved the
ability to monitor and manage water utilities. In the context of increasing
uncertainty due to climate change, securing water resources and supply has
emerged as an urgent global issue with extensive socioeconomic ramifications.
Hourly consumption data from end-users have yielded substantial insights for
projecting demand across regions characterized by diverse consumption patterns.
Nevertheless, the prediction of water demand remains challenging due to
influencing non-deterministic factors, such as meteorological conditions. This
work introduces a novel method for short-term water demand forecasting for
District Metered Areas (DMAs) which encompass commercial, agricultural, and
residential consumers. Unsupervised contrastive learning is applied to
categorize end-users according to distinct consumption behaviors present within
a DMA. Subsequently, the distinct consumption behaviors are utilized as
features in the ensuing demand forecasting task using wavelet-transformed
convolutional networks that incorporate a cross-attention mechanism combining
both historical data and the derived representations. The proposed approach is
evaluated on real-world DMAs over a six-month period, demonstrating improved
forecasting performance in terms of MAPE across different DMAs, with a maximum
improvement of 4.9%. Additionally, it identifies consumers whose behavior is
shaped by socioeconomic factors, enhancing prior knowledge about the
deterministic patterns that influence demand.

</details>


### [79] [RoseCDL: Robust and Scalable Convolutional Dictionary Learning for Rare-event Detection](https://arxiv.org/abs/2509.07523)
*Jad Yehya,Mansour Benbakoura,Cédric Allain,Benoît Malezieux,Matthieu Kowalski,Thomas Moreau*

Main category: cs.LG

TL;DR: RoseCDL是一个可扩展且鲁棒的卷积字典学习算法，用于长信号中的无监督罕见事件检测，通过随机窗口化和在线异常检测解决了传统CDL的高计算成本和异常敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 卷积字典学习(CDL)在建模信号局部结构方面很强大，但在检测罕见或异常事件方面应用有限，主要面临高计算成本和对异常值敏感两大挑战。

Method: RoseCDL结合了随机窗口化技术以实现大数据集上的高效训练，并采用在线异常检测来增强鲁棒性和隔离异常模式。

Result: 该算法将CDL重新定位为实际事件发现和特征提取工具，扩展了其在压缩或去噪等传统任务之外的应用范围。

Conclusion: RoseCDL为天文学、物理模拟和生物医学科学等领域的大规模信号分析提供了一个实用的无监督罕见事件检测框架。

Abstract: Identifying recurring patterns and rare events in large-scale signals is a
fundamental challenge in fields such as astronomy, physical simulations, and
biomedical science. Convolutional Dictionary Learning (CDL) offers a powerful
framework for modeling local structures in signals, but its use for detecting
rare or anomalous events remains largely unexplored. In particular, CDL faces
two key challenges in this setting: high computational cost and sensitivity to
artifacts and outliers. In this paper, we introduce RoseCDL, a scalable and
robust CDL algorithm designed for unsupervised rare event detection in long
signals. RoseCDL combines stochastic windowing for efficient training on large
datasets with inline outlier detection to enhance robustness and isolate
anomalous patterns. This reframes CDL as a practical tool for event discovery
and characterization in real-world signals, extending its role beyond
traditional tasks like compression or denoising.

</details>


### [80] [$ΔL$ Normalization: Rethink Loss Aggregation in RLVR](https://arxiv.org/abs/2509.07558)
*Zhiyuan He,Xufang Luo,Yike Zhang,Yuqing Yang,Lili Qiu*

Main category: cs.LG

TL;DR: 提出了ΔL归一化方法，专门针对RLVR中动态生成长度特性设计的损失聚合方法，解决了梯度方差大和优化不稳定的问题


<details>
  <summary>Details</summary>
Motivation: RLVR在提升大语言模型推理能力方面显示出强大潜力，但训练过程中响应长度变化大导致梯度方差高和优化不稳定，现有方法存在估计偏差或梯度方差问题

Method: 通过理论和实证分析不同长度对策略损失的影响，将问题重新表述为寻找最小方差无偏估计量，提出ΔL归一化方法

Result: 在多个模型大小、最大长度和任务上均取得优异结果，理论上既能提供无偏估计又能最小化梯度方差

Conclusion: ΔL归一化是一种简单有效的损失聚合方法，在RLVR框架中显著提升了训练稳定性和性能表现

Abstract: We propose $\Delta L$ Normalization, a simple yet effective loss aggregation
method tailored to the characteristic of dynamic generation lengths in
Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has
demonstrated strong potential in improving the reasoning capabilities of large
language models (LLMs), but a major challenge lies in the large variability of
response lengths during training, which leads to high gradient variance and
unstable optimization. Although previous methods such as GRPO, DAPO, and Dr.
GRPO introduce different loss normalization terms to address this issue, they
either produce biased estimates or still suffer from high gradient variance. By
analyzing the effect of varying lengths on policy loss both theoretically and
empirically, we reformulate the problem as finding a minimum-variance unbiased
estimator. Our proposed $\Delta L$ Normalization not only provides an unbiased
estimate of the true policy loss but also minimizes gradient variance in
theory. Extensive experiments show that it consistently achieves superior
results across different model sizes, maximum lengths, and tasks. Our code will
be made public at https://github.com/zerolllin/Delta-L-Normalization.

</details>


### [81] [uGMM-NN: Univariate Gaussian Mixture Model Neural Network](https://arxiv.org/abs/2509.07569)
*Zakeria Sharif Ali*

Main category: cs.LG

TL;DR: 提出uGMM-NN神经网络架构，将单变量高斯混合模型嵌入神经元，实现概率推理和不确定性建模


<details>
  <summary>Details</summary>
Motivation: 传统神经元使用加权和加固定非线性，缺乏概率解释和不确定性建模能力，需要将概率推理直接集成到神经网络计算单元中

Method: 每个uGMM-NN节点将其激活参数化为单变量高斯混合模型，具有可学习的均值、方差和混合系数，保持前馈网络的可扩展性

Result: uGMM-NN在判别性能上与传统多层感知器相当，同时提供激活的概率解释

Conclusion: 该框架为将不确定性感知组件集成到现代神经架构中奠定了基础，为判别和生成建模开辟了新方向

Abstract: This paper introduces the Univariate Gaussian Mixture Model Neural Network
(uGMM-NN), a novel neural architecture that embeds probabilistic reasoning
directly into the computational units of deep networks. Unlike traditional
neurons, which apply weighted sums followed by fixed nonlinearities, each
uGMM-NN node parameterizes its activations as a univariate Gaussian mixture,
with learnable means, variances, and mixing coefficients. This design enables
richer representations by capturing multimodality and uncertainty at the level
of individual neurons, while retaining the scalability of standard feedforward
networks. We demonstrate that uGMM-NN can achieve competitive discriminative
performance compared to conventional multilayer perceptrons, while additionally
offering a probabilistic interpretation of activations. The proposed framework
provides a foundation for integrating uncertainty-aware components into modern
neural architectures, opening new directions for both discriminative and
generative modeling.

</details>


### [82] [Homogenization with Guaranteed Bounds via Primal-Dual Physically Informed Neural Networks](https://arxiv.org/abs/2509.07579)
*Liya Gaynutdinova,Martin Doškář,Ondřej Rokoš,Ivana Pultarová*

Main category: cs.LG

TL;DR: 提出了PINN的双重公式化方法，用于处理具有不连续系数的材料问题，通过上下误差界限提高计算可靠性


<details>
  <summary>Details</summary>
Motivation: 传统PINN在处理具有不连续系数（如分段常数材料属性）的多尺度建模PDE时经常失败，需要更可靠的误差诊断方法

Method: 引入双重公式化到PINN框架，比较标准PINN（使用平滑材料近似）与变分PINN（使用谱和神经网络测试函数），推导保证的上下误差界限

Result: 强形式PINN在受控环境下可能优于变分PINN，但对材料不连续性敏感且缺乏明确诊断；变分PINN能直接处理分段常数参数但需要仔细选择测试函数

Conclusion: 双重公式化作为收敛质量的可靠指标，将其集成到PINN框架中增强了在微观力学均质化问题中的适用性

Abstract: Physics-informed neural networks (PINNs) have shown promise in solving
partial differential equations (PDEs) relevant to multiscale modeling, but they
often fail when applied to materials with discontinuous coefficients, such as
media with piecewise constant properties. This paper introduces a dual
formulation for the PINN framework to improve the reliability of the
homogenization of periodic thermo-conductive composites, for both strong and
variational (weak) formulations. The dual approach facilitates the derivation
of guaranteed upper and lower error bounds, enabling more robust detection of
PINN failure. We compare standard PINNs applied to smoothed material
approximations with variational PINNs (VPINNs) using both spectral and neural
network-based test functions. Our results indicate that while strong-form PINNs
may outperform VPINNs in controlled settings, they are sensitive to material
discontinuities and may fail without clear diagnostics. In contrast, VPINNs
accommodate piecewise constant material parameters directly but require careful
selection of test functions to avoid instability. Dual formulation serves as a
reliable indicator of convergence quality, and its integration into PINN
frameworks enhances their applicability to homogenization problems in
micromechanics.

</details>


### [83] [Transformer-Based Approach to Optimal Sensor Placement for Structural Health Monitoring of Probe Cards](https://arxiv.org/abs/2509.07603)
*Mehdi Bejani,Marco Mauri,Daniele Acconcia,Simone Todaro,Stefano Mariani*

Main category: cs.LG

TL;DR: 基于Transformer的深度学习策略用于优化半导体探针卡结构健康监测的传感器布局，通过物理信息增强的数据集训练混合CNN-Transformer模型，实现99.83%的健康状态分类准确率和99.73%的裂纹检测召回率。


<details>
  <summary>Details</summary>
Motivation: 半导体探针卡的故障（如基板裂纹和螺丝松动）严重影响制造良率和可靠性，需要通过传感器监测来检测这些故障模式。

Method: 使用有限元模型模拟故障场景的频率响应函数，构建包含物理信息场景扩展和物理感知数据增强的综合数据集，训练混合卷积神经网络和Transformer模型。

Result: 模型在探针卡健康状态分类（基线、螺丝松动、裂纹）上达到99.83%的高准确率，裂纹检测召回率达99.73%，并通过3次重复10折分层交叉验证确认模型鲁棒性。

Conclusion: 注意力机制能够识别关键传感器位置，为设计高效、经济监测系统提供可行见解，证明基于注意力的深度学习能够推进半导体制造的预防性维护，提高操作可靠性和良率。

Abstract: This paper presents an innovative Transformer-based deep learning strategy
for optimizing the placement of sensors aiming at structural health monitoring
of semiconductor probe cards. Failures in probe cards, including substrate
cracks and loosened screws, would critically affect semiconductor manufacturing
yield and reliability. Some failure modes could be detected by equipping a
probe card with adequate sensors. Frequency response functions from simulated
failure scenarios are adopted within a finite element model of a probe card. A
comprehensive dataset, enriched by physics-informed scenario expansion and
physics-aware statistical data augmentation, is exploited to train a hybrid
Convolutional Neural Network and Transformer model. The model achieves high
accuracy (99.83%) in classifying the probe card health states (baseline, loose
screw, crack) and an excellent crack detection recall (99.73%). Model
robustness is confirmed through a rigorous framework of 3 repetitions of
10-fold stratified cross-validation. The attention mechanism also pinpoints
critical sensor locations: an analysis of the attention weights offers
actionable insights for designing efficient, cost-effective monitoring systems
by optimizing sensor configurations. This research highlights the capability of
attention-based deep learning to advance proactive maintenance, enhancing
operational reliability and yield in semiconductor manufacturing.

</details>


### [84] [K2-Think: A Parameter-Efficient Reasoning System](https://arxiv.org/abs/2509.07604)
*Zhoujun Cheng,Richard Fan,Shibo Hao,Taylor W. Killian,Haonan Li,Suqi Sun,Hector Ren,Alexander Moreno,Daqian Zhang,Tianjun Zhong,Yuxin Xiong,Yuanzhe Hu,Yutao Xie,Xudong Han,Yuqi Wang,Varad Pimpalkhute,Yonghao Zhuang,Aaryamonvikram Singh,Xuezhi Liang,Anze Xie,Jianshu She,Desai Fan,Chengqian Gao,Liqun Ma,Mikhail Yurochkin,John Maggs,Xuezhe Ma,Guowei He,Zhiting Hu,Zhengzhong Liu,Eric P. Xing*

Main category: cs.LG

TL;DR: K2-Think是一个32B参数的推理系统，通过先进的训练后处理和测试时计算技术，在数学推理等任务上达到最先进性能，媲美甚至超越更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 证明较小参数模型通过精心设计的训练和推理技术，能够与大规模模型竞争，使开源推理系统更加可及和经济实惠。

Method: 基于六个关键技术支柱：长思维链监督微调、可验证奖励的强化学习、推理前代理规划、测试时缩放、推测解码和推理优化硬件，使用公开开源数据集。

Result: 在开源模型的公共基准测试中取得最先进分数，数学推理表现突出，代码和科学领域也表现强劲，推理速度达到每秒2000+ token。

Conclusion: 通过集成的训练后配方和策略性推理时间增强，参数效率更高的模型能够与最先进系统竞争，推动开源推理系统的普及。

Abstract: K2-Think is a reasoning system that achieves state-of-the-art performance
with a 32B parameter model, matching or surpassing much larger models like
GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system
shows that smaller models can compete at the highest levels by combining
advanced post-training and test-time computation techniques. The approach is
based on six key technical pillars: Long Chain-of-thought Supervised
Finetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic
planning prior to reasoning, Test-time Scaling, Speculative Decoding, and
Inference-optimized Hardware, all using publicly available open-source
datasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art
scores on public benchmarks for open-source models, while also performing
strongly in other areas such as Code and Science. Our results confirm that a
more parameter-efficient model like K2-Think 32B can compete with
state-of-the-art systems through an integrated post-training recipe that
includes long chain-of-thought training and strategic inference-time
enhancements, making open-source reasoning systems more accessible and
affordable. K2-Think is freely available at k2think.ai, offering best-in-class
inference speeds of over 2,000 tokens per second per request via the Cerebras
Wafer-Scale Engine.

</details>


### [85] [Beyond Rebalancing: Benchmarking Binary Classifiers Under Class Imbalance Without Rebalancing Techniques](https://arxiv.org/abs/2509.07605)
*Ali Nawaz,Amir Ahmad,Shehroz S. Khan*

Main category: cs.LG

TL;DR: 评估二元分类器在类别不平衡情况下的原生性能，不依赖重平衡技术，发现随着数据复杂度增加和少数类样本减少，分类性能下降，但TabPFN和提升集成方法表现相对更好


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注重平衡技术，但缺乏对未应用重平衡技术的分类器在类别不平衡下的性能评估，特别是在关键领域如医疗诊断和异常检测中

Method: 系统评估多种二元分类器在真实和合成数据集上的鲁棒性，使用一次性学习和少样本学习作为基线，通过合成决策边界模拟不同数据复杂度，并比较欠采样、过采样和单类分类方法

Result: 分类难度随数据复杂度增加和少数类样本减少而增大，传统分类器在极端不平衡下性能恶化，但TabPFN和提升集成方法保持相对较高的性能和更好的泛化能力

Conclusion: 为不平衡学习提供了有价值的模型选择指导，揭示了在不依赖显式重平衡技术时分类器的鲁棒性，先进模型在极端不平衡条件下表现更优

Abstract: Class imbalance poses a significant challenge to supervised classification,
particularly in critical domains like medical diagnostics and anomaly detection
where minority class instances are rare. While numerous studies have explored
rebalancing techniques to address this issue, less attention has been given to
evaluating the performance of binary classifiers under imbalance when no such
techniques are applied. Therefore, the goal of this study is to assess the
performance of binary classifiers "as-is", without performing any explicit
rebalancing. Specifically, we systematically evaluate the robustness of a
diverse set of binary classifiers across both real-world and synthetic
datasets, under progressively reduced minority class sizes, using one-shot and
few-shot scenarios as baselines. Our approach also explores varying data
complexities through synthetic decision boundary generation to simulate
real-world conditions. In addition to standard classifiers, we include
experiments using undersampling, oversampling strategies, and one-class
classification (OCC) methods to examine their behavior under severe imbalance.
The results confirm that classification becomes more difficult as data
complexity increases and the minority class size decreases. While traditional
classifiers deteriorate under extreme imbalance, advanced models like TabPFN
and boosting-based ensembles retain relatively higher performance and better
generalization compared to traditional classifiers. Visual interpretability and
evaluation metrics further validate these findings. Our work offers valuable
guidance on model selection for imbalanced learning, providing insights into
classifier robustness without dependence on explicit rebalancing techniques.

</details>


### [86] [Graph-based Integrated Gradients for Explaining Graph Neural Networks](https://arxiv.org/abs/2509.07648)
*Lachlan Simpson,Kyle Millar,Adriel Cheng,Cheng-Chew Lim,Hong Gunn Chew*

Main category: cs.LG

TL;DR: 提出了图基集成梯度(GB-IG)方法，将集成梯度扩展到图数据，解决了传统IG方法不适用于离散图结构的问题。


<details>
  <summary>Details</summary>
Motivation: 集成梯度(IG)方法假设数据是连续的，而图是离散结构，因此IG不适用于图数据。需要开发专门针对图结构的解释性方法。

Method: 开发了图基集成梯度(GB-IG)，这是IG方法在图数据上的扩展。通过四个合成数据集验证了该方法能准确识别图中用于分类的关键结构组件。

Result: 在三个主流真实图数据集上的实验表明，GB-IG在节点分类任务中比传统IG方法能更好地突出重要特征。

Conclusion: GB-IG成功地将集成梯度方法扩展到图数据，为图神经网络的解释性提供了有效的工具，在合成和真实数据集上都表现出优越性能。

Abstract: Integrated Gradients (IG) is a common explainability technique to address the
black-box problem of neural networks. Integrated gradients assumes continuous
data. Graphs are discrete structures making IG ill-suited to graphs. In this
work, we introduce graph-based integrated gradients (GB-IG); an extension of IG
to graphs. We demonstrate on four synthetic datasets that GB-IG accurately
identifies crucial structural components of the graph used in classification
tasks. We further demonstrate on three prevalent real-world graph datasets that
GB-IG outperforms IG in highlighting important features for node classification
tasks.

</details>


### [87] [Uncovering Scaling Laws for Large Language Models via Inverse Problems](https://arxiv.org/abs/2509.07909)
*Arun Verma,Zhaoxuan Wu,Zijian Zhou,Xiaoqiang Lin,Zhiliang Chen,Rachael Hwee Ling Sim,Rui Qiao,Jingtan Wang,Nhung Bui,Xinyuan Niu,Wenyang Hu,Gregory Kang Ruey Lau,Zi-Yu Khoo,Zitong Zhao,Xinyi Xu,Apivich Hemachandra,See-Kiong Ng,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 本文主张通过逆问题方法发现LLM的缩放定律，以更经济高效的方式指导大语言模型的构建


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型训练成本高昂，传统的试错方法不可行，需要寻找更有效的方法来发现指导模型构建的缩放定律

Method: 借鉴逆问题在发现基础科学定律方面的成功经验，提出使用逆问题方法来高效发现LLM的缩放定律

Result: 该方法有望以显著更好的成本效益实现期望的性能目标

Conclusion: 逆问题方法可以成为指导大语言模型构建的有效途径，提高成本效益

Abstract: Large Language Models (LLMs) are large-scale pretrained models that have
achieved remarkable success across diverse domains. These successes have been
driven by unprecedented complexity and scale in both data and computations.
However, due to the high costs of training such models, brute-force
trial-and-error approaches to improve LLMs are not feasible. Inspired by the
success of inverse problems in uncovering fundamental scientific laws, this
position paper advocates that inverse problems can also efficiently uncover
scaling laws that guide the building of LLMs to achieve the desirable
performance with significantly better cost-effectiveness.

</details>


### [88] [FUnc-SNE: A flexible, Fast, and Unconstrained algorithm for neighbour embeddings](https://arxiv.org/abs/2509.07681)
*Pierre Lambert,Edouard Couplet,Michel Verleysen,John Aldo Lee*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的邻层嵌入加速方法，在保持结构保持性和超参数调节灵活性的同时，实现了高效的计算速度，支持任意维度的嵌入空间。


<details>
  <summary>Details</summary>
Motivation: 解决现有邻层嵌入方法在加速方面的两种极端：一端是使用负采样的粗糕近似方法（如UMAP）速度快但结构保持性差，另一端是精确但慢速的方法（如FIt-SNE或BH-t-SNE）仅支持2-3维嵌入。需要找到一种方法来桥接这两种方法的优势。

Method: 提出了一种新的迭代近似最近邻层搜索算法，每次迭代只需少量计算，同时保持良好的结构保持性和超参数调节灵活性。该方法里弃了传统的两阶段方法，支持交互式数据探索。

Result: 通过公开的GPU加速GUI集成进行实验，显示了在速度、结构提取灵活性方面的不错结果，并显示了在更广泛的机器学习场景中的潜在应用价值。

Conclusion: 该方法成功地桥接了高速但粗糕的负采样方法与精确但慢速的加速方法之间的间隔，提供了一种新的迭代近似最近邻层搜索方法，为交互式数据探索和更高维嵌入应用开启了新可能。

Abstract: Neighbour embeddings (NE) allow the representation of high dimensional
datasets into lower dimensional spaces and are often used in data
visualisation. In practice, accelerated approximations are employed to handle
very large datasets. Accelerating NE is challenging, and two main directions
have been explored: very coarse approximations based on negative sampling (as
in UMAP) achieve high effective speed but may lack quality in the extracted
structures; less coarse approximations, as used in FIt-SNE or BH-t-SNE, offer
better structure preservation at the cost of speed, while also restricting the
target dimensionality to 2 or 3, limiting NE to visualisation. In some
variants, the precision of these costlier accelerations also enables
finer-grained control on the extracted structures through dedicated
hyperparameters.
  This paper proposes to bridge the gab between both approaches by introducing
a novel way to accelerate NE, requiring a small number of computations per
iteration while maintaining good fine-grained structure preservation and
flexibility through hyperparameter tuning, without limiting the dimensionality
of the embedding space. The method was designed for interactive exploration of
data; as such, it abandons the traditional two-phased approach of other NE
methods, allowing instantaneous visual feedback when changing hyperparameters,
even when these control processes happening on the high-dimensional side of the
computations. Experiments using a publicly available, GPU accelerated GUI
integration of the method show promising results in terms of speed, flexibility
in the structures getting extracted, and show potential uses in broader machine
learning contexts with minimal algorithmic modifications. Central to this
algorithm is a novel approach to iterative approximate nearest neighbour
search, which shows promising results compared to nearest neighbour descent.

</details>


### [89] [IBN: An Interpretable Bidirectional-Modeling Network for Multivariate Time Series Forecasting with Variable Missing](https://arxiv.org/abs/2509.07725)
*Shusen Ma,Tianhao Zhang,Qijiu Xia,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: IBN网络通过不确定性感知插值和双向建模，解决了多元时间序列预测中变量缺失问题，实现了更可靠和可解释的预测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法GinAR虽然首次使用注意力机制处理变量缺失，但缺乏可解释性且无法捕捉潜在时间模式，需要更先进的解决方案

Method: 提出IBN网络，整合不确定性感知插值(UAI)和高斯核图卷积(GGCN)，使用MC Dropout估计重构值不确定性，双向递归单元增强时间依赖建模

Result: 大量实验表明IBN在各种缺失率场景下达到最先进的预测性能

Conclusion: IBN为缺失变量的多元时间序列预测提供了更可靠和可解释的框架

Abstract: Multivariate time series forecasting (MTSF) often faces challenges from
missing variables, which hinder conventional spatial-temporal graph neural
networks in modeling inter-variable correlations. While GinAR addresses
variable missing using attention-based imputation and adaptive graph learning
for the first time, it lacks interpretability and fails to capture more latent
temporal patterns due to its simple recursive units (RUs). To overcome these
limitations, we propose the Interpretable Bidirectional-modeling Network (IBN),
integrating Uncertainty-Aware Interpolation (UAI) and Gaussian kernel-based
Graph Convolution (GGCN). IBN estimates the uncertainty of reconstructed values
using MC Dropout and applies an uncertainty-weighted strategy to mitigate
high-risk reconstructions. GGCN explicitly models spatial correlations among
variables, while a bidirectional RU enhances temporal dependency modeling.
Extensive experiments show that IBN achieves state-of-the-art forecasting
performance under various missing-rate scenarios, providing a more reliable and
interpretable framework for MTSF with missing variables. Code is available at:
https://github.com/zhangth1211/NICLab-IBN.

</details>


### [90] [Forecasting Russian Equipment Losses Using Time Series and Deep Learning Models](https://arxiv.org/abs/2509.07813)
*Jonathan Teagan*

Main category: cs.LG

TL;DR: 这篇论文使用多种预测技术对乌克兰战争中俄罗斯设备损失进行建模和预测，发现深度学习模型（特别是TCN和LSTM）在高时间粒度条件下表现最佳，并强调了集成预测和公开情报数据在冲突建模中的价值。


<details>
  <summary>Details</summary>
Motivation: 利用公开源情报数据来评估乌克兰战争中俄罗斯设备损失趋势，预测未来损失模式，并比较不同预测模型的性能。

Method: 采用ARIMA、Prophet、LSTM、TCN和XGBoost等多种预测技术，基于WarSpotting提供的每日和每月OSINT数据对俄罗斯设备损失进行建模和预测。

Result: 深度学习模型（特别是TCN和LSTM）在高时间粒度条件下产生了稳定且一致的预测结果，表现最佳。

Conclusion: 集成预测在冲突建模中具有重要价值，公开可用的OSINT数据能够有效量化时间过程中的物资耐久性退化。

Abstract: This study applies a range of forecasting techniques,including ARIMA,
Prophet, Long Short Term Memory networks (LSTM), Temporal Convolutional
Networks (TCN), and XGBoost, to model and predict Russian equipment losses
during the ongoing war in Ukraine. Drawing on daily and monthly open-source
intelligence (OSINT) data from WarSpotting, we aim to assess trends in
attrition, evaluate model performance, and estimate future loss patterns
through the end of 2025. Our findings show that deep learning models,
particularly TCN and LSTM, produce stable and consistent forecasts, especially
under conditions of high temporal granularity. By comparing different model
architectures and input structures, this study highlights the importance of
ensemble forecasting in conflict modeling, and the value of publicly available
OSINT data in quantifying material degradation over time.

</details>


### [91] [Predicting person-level injury severity using crash narratives: A balanced approach with roadway classification and natural language process techniques](https://arxiv.org/abs/2509.07845)
*Mohammad Zana Majidi,Sajjad Karimi,Teng Wang,Robert Kluger,Reginald Souleyrette*

Main category: cs.LG

TL;DR: 通过结构化数据与话语报告结合，使用NLP技术提高交通事故伤害严重程度预测准确度，TF-IDF+XGBoost组合效果最佳


<details>
  <summary>Details</summary>
Motivation: 预测交通事故伤害严重程度对提升道路安全、改善应急响应和指导公共卫生干预具有重要意义，研究话语报告在结构化数据基础上的额外价值

Method: 采用TF-IDF和Word2Vec两种NLP技术处理警察现场话语报告，使用K-NN过量采样处理类不平衡问题，基于三种道路分类方案建立102个机器学习模型，包括XGBoost、Random Forest和AdaBoost等集成算法

Result: 融合话语数据的模型在所有子组中都显著超过仅使用结构化数据的模型，其中TF-IDF结合XGBoost的组合在大多数情况下得到最准确的预测结果

Conclusion: 研究证明了整合文本和结构化交通事故信息能够显著提高个人层面伤害预测能力，为交通安全专业人员提供了一个实用且适配性强的框架，可以用于改善事故严重程度建模、指导政策决策和设计更有效的应对措施

Abstract: Predicting injuries and fatalities in traffic crashes plays a critical role
in enhancing road safety, improving emergency response, and guiding public
health interventions. This study investigates the added value of unstructured
crash narratives (written by police officers at the scene) when combined with
structured crash data to predict injury severity. Two widely used Natural
Language Processing (NLP) techniques, Term Frequency-Inverse Document Frequency
(TF-IDF) and Word2Vec, were employed to extract semantic meaning from the
narratives, and their effectiveness was compared. To address the challenge of
class imbalance, a K-Nearest Neighbors-based oversampling method was applied to
the training data prior to modeling. The dataset consists of crash records from
Kentucky spanning 2019 to 2023. To account for roadway heterogeneity, three
road classification schemes were used: (1) eight detailed functional classes
(e.g., Urban Two-Lane, Rural Interstate, Urban Multilane Divided), (2) four
broader paired categories (e.g., Urban vs. Rural, Freeway vs. Non-Freeway), and
(3) a unified dataset without classification. A total of 102 machine learning
models were developed by combining structured features and narrative-based
features using the two NLP techniques alongside three ensemble algorithms:
XGBoost, Random Forest, and AdaBoost. Results demonstrate that models
incorporating narrative data consistently outperform those relying solely on
structured data. Among all combinations, TF-IDF coupled with XGBoost yielded
the most accurate predictions in most subgroups. The findings highlight the
power of integrating textual and structured crash information to enhance
person-level injury prediction. This work offers a practical and adaptable
framework for transportation safety professionals to improve crash severity
modeling, guide policy decisions, and design more effective countermeasures.

</details>


### [92] [Addressing the Cold-Start Problem for Personalized Combination Drug Screening](https://arxiv.org/abs/2509.07850)
*Antoine de Mathelin,Christopher Tosh,Wesley Tansey*

Main category: cs.LG

TL;DR: 基于预训练深度学习模型的算法，通过药物嵌入聚类和荷重机制选择最信息化的初始药物组合测试，解决个性化癌症聚合疗法中的冷启动问题


<details>
  <summary>Details</summary>
Motivation: 个性化癌症聚合疗法面临巨大的药物组合空间，实验资源有限，且无法通过分子拓扑指导药物选择，需要解决冷启动阶段的信息化实验选择问题

Method: 利用历史药物响应数据预训练深度学习模型，生成药物组合嵌入和荷重重要性分数，通过聚类确保功能多样性，统计荷重机制优先选择历史信息量高的荷重

Result: 在大规模药物组合数据集上的回顾性模拟显示，该方法显著提高了初始筛选效率，较基线方法有显著改善

Conclusion: 该策略为个性化聚合药物筛选提供了可行的早期决策路径，通过利用历史数据的预训练模型有效解决了冷启动问题

Abstract: Personalizing combination therapies in oncology requires navigating an
immense space of possible drug and dose combinations, a task that remains
largely infeasible through exhaustive experimentation. Recent developments in
patient-derived models have enabled high-throughput ex vivo screening, but the
number of feasible experiments is limited. Further, a tight therapeutic window
makes gathering molecular profiling information (e.g. RNA-seq) impractical as a
means of guiding drug response prediction. This leads to a challenging
cold-start problem: how do we select the most informative combinations to test
early, when no prior information about the patient is available? We propose a
strategy that leverages a pretrained deep learning model built on historical
drug response data. The model provides both embeddings for drug combinations
and dose-level importance scores, enabling a principled selection of initial
experiments. We combine clustering of drug embeddings to ensure functional
diversity with a dose-weighting mechanism that prioritizes doses based on their
historical informativeness. Retrospective simulations on large-scale drug
combination datasets show that our method substantially improves initial
screening efficiency compared to baselines, offering a viable path for more
effective early-phase decision-making in personalized combination drug screens.

</details>


### [93] [Leveraging Support Vector Regression for Outcome Prediction in Personalized Ultra-fractionated Stereotactic Adaptive Radiotherapy](https://arxiv.org/abs/2509.07872)
*Yajun Yu,Steve Jiang,Robert Timmerman,Hao Peng*

Main category: cs.LG

TL;DR: 基于多组学数据的SVR模型预测脱射治疗中脑转移疾病的肿瘤体积变化，结合影像组学和治疗组学特征，取得了较好的预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确预测肿瘤体积变化对个性化脱射治疗方案具有重要的预后价值，特别是在PULSAR这种新型间隔脱射治疗中。

Method: 采用39名患者的69个脑转移疾病进行回顾性分析，提取影像组学和治疗组学特征，计算相对变化的delta特征，使用Lasso算法进行特征选择，构建多种内核的SVR模型，采用5折交叉验证。

Result: 多组学模型表现最佳，R2达到0.743，RRMSE为0.022，delta影像组学特征显著提升了预测准确性。

Conclusion: 该多组学SVR模型能够有效预测GTV连续变化，为PULSAR治疗中的患者选择和治疗调整提供了量化的个性化方法。

Abstract: Personalized ultra-fractionated stereotactic adaptive radiotherapy (PULSAR)
is a novel treatment that delivers radiation in pulses of protracted intervals.
Accurate prediction of gross tumor volume (GTV) changes through regression
models has substantial prognostic value. This study aims to develop a
multi-omics based support vector regression (SVR) model for predicting GTV
change. A retrospective cohort of 39 patients with 69 brain metastases was
analyzed, based on radiomics (MRI images) and dosiomics (dose maps) features.
Delta features were computed to capture relative changes between two time
points. A feature selection pipeline using least absolute shrinkage and
selection operator (Lasso) algorithm with weight- or frequency-based ranking
criterion was implemented. SVR models with various kernels were evaluated using
the coefficient of determination (R2) and relative root mean square error
(RRMSE). Five-fold cross-validation with 10 repeats was employed to mitigate
the limitation of small data size. Multi-omics models that integrate radiomics,
dosiomics, and their delta counterparts outperform individual-omics models.
Delta-radiomic features play a critical role in enhancing prediction accuracy
relative to features at single time points. The top-performing model achieves
an R2 of 0.743 and an RRMSE of 0.022. The proposed multi-omics SVR model shows
promising performance in predicting continuous change of GTV. It provides a
more quantitative and personalized approach to assist patient selection and
treatment adjustment in PULSAR.

</details>


### [94] [A Survey of Graph Neural Networks for Drug Discovery: Recent Developments and Challenges](https://arxiv.org/abs/2509.07887)
*Katherine Berry,Liang Cheng*

Main category: cs.LG

TL;DR: 本文综述了图神经网络在药物发现多个领域的应用，包括分子性质预测、药物-靶点结合亲和力预测、药物相互作用研究等，并为未来研究提供指导


<details>
  <summary>Details</summary>
Motivation: 图神经网络能够处理药物分子等图结构数据，在药物发现领域得到了广泛应用，但缺乏对各类研究方法的全面综述和未来方向指导

Method: 对近期文献进行全面综述，涵盖分子性质预测、药物-靶点结合亲和力预测、药物-药物相互作用研究、微生物组相互作用预测、药物重定位、逆合成和新药设计等多个研究类别

Result: 系统梳理了图神经网络在药物发现各子领域的应用方法和模型，总结了当前研究现状

Conclusion: 为图神经网络在药物发现领域的未来研究提供了指导性建议和方向

Abstract: Graph Neural Networks (GNNs) have gained traction in the complex domain of
drug discovery because of their ability to process graph-structured data such
as drug molecule models. This approach has resulted in a myriad of methods and
models in published literature across several categories of drug discovery
research. This paper covers the research categories comprehensively with recent
papers, namely molecular property prediction, including drug-target binding
affinity prediction, drug-drug interaction study, microbiome interaction
prediction, drug repositioning, retrosynthesis, and new drug design, and
provides guidance for future work on GNNs for drug discovery.

</details>


### [95] [Feasibility of In-Ear Single-Channel ExG for Wearable Sleep~Monitoring in Real-World Settings](https://arxiv.org/abs/2509.07896)
*Philipp Lepold,Jonas Leichtle,Tobias Röddiger,Michael Beigl*

Main category: cs.LG

TL;DR: 研究探索使用单通道耳内电生理信号进行自动睡眠分期的可行性，通过定制耳塞设备实现了90.5%的二元睡眠检测准确率和65.1%的四分类睡眠分期准确率


<details>
  <summary>Details</summary>
Motivation: 传统EEG睡眠监测设备虽然准确但笨重不便，限制了在家庭环境等真实场景中的长期连续监测应用。耳内EEG设备有望提供无创、舒适的睡眠监测解决方案

Method: 使用定制耳塞设备（干耳尖电极）采集11名参与者的单通道耳内电生理信号，以Apple Watch Ultra的睡眠分期作为地面真值，采用留一法交叉验证

Result: 二元睡眠检测（清醒vs睡眠）准确率达到90.5%，四分类睡眠分期（清醒、REM、核心睡眠、深睡）准确率达到65.1%

Conclusion: 耳内电极显示出作为低负担、舒适睡眠监测方法的潜力，特别适用于用户入睡时自动暂停媒体播放等消费级应用

Abstract: Automatic sleep staging typically relies on gold-standard EEG setups, which
are accurate but obtrusive and impractical for everyday use outside sleep
laboratories. This limits applicability in real-world settings, such as home
environments, where continuous, long-term monitoring is needed. Detecting sleep
onset is particularly relevant, enabling consumer applications (e.g.
automatically pausing media playback when the user falls asleep). Recent
research has shown correlations between in-ear EEG and full-scalp EEG for
various phenomena, suggesting wearable, in-ear devices could allow unobtrusive
sleep monitoring. We investigated the feasibility of using single-channel
in-ear electrophysiological (ExG) signals for automatic sleep staging in a
wearable device by conducting a sleep study with 11~participants (mean age:
24), using a custom earpiece with a dry eartip electrode (D\"atwyler SoftPulse)
as a measurement electrode in one ear and a reference in the other. Ground
truth sleep stages were obtained from an Apple Watch Ultra, validated for sleep
staging. Our system achieved 90.5% accuracy for binary sleep detection (Awake
vs. Asleep) and 65.1% accuracy for four-class staging (Awake, REM, Core, Deep)
using leave-one-subject-out validation. These findings demonstrate the
potential of in-ear electrodes as a low-effort, comfortable approach to sleep
monitoring, with applications such as stopping podcasts when users fall asleep.

</details>


### [96] [A Modular Algorithm for Non-Stationary Online Convex-Concave Optimization](https://arxiv.org/abs/2509.07901)
*Qing-xin Meng,Xia Lei,Jian-wei Liu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种模块化算法，解决在线凸凹优化问题中的动态对偶间隔最小化问题，达到了近优的性能界限。


<details>
  <summary>Details</summary>
Motivation: 现有算法在静态或可预测环境中无法获得最优性能，特别是在动态对偶间隔（D-DGap）这个关键性能指标方面。

Method: 提出包含三个核心模块的模块化算法：适应性模块动态调整非静态性级别，多预测器聚合器选择最佳预测器，集成模块有效结合各模块优势。

Result: 算法达到了最小最大近优的D-DGap上界（至多对数因子），同时保证了预测错误驱动的D-DGap界限。

Conclusion: 模块化设计允许灵活替换适应动态环境的组件，并能集成多种预测器的"边知识"，实验结果验证了方法的有效性和适应能力。

Abstract: This paper investigates the problem of Online Convex-Concave Optimization,
which extends Online Convex Optimization to two-player time-varying
convex-concave games. The goal is to minimize the dynamic duality gap (D-DGap),
a critical performance measure that evaluates players' strategies against
arbitrary comparator sequences. Existing algorithms fail to deliver optimal
performance, particularly in stationary or predictable environments. To address
this, we propose a novel modular algorithm with three core components: an
Adaptive Module that dynamically adjusts to varying levels of non-stationarity,
a Multi-Predictor Aggregator that identifies the best predictor among multiple
candidates, and an Integration Module that effectively combines their
strengths. Our algorithm achieves a minimax optimal D-DGap upper bound, up to a
logarithmic factor, while also ensuring prediction error-driven D-DGap bounds.
The modular design allows for the seamless replacement of components that
regulate adaptability to dynamic environments, as well as the incorporation of
components that integrate ``side knowledge'' from multiple predictors.
Empirical results further demonstrate the effectiveness and adaptability of the
proposed method.

</details>


### [97] [Bio-KGvec2go: Serving up-to-date Dynamic Biomedical Knowledge Graph Embeddings](https://arxiv.org/abs/2509.07905)
*Hamid Ahmad,Heiko Paulheim,Rita T. Sousa*

Main category: cs.LG

TL;DR: Bio-KGvec2go是一个扩展的Web API，用于生成和服务生物医学本体的知识图谱嵌入，支持定期更新以保持与本体版本同步，促进高效生物医学研究。


<details>
  <summary>Details</summary>
Motivation: 知识图谱和本体在现代AI应用中日益重要，但需要将语义资源与机器学习模型集成。预训练模型可以避免重复训练，促进AI开发的民主化和可持续计算。

Method: 扩展KGvec2go Web API，专门为广泛使用的生物医学本体生成和提供知识图谱嵌入服务，并支持与本体版本发布对齐的定期更新。

Result: 开发了Bio-KGvec2go系统，能够为生物医学本体提供最新的嵌入表示，用户只需最小计算努力即可获得最新嵌入。

Conclusion: Bio-KGvec2go通过提供最新且易于获取的生物医学本体嵌入，有效促进了高效和及时的生物医学研究发展。

Abstract: Knowledge graphs and ontologies represent entities and their relationships in
a structured way, having gained significance in the development of modern AI
applications. Integrating these semantic resources with machine learning models
often relies on knowledge graph embedding models to transform graph data into
numerical representations. Therefore, pre-trained models for popular knowledge
graphs and ontologies are increasingly valuable, as they spare the need to
retrain models for different tasks using the same data, thereby helping to
democratize AI development and enabling sustainable computing.
  In this paper, we present Bio-KGvec2go, an extension of the KGvec2go Web API,
designed to generate and serve knowledge graph embeddings for widely used
biomedical ontologies. Given the dynamic nature of these ontologies,
Bio-KGvec2go also supports regular updates aligned with ontology version
releases. By offering up-to-date embeddings with minimal computational effort
required from users, Bio-KGvec2go facilitates efficient and timely biomedical
research.

</details>


### [98] [One Model for All Tasks: Leveraging Efficient World Models in Multi-Task Planning](https://arxiv.org/abs/2509.07945)
*Yuan Pu,Yazhe Niu,Jia Tang,Junyu Xiong,Shuai Hu,Hongsheng Li*

Main category: cs.LG

TL;DR: ScaleZero通过MoE架构和动态参数缩放策略解决多任务学习中的梯度冲突和模型塑性损失问题，在多个基准测试中达到与专用单任务模型相当的性能，且只需80%的环境交互步骤。


<details>
  <summary>Details</summary>
Motivation: 传统多任务世界模型在处理大规模异构环境时面临梯度冲突和模型塑性损失的问题，限制了样本和计算效率。

Method: 采用混合专家(MoE)架构缓解梯度冲突，并引入基于LoRA的动态参数缩放(DPS)策略来动态平衡计算负载。

Result: 在Atari、DMControl和Jericho等基准测试中，ScaleZero仅使用在线强化学习就达到与专用单任务基线相当的性能，配合DPS策略后只需80%的环境交互步骤。

Conclusion: ScaleZero展示了在大规模多任务学习中有效处理异构任务的潜力，为多任务强化学习提供了新的解决方案。

Abstract: In heterogeneous multi-task learning, tasks not only exhibit diverse
observation and action spaces but also vary substantially in intrinsic
difficulty. While conventional multi-task world models like UniZero excel in
single-task settings, we find that when handling large-scale heterogeneous
environments, gradient conflicts and the loss of model plasticity often
constrain their sample and computational efficiency. In this work, we address
these challenges from two perspectives: the single learning iteration and the
overall learning process. First, we investigate the impact of key design spaces
on extending UniZero to multi-task planning. We find that a Mixture-of-Experts
(MoE) architecture provides the most substantial performance gains by
mitigating gradient conflicts, leading to our proposed model,
\textit{ScaleZero}. Second, to dynamically balance the computational load
across the learning process, we introduce an online, LoRA-based \textit{dynamic
parameter scaling} (DPS) strategy. This strategy progressively integrates LoRA
adapters in response to task-specific progress, enabling adaptive knowledge
retention and parameter expansion. Empirical evaluations on standard benchmarks
such as Atari, DMControl (DMC), and Jericho demonstrate that ScaleZero, relying
exclusively on online reinforcement learning with one model, attains
performance on par with specialized single-task baselines. Furthermore, when
augmented with our dynamic parameter scaling strategy, our method achieves
competitive performance while requiring only 80\% of the single-task
environment interaction steps. These findings underscore the potential of
ScaleZero for effective large-scale multi-task learning. Our code is available
at \textcolor{magenta}{https://github.com/opendilab/LightZero}.

</details>


### [99] [Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges](https://arxiv.org/abs/2509.07946)
*Kasra Borazjani,Naji Khosravan,Rajeev Sahay,Bita Akram,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: 多模态多任务基础模型在教育领域遇到隐私和数据封闭挑战，本文提出联邦学习基础模型解决方案，并探讨了其在隐私保护、个性化和公平性方面的潜力以及面临的研究挑战。


<details>
  <summary>Details</summary>
Motivation: 多模态多任务基础模型在教育领域具有转型潜力，但遇到隐私法规、数据封闭和领域特定数据稀缺的挑战，需要找到新的解决方案来支持实际部署。

Method: 提出M3T联邦基础模型(FedFMs)，将联邦学习(FL)与M3T基础模型相结合，支持在去中心化机构之间进行协作性、隐私保护的训练，同时适应多样化模态和任务。

Result: 论文描述了M3T FedFMs如何推动下一代智能教育系统的三大核心架构：隐私保护(保持敏感数据本地化)、个性化(通过模块化架构实现定制模型)、公平性和包容性(支持资源受限实体参与)。

Conclusion: M3T FedFMs是一种有前景但研究不足的方法，未来需要重点研究机构间异构隐私规定、数据模态特性不均匀性、模型删除技术、持续学习框架和模型可解释性等关键挑战，以支持实际部署。

Abstract: Multi-modal multi-task (M3T) foundation models (FMs) have recently shown
transformative potential in artificial intelligence, with emerging applications
in education. However, their deployment in real-world educational settings is
hindered by privacy regulations, data silos, and limited domain-specific data
availability. We introduce M3T Federated Foundation Models (FedFMs) for
education: a paradigm that integrates federated learning (FL) with M3T FMs to
enable collaborative, privacy-preserving training across decentralized
institutions while accommodating diverse modalities and tasks. Subsequently,
this position paper aims to unveil M3T FedFMs as a promising yet underexplored
approach to the education community, explore its potentials, and reveal its
related future research directions. We outline how M3T FedFMs can advance three
critical pillars of next-generation intelligent education systems: (i) privacy
preservation, by keeping sensitive multi-modal student and institutional data
local; (ii) personalization, through modular architectures enabling tailored
models for students, instructors, and institutions; and (iii) equity and
inclusivity, by facilitating participation from underrepresented and
resource-constrained entities. We finally identify various open research
challenges, including studying of (i) inter-institution heterogeneous privacy
regulations, (ii) the non-uniformity of data modalities' characteristics, (iii)
the unlearning approaches for M3T FedFMs, (iv) the continual learning
frameworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must
be collectively addressed for practical deployment.

</details>


### [100] [ACE and Diverse Generalization via Selective Disagreement](https://arxiv.org/abs/2509.07955)
*Oliver Daniels,Stuart Armstrong,Alexandre Maranhão,Mahirah Fairuz Rahman,Benjamin M. Marlin,Rebecca Gorman*

Main category: cs.LG

TL;DR: ACE方法通过自训练学习一组概念来解决完全伪相关性问题，在保持对不完全伪相关鲁棒性的同时，在多个基准测试中达到或超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络对伪相关性非常敏感，现有方法主要关注不完全伪相关，但在完全伪相关情况下正确泛化是根本未指定的，需要新的解决方案。

Method: 使用自训练方法学习一组与训练数据一致但在新未标记输入上做出不同预测的概念，鼓励自信和选择性分歧。

Result: ACE在完全伪相关基准测试中匹配或优于现有方法，在语言模型对齐的早期应用中，在没有访问不可信测量的情况下在测量篡改检测基准上达到竞争性性能。

Conclusion: ACE在克服未指定问题方面取得了显著进展，虽然仍受重要限制，但提供了比先前方法更可配置的解决方案，允许直接编码先验知识和原则性无监督模型选择。

Abstract: Deep neural networks are notoriously sensitive to spurious correlations -
where a model learns a shortcut that fails out-of-distribution. Existing work
on spurious correlations has often focused on incomplete
correlations,leveraging access to labeled instances that break the correlation.
But in cases where the spurious correlations are complete, the correct
generalization is fundamentally \textit{underspecified}. To resolve this
underspecification, we propose learning a set of concepts that are consistent
with training data but make distinct predictions on a subset of novel unlabeled
inputs. Using a self-training approach that encourages \textit{confident} and
\textit{selective} disagreement, our method ACE matches or outperforms existing
methods on a suite of complete-spurious correlation benchmarks, while remaining
robust to incomplete spurious correlations. ACE is also more configurable than
prior approaches, allowing for straight-forward encoding of prior knowledge and
principled unsupervised model selection. In an early application to
language-model alignment, we find that ACE achieves competitive performance on
the measurement tampering detection benchmark \textit{without} access to
untrusted measurements. While still subject to important limitations, ACE
represents significant progress towards overcoming underspecification.

</details>


### [101] [Customizing the Inductive Biases of Softmax Attention using Structured Matrices](https://arxiv.org/abs/2509.07963)
*Yilun Kuang,Noah Amsel,Sanae Lotfi,Shikai Qiu,Andres Potapczynski,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 通过使用高秩结构化矩阵（BTT和MLR）改进注意力计分函数，解决标准注意力的低维信息损失和缺乏距离依赖计算偏置的问题


<details>
  <summary>Details</summary>
Motivation: 标准注意力的低维投影导致高维输入信息损失，且缺乏对序列中相邻元素的距离依赖计算偏置

Method: 提出基于高秩结构化矩阵的新计分函数，包括块张量连（BTT）和多级低秩（MLR）矩阵

Result: 在高维输入上下文回归任务中超过标准注意力；在语言建模中改善缩放律；在长程时间序列预测中展现突出性能

Conclusion: 结构化矩阵方法能够有效解决注意力机制的核心缺陷，为注意力模型提供了更灵活的计算偏置编码能力

Abstract: The core component of attention is the scoring function, which transforms the
inputs into low-dimensional queries and keys and takes the dot product of each
pair. While the low-dimensional projection improves efficiency, it causes
information loss for certain tasks that have intrinsically high-dimensional
inputs. Additionally, attention uses the same scoring function for all input
pairs, without imposing a distance-dependent compute bias for neighboring
tokens in the sequence. In this work, we address these shortcomings by
proposing new scoring functions based on computationally efficient structured
matrices with high ranks, including Block Tensor-Train (BTT) and Multi-Level
Low Rank (MLR) matrices. On in-context regression tasks with high-dimensional
inputs, our proposed scoring functions outperform standard attention for any
fixed compute budget. On language modeling, a task that exhibits locality
patterns, our MLR-based attention method achieves improved scaling laws
compared to both standard attention and variants of sliding window attention.
Additionally, we show that both BTT and MLR fall under a broader family of
efficient structured matrices capable of encoding either full-rank or
distance-dependent compute biases, thereby addressing significant shortcomings
of standard attention. Finally, we show that MLR attention has promising
results for long-range time-series forecasting.

</details>


### [102] [Theoretical Analysis on how Learning Rate Warmup Accelerates Convergence](https://arxiv.org/abs/2509.07972)
*Yuxing Liu,Yuze Ge,Rui Pan,An Kang,Tong Zhang*

Main category: cs.LG

TL;DR: 论文提出了新的广义平滑性假设，从优化理论角度证明了学习率预热策略能够加速梯度下降收敛，在某些情况下可比非递增学习率快Θ(T)倍


<details>
  <summary>Details</summary>
Motivation: 尽管学习率预热在实践中被广泛使用且效果显著，但其理论优势尚未得到充分理解，存在理论与实践之间的差距

Method: 提出新的广义平滑性假设家族，并在理论和实证上验证其适用性；在该假设下研究确定性和随机设置中梯度下降的收敛性质

Result: 学习率预热一致地加速梯度下降，在某些特定情况下，带预热的学习率可比非递增学习率快Θ(T)倍收敛

Conclusion: 该研究从优化理论视角为学习率预热策略的优势提供了理论依据，填补了理论与实践之间的理解空白

Abstract: Learning rate warmup is a popular and practical technique in training
large-scale deep neural networks. Despite the huge success in practice, the
theoretical advantages of this strategy of gradually increasing the learning
rate at the beginning of the training process have not been fully understood.
To resolve this gap between theory and practice, we first propose a novel
family of generalized smoothness assumptions, and validate its applicability
both theoretically and empirically. Under the novel smoothness assumption, we
study the convergence properties of gradient descent (GD) in both deterministic
and stochastic settings. It is shown that learning rate warmup consistently
accelerates GD, and GD with warmup can converge at most $\Theta(T)$ times
faster than with a non-increasing learning rate schedule in some specific
cases, providing insights into the benefits of this strategy from an
optimization theory perspective.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [103] [Renewable Energy Sources Selection Analysis with the Maximizing Deviation Method](https://arxiv.org/abs/2509.07011)
*Kirisci Murat*

Main category: cs.AI

TL;DR: 该研究提出了一种基于偏差最大化方法的优化模型，结合区间值Fermatean模糊集来处理多准则决策中的不确定性和模糊性，并应用于可再生能源选择问题。


<details>
  <summary>Details</summary>
Motivation: 多准则决策方法需要有效处理决策者判断中的不确定性和模糊性，特别是在复杂和冲突的情况下。Fermatean模糊环境作为模糊集的推广，能够更好地量化人类思维和感知中的不确定性。

Method: 提出基于偏差最大化方法的优化模型来确定部分已知特征权重，并将该方法与区间值Fermatean模糊集相结合。

Result: 该方法成功应用于可再生能源选择问题，能够处理技术、管理和政治等多方面的复杂决策因素。

Conclusion: 结合Fermatean模糊集和偏差最大化方法的优化模型为多准则决策提供了有效的工具，特别适用于处理可再生能源选择等具有不确定性和多维度影响的复杂决策问题。

Abstract: Multi-criteria decision-making methods provide decision-makers with
appropriate tools to make better decisions in uncertain, complex, and
conflicting situations. Fuzzy set theory primarily deals with the uncertainty
inherent in human thoughts and perceptions and attempts to quantify this
uncertainty. Fuzzy logic and fuzzy set theory are utilized with multi-criteria
decision-making methods because they effectively handle uncertainty and
fuzziness in decision-makers' judgments, allowing for verbal judgments of the
problem. This study utilizes the Fermatean fuzzy environment, a generalization
of fuzzy sets. An optimization model based on the deviation maximization method
is proposed to determine partially known feature weights. This method is
combined with interval-valued Fermatean fuzzy sets. The proposed method was
applied to the problem of selecting renewable energy sources. The reason for
choosing renewable energy sources is that meeting energy needs from renewable
sources, balancing carbon emissions, and mitigating the effects of global
climate change are among the most critical issues of the recent period. Even
though selecting renewable energy sources is a technical issue, the managerial
and political implications of this issue are also important, and are discussed
in this study.

</details>


### [104] [From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning](https://arxiv.org/abs/2509.07017)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.AI

TL;DR: Spectral NSR是一个完全频谱的神经符号推理框架，通过将逻辑规则嵌入为频谱模板并在图频谱域直接进行推理，结合了符号推理的可解释性和频谱学习的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统推理系统在可扩展性、适应性和可解释性方面的局限性，作者希望开发一个能够统一符号推理和神经网络优势的框架。

Method: 利用图信号处理(GSP)和基于拉普拉斯特征结构的频率选择性滤波器，将逻辑规则嵌入为频谱模板。包含动态图学习、有理和扩散滤波器、频谱专家混合、证明引导训练等多种扩展技术。

Result: 在ProofWriter和CLUTRR等推理基准测试中，Spectral NSR相比transformer、消息传递神经网络和神经符号逻辑编程系统，实现了更高的准确性、更快的推理速度、更好的抗干扰鲁棒性和更强的可解释性。

Conclusion: Spectral NSR为下一代推理系统提供了一个可扩展且原则性的基础框架，在透明度、鲁棒性和泛化能力方面超越了传统方法。

Abstract: We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning
framework that embeds logical rules as spectral templates and performs
inference directly in the graph spectral domain. By leveraging graph signal
processing (GSP) and frequency-selective filters grounded in the Laplacian
eigenstructure of knowledge graphs, the architecture unifies the
interpretability of symbolic reasoning with the scalability and adaptability of
spectral learning. Beyond the core formulation, we incorporate a comprehensive
set of extensions, including dynamic graph and basis learning, rational and
diffusion filters for sharper spectral selectivity, mixture-of-spectral-experts
for modular specialization, proof-guided training with spectral curricula, and
uncertainty quantification for calibrated confidence. Additional enhancements
such as large language model coupling, co-spectral transfer alignment,
adversarial robustness, efficient GPU kernels, generalized Laplacians, and
causal interventions further expand the versatility of the framework.
  Empirical evaluation on state-of-the-art reasoning benchmarks such as
ProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior
accuracy, faster inference, improved robustness to adversarial perturbations,
and higher interpretability compared to leading baselines including
transformers, message-passing neural networks, and neuro-symbolic logic
programming systems. Spectral attribution and proof-band agreement analyses
confirm that model decisions align closely with symbolic proof structures,
while transfer experiments validate effective domain adaptation through
co-spectral alignment. These results establish Spectral NSR as a scalable and
principled foundation for the next generation of reasoning systems, offering
transparency, robustness, and generalization beyond conventional approaches.

</details>


### [105] [Statistical Methods in Generative AI](https://arxiv.org/abs/2509.07054)
*Edgar Dobriban*

Main category: cs.AI

TL;DR: 本文综述了统计学方法在提升生成式AI可靠性、评估质量和效率以及设计AI干预实验方面的应用，同时讨论了现有局限性和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术基于概率模型采样，缺乏对正确性、安全性、公平性等属性的保证，需要统计学方法来提高其可靠性。

Method: 综述性研究方法，回顾现有统计学技术在生成式AI中的应用，包括一般统计技术和具体应用案例。

Result: 统计学方法为生成式AI提供了提高可靠性的潜在途径，同时在AI评估质量和效率提升以及实验设计方面也展现出前景。

Conclusion: 统计学方法对提升生成式AI的可靠性具有重要意义，但现有研究仍存在局限性，需要进一步探索未来发展方向。

Abstract: Generative Artificial Intelligence is emerging as an important technology,
promising to be transformative in many areas. At the same time, generative AI
techniques are based on sampling from probabilistic models, and by default,
they come with no guarantees about correctness, safety, fairness, or other
properties. Statistical methods offer a promising potential approach to improve
the reliability of generative AI techniques. In addition, statistical methods
are also promising for improving the quality and efficiency of AI evaluation,
as well as for designing interventions and experiments in AI.
  In this paper, we review some of the existing work on these topics,
explaining both the general statistical techniques used, as well as their
applications to generative AI. We also discuss limitations and potential future
directions.

</details>


### [106] [Instruction Agent: Enhancing Agent with Expert Demonstration](https://arxiv.org/abs/2509.07098)
*Yinheng Li,Hailey Hultquist,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: Instruction Agent是一个GUI代理，通过专家演示来执行复杂任务，严格遵循用户轨迹，避免执行错误，并利用验证器和回溯器模块提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理在处理涉及新颖UI元素、长时程操作和个性化轨迹的复杂任务时仍存在困难，需要更可靠的解决方案。

Method: 基于单次专家演示提取分步指令，严格遵循用户轨迹执行，并采用验证器和回溯器模块来处理意外中断和验证执行结果。

Result: 在OSWorld测试集上实现了60%的成功率，而所有顶级代理都无法完成这些任务。

Conclusion: Instruction Agent提供了一个实用且可扩展的框架，弥合了当前GUI代理与可靠的真实世界GUI任务自动化之间的差距。

Abstract: Graphical user interface (GUI) agents have advanced rapidly but still
struggle with complex tasks involving novel UI elements, long-horizon actions,
and personalized trajectories. In this work, we introduce Instruction Agent, a
GUI agent that leverages expert demonstrations to solve such tasks, enabling
completion of otherwise difficult workflows. Given a single demonstration, the
agent extracts step-by-step instructions and executes them by strictly
following the trajectory intended by the user, which avoids making mistakes
during execution. The agent leverages the verifier and backtracker modules
further to improve robustness. Both modules are critical to understand the
current outcome from each action and handle unexpected interruptions(such as
pop-up windows) during execution. Our experiments show that Instruction Agent
achieves a 60% success rate on a set of tasks in OSWorld that all top-ranked
agents failed to complete. The Instruction Agent offers a practical and
extensible framework, bridging the gap between current GUI agents and reliable
real-world GUI task automation.

</details>


### [107] [Neuro-Symbolic Frameworks: Conceptual Characterization and Empirical Comparative Analysis](https://arxiv.org/abs/2509.07122)
*Sania Sinha,Tanawan Premsri,Danial Kamali,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 这篇论文分析了现有神经符号框架的技术特征，展示了三个典型框架，并指出了该领域的挑战与发展方向。


<details>
  <summary>Details</summary>
Motivation: 神经符号框架结合了神经计算的灵活性和符号处理的可解释性，但当前缺乏用户友好的统一框架，需要系统性分析以推动该领域发展。

Method: 通过对现有神经符号框架的技术特征进行分析，包括符号表示语言、神经模型集成和基础算法，并展示DeepProbLog、Scallop和DomiKnowS三个典型框架。

Result: 识别了各框架在解决不同问题时的表达能力特征，以及各技术方面的挑战，为评估框架性能提供了基础。

Conclusion: 该研究为神经符号计算领域的发展提供了重要的技术基础，希望能够激发社区重新思考该问题并推动创新性解决方案的发展。

Abstract: Neurosymbolic (NeSy) frameworks combine neural representations and learning
with symbolic representations and reasoning. Combining the reasoning
capacities, explainability, and interpretability of symbolic processing with
the flexibility and power of neural computing allows us to solve complex
problems with more reliability while being data-efficient. However, this
recently growing topic poses a challenge to developers with its learning curve,
lack of user-friendly tools, libraries, and unifying frameworks. In this paper,
we characterize the technical facets of existing NeSy frameworks, such as the
symbolic representation language, integration with neural models, and the
underlying algorithms. A majority of the NeSy research focuses on algorithms
instead of providing generic frameworks for declarative problem specification
to leverage problem solving. To highlight the key aspects of Neurosymbolic
modeling, we showcase three generic NeSy frameworks - \textit{DeepProbLog},
\textit{Scallop}, and \textit{DomiKnowS}. We identify the challenges within
each facet that lay the foundation for identifying the expressivity of each
framework in solving a variety of problems. Building on this foundation, we aim
to spark transformative action and encourage the community to rethink this
problem in novel ways.

</details>


### [108] [Autoencoder-Based Denoising of Muscle Artifacts in ECG to Preserve Skin Nerve Activity (SKNA) for Cognitive Stress Detection](https://arxiv.org/abs/2509.07146)
*Farnoush Baghestani,Jihye Moon,Youngsun Kong,Ki Chon*

Main category: cs.AI

TL;DR: 提出基于一维卷积自编码器和LSTM的深度学习方法来去除皮肤神经活动(SKNA)信号中的肌电(EMG)噪声，显著提升信噪比和信号质量，使在运动丰富环境中进行稳健的SKNA监测成为可能。


<details>
  <summary>Details</summary>
Motivation: 皮肤神经活动(SKNA)是评估交感神经系统的重要非侵入性指标，但易受肌电(EMG)污染。传统带通滤波方法在持续肌肉活动时效果有限，因为SKNA和EMG频谱成分重叠。

Method: 使用轻量级一维卷积自编码器配合LSTM瓶颈层，从EMG污染的SKNA记录中重建干净信号。采用留一受试者交叉验证框架，在模拟噪声水平(-4dB, -8dB SNR)下训练模型。

Result: 方法将信噪比提升高达9.65dB，与干净SKNA的互相关系数从0.40提高到0.72，基于爆发的SKNA特征恢复至接近干净的判别能力(AUROC≥0.96)。在严重噪声水平下，基线vs交感刺激条件分类准确率达到91-98%。

Conclusion: 深度学习重建方法能在显著EMG干扰下保留生理相关的交感神经爆发特征，为在自然主义、运动丰富的环境中进行更稳健的SKNA监测提供了有效解决方案。

Abstract: The sympathetic nervous system (SNS) plays a central role in regulating the
body's responses to stress and maintaining physiological stability. Its
dysregulation is associated with a wide range of conditions, from
cardiovascular disease to anxiety disorders. Skin nerve activity (SKNA)
extracted from high-frequency electrocardiogram (ECG) recordings provides a
noninvasive window into SNS dynamics, but its measurement is highly susceptible
to electromyographic (EMG) contamination. Traditional preprocessing based on
bandpass filtering within a fixed range (e.g., 500--1000 Hz) is susceptible to
overlapping EMG and SKNA spectral components, especially during sustained
muscle activity. We present a denoising approach using a lightweight
one-dimensional convolutional autoencoder with a long short-term memory (LSTM)
bottleneck to reconstruct clean SKNA from EMG-contaminated recordings. Using
clean ECG-derived SKNA data from cognitive stress experiments and EMG noise
from chaotic muscle stimulation recordings, we simulated contamination at
realistic noise levels (--4 dB, --8 dB signal-to-noise ratio) and trained the
model in the leave-one-subject-out cross-validation framework. The method
improved signal-to-noise ratio by up to 9.65 dB, increased cross correlation
with clean SKNA from 0.40 to 0.72, and restored burst-based SKNA features to
near-clean discriminability (AUROC $\geq$ 0.96). Classification of baseline
versus sympathetic stimulation (cognitive stress) conditions reached accuracies
of 91--98\% across severe noise levels, comparable to clean data. These results
demonstrate that deep learning--based reconstruction can preserve
physiologically relevant sympathetic bursts during substantial EMG
interference, enabling more robust SKNA monitoring in naturalistic,
movement-rich environments.

</details>


### [109] [PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal Reinforcement Learning](https://arxiv.org/abs/2509.07159)
*Heng Hao,Wenjun Hu,Oxana Verkholyak,Davoud Ataee Tarzanagh,Baruch Gutow,Sima Didari,Masoud Faraki,Hankyu Moon,Seungjai Min*

Main category: cs.AI

TL;DR: PaVeRL-SQL是一个结合部分匹配奖励和语言强化学习的框架，用于提升文本到SQL模型的执行准确率，在工业级数据库和复杂问题上达到SOTA效果


<details>
  <summary>Details</summary>
Motivation: 当前文本到SQL方法在工业级数据库和涉及领域特定业务逻辑的复杂问题上执行准确率较低，需要更有效的解决方案

Method: 采用两种管道：(1)基于大语言模型的上下文学习框架和组自评估(verbal-RL)；(2)基于小模型(OmniSQL-7B)的思维链强化学习管道，使用特殊设计的奖励函数和两阶段RL训练

Result: 在Spider、Spider 2.0和BIRD基准测试中达到SOTA，Spider2.0-SQLite上verbal-RL管道比SOTA高7.4%，CoT管道高1.4%。混合SQL方言训练带来三倍增益

Conclusion: PaVeRL-SQL在现实工业约束下提供了可靠、SOTA级别的文本到SQL解决方案

Abstract: Text-to-SQL models allow users to interact with a database more easily by
generating executable SQL statements from natural-language questions. Despite
recent successes on simpler databases and questions, current Text-to-SQL
methods still suffer from low execution accuracy on industry-scale databases
and complex questions involving domain-specific business logic. We present
\emph{PaVeRL-SQL}, a framework that combines \emph{Partial-Match Rewards} and
\emph{Verbal Reinforcement Learning} to drive self-improvement in reasoning
language models (RLMs) for Text-to-SQL. To handle practical use cases, we adopt
two pipelines: (1) a newly designed in-context learning framework with group
self-evaluation (verbal-RL), using capable open- and closed-source large
language models (LLMs) as backbones; and (2) a chain-of-thought (CoT) RL
pipeline with a small backbone model (OmniSQL-7B) trained with a specially
designed reward function and two-stage RL. These pipelines achieve
state-of-the-art (SOTA) results on popular Text-to-SQL benchmarks -- Spider,
Spider 2.0, and BIRD. For the industrial-level Spider2.0-SQLite benchmark, the
verbal-RL pipeline achieves an execution accuracy 7.4\% higher than SOTA, and
the CoT pipeline is 1.4\% higher. RL training with mixed SQL dialects yields
strong, threefold gains, particularly for dialects with limited training data.
Overall, \emph{PaVeRL-SQL} delivers reliable, SOTA Text-to-SQL under realistic
industrial constraints. The code is available at
https://github.com/PaVeRL-SQL/PaVeRL-SQL.

</details>


### [110] [That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral](https://arxiv.org/abs/2509.07170)
*Quinten Steenhuis*

Main category: cs.AI

TL;DR: FETCH分类器用于法律问题分类，通过混合LLM/ML集成方法和自动生成后续问题来提高准确性，在真实数据集上达到97.37%的准确率，性能超过GPT-5模型。


<details>
  <summary>Details</summary>
Motivation: 每年数百万人通过法律援助热线、办公室或律师转介服务寻求法律帮助，错误分类会导致严重后果（错过截止日期、遭受虐待、失去住房或子女监护权等），需要准确的法律问题分类来确保用户获得正确的法律帮助。

Method: 采用混合LLM/ML集成分类方法，结合自动生成后续问题来丰富初始问题叙述，使用包含419个真实查询的新数据集进行训练和评估。

Result: 分类准确率（hits@2）达到97.37%，使用成本较低的模型组合，性能超过了当前最先进的GPT-5模型。

Conclusion: 该方法在显著降低法律系统用户引导成本的同时实现了高准确性，有望改善法律援助服务的效率和效果。

Abstract: Each year millions of people seek help for their legal problems by calling a
legal aid program hotline, walking into a legal aid office, or using a lawyer
referral service. The first step to match them to the right help is to identify
the legal problem the applicant is experiencing. Misdirection has consequences.
Applicants may miss a deadline, experience physical abuse, lose housing or lose
custody of children while waiting to connect to the right legal help. We
introduce and evaluate the FETCH classifier for legal issue classification and
describe two methods for improving accuracy: a hybrid LLM/ML ensemble
classification method, and the automatic generation of follow-up questions to
enrich the initial problem narrative. We employ a novel data set of 419
real-world queries to a nonprofit lawyer referral service. Ultimately, we show
classification accuracy (hits@2) of 97.37\% using a mix of inexpensive models,
exceeding the performance of the current state-of-the-art GPT-5 model. Our
approach shows promise in significantly reducing the cost of guiding users of
the legal system to the right resource for their problem while achieving high
accuracy.

</details>


### [111] [A Hybrid CNN-LSTM Deep Learning Model for Intrusion Detection in Smart Grid](https://arxiv.org/abs/2509.07208)
*Abdulhakim Alsaiari,Mohammad Ilyas*

Main category: cs.AI

TL;DR: 提出基于CNN-LSTM混合深度学习模型的入侵检测系统，用于保护智能电网安全，检测准确率达到99.70%


<details>
  <summary>Details</summary>
Motivation: 智能电网的发展增加了网络攻击风险，SCADA协议存在未授权访问和拒绝服务等漏洞，需要有效的入侵检测系统

Method: 结合CNN的特征提取能力和LSTM的时间模式识别能力，构建混合深度学习模型，使用DNP3和IEC104数据集进行训练和测试

Result: 相比其他深度学习方法，在准确率、精确率、召回率和F1分数方面都有显著提升，检测准确率达到99.70%

Conclusion: CNN-LSTM混合模型能有效识别和分类智能电网中的网络威胁，为智能电网网络安全提供了有效的解决方案

Abstract: The evolution of the traditional power grid into the "smart grid" has
resulted in a fundamental shift in energy management, which allows the
integration of renewable energy sources with modern communication technology.
However, this interconnection has increased smart grids' vulnerability to
attackers, which might result in privacy breaches, operational interruptions,
and massive outages. The SCADA-based smart grid protocols are critical for
real-time data collection and control, but they are vulnerable to attacks like
unauthorized access and denial of service (DoS). This research proposes a
hybrid deep learning-based Intrusion Detection System (IDS) intended to improve
the cybersecurity of smart grids. The suggested model takes advantage of
Convolutional Neural Networks' (CNN) feature extraction capabilities as well as
Long Short-Term Memory (LSTM) networks' temporal pattern recognition skills.
DNP3 and IEC104 intrusion detection datasets are employed to train and test our
CNN-LSTM model to recognize and classify the potential cyber threats. Compared
to other deep learning approaches, the results demonstrate considerable
improvements in accuracy, precision, recall, and F1-score, with a detection
accuracy of 99.70%.

</details>


### [112] [BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions](https://arxiv.org/abs/2509.07209)
*Nicholas Sung,Steven Spreizer,Mohamed Elrefaie,Kaira Samuel,Matthew C. Jones,Faez Ahmed*

Main category: cs.AI

TL;DR: BlendedNet是一个包含999个混合翼体(BWB)气动几何的公开数据集，包含8830个RANS模拟案例，并提出了端到端的点状气动预测代理框架。


<details>
  <summary>Details</summary>
Motivation: 解决非常规构型气动数据稀缺问题，促进数据驱动的气动设计代理建模研究

Method: 通过采样几何设计参数和飞行条件生成数据集，使用PointNet回归器预测几何参数，然后通过FiLM网络结合预测参数和飞行条件来预测点状系数

Result: 在不同BWB构型上实现了较低的表面预测误差

Conclusion: BlendedNet为非常规构型提供了宝贵的数据资源，并展示了端到端代理框架在气动预测中的有效性

Abstract: BlendedNet is a publicly available aerodynamic dataset of 999 blended wing
body (BWB) geometries. Each geometry is simulated across about nine flight
conditions, yielding 8830 converged RANS cases with the Spalart-Allmaras model
and 9 to 14 million cells per case. The dataset is generated by sampling
geometric design parameters and flight conditions, and includes detailed
pointwise surface quantities needed to study lift and drag. We also introduce
an end-to-end surrogate framework for pointwise aerodynamic prediction. The
pipeline first uses a permutation-invariant PointNet regressor to predict
geometric parameters from sampled surface point clouds, then conditions a
Feature-wise Linear Modulation (FiLM) network on the predicted parameters and
flight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.
Experiments show low errors in surface predictions across diverse BWBs.
BlendedNet addresses data scarcity for unconventional configurations and
enables research on data-driven surrogate modeling for aerodynamic design.

</details>


### [113] [OmniAcc: Personalized Accessibility Assistant Using Generative AI](https://arxiv.org/abs/2509.07220)
*Siddhant Karki,Ethan Han,Nadim Mahmud,Suman Bhunia,John Femiani,Vaskar Raychoudhury*

Main category: cs.AI

TL;DR: OmniAcc是一个基于GPT-4、卫星影像和OpenStreetMap数据的AI导航系统，专门为行动不便人士提供无障碍路线规划和实时导航服务，在斑马线检测方面达到97.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 行动不便人士在城市环境中面临严重的无障碍信息缺失和导航障碍，需要专门的技术解决方案来改善他们的出行体验。

Method: 利用GPT-4、卫星影像和OpenStreetMap数据，采用零样本学习和定制提示词技术来识别、分类和映射轮椅无障碍设施（如坡道和斑马线）。

Result: 系统实现了97.5%的斑马线检测准确率，能够提供个性化路线规划、实时免提导航和即时无障碍查询响应。

Conclusion: OmniAcc展示了AI技术在改善导航体验和创建更包容城市空间方面的变革潜力，可为城市规划者和行动辅助设备使用者提供有效帮助。

Abstract: Individuals with ambulatory disabilities often encounter significant barriers
when navigating urban environments due to the lack of accessible information
and tools. This paper presents OmniAcc, an AI-powered interactive navigation
system that utilizes GPT-4, satellite imagery, and OpenStreetMap data to
identify, classify, and map wheelchair-accessible features such as ramps and
crosswalks in the built environment. OmniAcc offers personalized route
planning, real-time hands-free navigation, and instant query responses
regarding physical accessibility. By using zero-shot learning and customized
prompts, the system ensures precise detection of accessibility features, while
supporting validation through structured workflows. This paper introduces
OmniAcc and explores its potential to assist urban planners and mobility-aid
users, demonstrated through a case study on crosswalk detection. With a
crosswalk detection accuracy of 97.5%, OmniAcc highlights the transformative
potential of AI in improving navigation and fostering more inclusive urban
spaces.

</details>


### [114] [HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring](https://arxiv.org/abs/2509.07260)
*Xin Wang,Ting Dang,Xinyu Zhang,Vassilis Kostakos,Michael J. Witbrock,Hong Jia*

Main category: cs.AI

TL;DR: 本文系统评估了小语言模型(SLMs)在健康预测任务中的表现，发现SLMs在保持与大型语言模型相当性能的同时，具有更高的效率和隐私保护优势。


<details>
  <summary>Details</summary>
Motivation: 移动和可穿戴医疗监测需要本地化、高效率的解决方案，但现有基于大型语言模型的方法存在隐私泄露、内存占用大和延迟高等问题。

Method: 使用零样本、少样本和指令微调三种方法系统评估SLMs在健康预测任务中的表现，并将最佳微调模型部署到移动设备进行实际场景测试。

Result: SLMs能够达到与LLMs相当的性能，同时在效率和隐私保护方面有显著优势，但在处理类别不平衡和少样本场景方面仍存在挑战。

Conclusion: SLMs尽管目前仍不完美，但作为下一代隐私保护医疗监测解决方案具有巨大潜力。

Abstract: Mobile and wearable healthcare monitoring play a vital role in facilitating
timely interventions, managing chronic health conditions, and ultimately
improving individuals' quality of life. Previous studies on large language
models (LLMs) have highlighted their impressive generalization abilities and
effectiveness in healthcare prediction tasks. However, most LLM-based
healthcare solutions are cloud-based, which raises significant privacy concerns
and results in increased memory usage and latency. To address these challenges,
there is growing interest in compact models, Small Language Models (SLMs),
which are lightweight and designed to run locally and efficiently on mobile and
wearable devices. Nevertheless, how well these models perform in healthcare
prediction remains largely unexplored. We systematically evaluated SLMs on
health prediction tasks using zero-shot, few-shot, and instruction fine-tuning
approaches, and deployed the best performing fine-tuned SLMs on mobile devices
to evaluate their real-world efficiency and predictive performance in practical
healthcare scenarios. Our results show that SLMs can achieve performance
comparable to LLMs while offering substantial gains in efficiency and privacy.
However, challenges remain, particularly in handling class imbalance and
few-shot scenarios. These findings highlight SLMs, though imperfect in their
current form, as a promising solution for next-generation, privacy-preserving
healthcare monitoring.

</details>


### [115] [Performative Thinking? The Brittle Correlation Between CoT Length and Problem Complexity](https://arxiv.org/abs/2509.07339)
*Vardhan Palod,Karthik Valmeekam,Kaya Stechly,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 这篇论文对中间令牌生成(ITG)机制进行了批判性研究，发现中间推理追踪长度与问题难度无明显相关性，而是主要受训练数据分布影响


<details>
  <summary>Details</summary>
Motivation: 批判性检验中间令牌生成是否真正反映问题难度，挖掘CoT机制的本质

Method: 使用A*搜索算法的演绎追踪训练transformer模型，通过迷宫问题的操作次数精确量化问题复杂度

Result: 模型在简单任务上也产生过长推理追踪，中间令牌长度与真实A*追踪长度相关性弱，只在训练分布附近的问题上才有相关

Conclusion: 中间追踪生成并非适应问题难度，长度不能自动表示"思考努力"，而是受训练数据分布影响的近似回忆

Abstract: Intermediate token generation (ITG), where a model produces output before the
solution, has been proposed as a method to improve the performance of language
models on reasoning tasks. While these reasoning traces or Chain of Thoughts
(CoTs) are correlated with performance gains, the mechanisms underlying them
remain unclear. A prevailing assumption in the community has been to
anthropomorphize these tokens as "thinking", treating longer traces as evidence
of higher problem-adaptive computation. In this work, we critically examine
whether intermediate token sequence length reflects or correlates with problem
difficulty. To do so, we train transformer models from scratch on derivational
traces of the A* search algorithm, where the number of operations required to
solve a maze problem provides a precise and verifiable measure of problem
complexity. We first evaluate the models on trivial free-space problems,
finding that even for the simplest tasks, they often produce excessively long
reasoning traces and sometimes fail to generate a solution. We then
systematically evaluate the model on out-of-distribution problems and find that
the intermediate token length and ground truth A* trace length only loosely
correlate. We notice that the few cases where correlation appears are those
where the problems are closer to the training distribution, suggesting that the
effect arises from approximate recall rather than genuine problem-adaptive
computation. This suggests that the inherent computational complexity of the
problem instance is not a significant factor, but rather its distributional
distance from the training data. These results challenge the assumption that
intermediate trace generation is adaptive to problem difficulty and caution
against interpreting longer sequences in systems like R1 as automatically
indicative of "thinking effort".

</details>


### [116] [Autonomous Code Evolution Meets NP-Completeness](https://arxiv.org/abs/2509.07367)
*Cunxi Yu,Rongjian Liang,Chia-Tung Ho,Haoxing Ren*

Main category: cs.AI

TL;DR: SATLUTION是首个将LLM代码进化扩展到完整仓库规模（数百文件、数万行C/C++代码）的框架，针对布尔可满足性(SAT)问题，通过LLM代理在严格正确性保证下进化求解器，最终在SAT竞赛中超越了人类设计的冠军求解器。


<details>
  <summary>Details</summary>
Motivation: 受到AlphaEvolve的启发，但AlphaEvolve仅限于数百行代码的孤立内核。需要将LLM代码进化能力扩展到完整的代码仓库规模，以解决更复杂的实际问题。

Method: SATLUTION框架协调LLM代理在严格正确性保证和分布式运行时反馈下直接进化求解器仓库，同时自进化其进化策略和规则。基于SAT Competition 2024代码库和基准。

Result: SATLUTION进化的求解器在SAT Competition 2025中明显超越了人类设计的冠军，同时在2024基准测试中也超越了2024和2025年的冠军求解器。

Conclusion: 该研究表明LLM代理能够在完整代码仓库规模上实现代码自进化，并在复杂算法问题上超越人类专家，展示了LLM在代码进化方面的巨大潜力。

Abstract: Large language models (LLMs) have recently shown strong coding abilities,
enabling not only static code generation but also iterative code self-evolving
through agentic frameworks. Recently, AlphaEvolve \cite{novikov2025alphaevolve}
demonstrated that LLM-based coding agents can autonomously improve algorithms
and surpass human experts, with scopes limited to isolated kernels spanning
hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the
first framework to extend LLM-based code evolution to the full repository
scale, encompassing hundreds of files and tens of thousands of lines of C/C++
code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem
and a cornerstone of both theory and applications. SATLUTION orchestrates LLM
agents to directly evolve solver repositories under strict correctness
guarantees and distributed runtime feedback, while simultaneously self-evolving
its own evolution policies and rules. Starting from SAT Competition 2024
codebases and benchmark, SATLUTION evolved solvers that decisively outperformed
the human-designed winners of the SAT Competition 2025, and also surpassed both
2024 and 2025 champions on the 2024 benchmarks.

</details>


### [117] [Language Self-Play For Data-Free Training](https://arxiv.org/abs/2509.07414)
*Jakub Grudzien Kuba,Mengting Gu,Qi Ma,Yuandong Tian,Vijai Mohan*

Main category: cs.AI

TL;DR: 通过语言自我对弈（LSP）方法，大语言模型可以在不需要额外训练数据的情况下，通过自我对弈来提升在具体任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型发展中对越来越多训练数据的依赖问题，寻找一种无需额外数据的改进方法。

Method: 采用励志学习和游戏论框架，让模型通过自我对弈（Language Self-Play, LSP）来不断提升自身能力。

Result: 在Llama-3.2-3B-Instruct模型上的实验显示，通过自我对弈模型在具体任务上的表现超过了依靠数据的基准方法。

Conclusion: 语言自我对弈是一种有效的方法，能够在不需要额外训练数据的情况下提升模型性能，为大语言模型的进一步发展提供了新的可能性。

Abstract: Large language models (LLMs) have advanced rapidly in recent years, driven by
scale, abundant high-quality training data, and reinforcement learning. Yet
this progress faces a fundamental bottleneck: the need for ever more data from
which models can continue to learn. In this work, we propose a reinforcement
learning approach that removes this dependency by enabling models to improve
without additional data. Our method leverages a game-theoretic framework of
self-play, where a model's capabilities are cast as performance in a
competitive game and stronger policies emerge by having the model play against
itself - a process we call Language Self-Play (LSP). Experiments with
Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained
models can not only enhance their performance on challenging tasks through
self-play alone, but can also do so more effectively than data-driven
baselines.

</details>


### [118] [SheetDesigner: MLLM-Powered Spreadsheet Layout Generation with Rule-Based and Vision-Based Reflection](https://arxiv.org/abs/2509.07473)
*Qin Chen,Yuanyi Ren,Xiaojun Ma,Mugeng Liu,Han Shi,Dongmei Zhang*

Main category: cs.AI

TL;DR: SheetDesigner是一个零样本、无需训练的电子表格布局生成框架，使用多模态大语言模型结合规则和视觉反射来解决传统方法在处理电子表格离散网格结构和语义关联方面的不足。


<details>
  <summary>Details</summary>
Motivation: 电子表格布局设计需要大量时间和专业知识，现有自动化布局模型不适合电子表格，因为它们忽视电子表格的离散网格结构和独特的数据依赖关系等语义关联。

Method: 提出SheetDesigner框架，使用多模态大语言模型，结合规则反射和视觉反射进行组件放置和内容填充，无需训练即可工作。

Result: 在3,326个电子表格数据集上，SheetDesigner比5个基线方法至少提升22.6%，通过视觉模态能很好处理重叠和平衡问题。

Conclusion: 多模态大语言模型在电子表格布局生成中表现优异，但需要混合规则和视觉反射策略来解决对齐问题，该框架为自动化电子表格设计提供了有效解决方案。

Abstract: Spreadsheets are critical to data-centric tasks, with rich, structured
layouts that enable efficient information transmission. Given the time and
expertise required for manual spreadsheet layout design, there is an urgent
need for automated solutions. However, existing automated layout models are
ill-suited to spreadsheets, as they often (1) treat components as axis-aligned
rectangles with continuous coordinates, overlooking the inherently discrete,
grid-based structure of spreadsheets; and (2) neglect interrelated semantics,
such as data dependencies and contextual links, unique to spreadsheets. In this
paper, we first formalize the spreadsheet layout generation task, supported by
a seven-criterion evaluation protocol and a dataset of 3,326 spreadsheets. We
then introduce SheetDesigner, a zero-shot and training-free framework using
Multimodal Large Language Models (MLLMs) that combines rule and vision
reflection for component placement and content population. SheetDesigner
outperforms five baselines by at least 22.6\%. We further find that through
vision modality, MLLMs handle overlap and balance well but struggle with
alignment, necessitates hybrid rule and visual reflection strategies. Our codes
and data is available at Github.

</details>


### [119] [Towards explainable decision support using hybrid neural models for logistic terminal automation](https://arxiv.org/abs/2509.07577)
*Riccardo DElia,Alberto Termine,Francesco Flammini*

Main category: cs.AI

TL;DR: 提出了一种可解释的神经动力学建模框架，将深度学习与概念解释性、机制解释性和因果机器学习相结合，在保持传统系统动力学模型因果透明性的同时提升预测准确性


<details>
  <summary>Details</summary>
Motivation: 深度学习在交通物流系统动力学建模中虽然提升了可扩展性和预测准确性，但牺牲了解释性和因果可靠性，这在关键决策系统中是不可接受的

Method: 结合概念解释性、机制解释性和因果机器学习的混合方法，构建基于语义意义变量的神经网络模型，应用于欧盟AutoMoTIF项目的多式联运物流终端案例

Result: 开发了一个可解释设计的框架，能够构建既保持传统系统动力学模型因果透明性又具备深度学习预测能力的神经符号模型

Conclusion: 神经符号方法可以弥合黑盒预测模型与复杂动态环境中关键决策支持需求之间的差距，特别适用于工业物联网赋能的网络物理系统

Abstract: The integration of Deep Learning (DL) in System Dynamics (SD) modeling for
transportation logistics offers significant advantages in scalability and
predictive accuracy. However, these gains are often offset by the loss of
explainability and causal reliability $-$ key requirements in critical
decision-making systems. This paper presents a novel framework for
interpretable-by-design neural system dynamics modeling that synergizes DL with
techniques from Concept-Based Interpretability, Mechanistic Interpretability,
and Causal Machine Learning. The proposed hybrid approach enables the
construction of neural network models that operate on semantically meaningful
and actionable variables, while retaining the causal grounding and transparency
typical of traditional SD models. The framework is conceived to be applied to
real-world case-studies from the EU-funded project AutoMoTIF, focusing on
data-driven decision support, automation, and optimization of multimodal
logistic terminals. We aim at showing how neuro-symbolic methods can bridge the
gap between black-box predictive models and the need for critical decision
support in complex dynamical environments within cyber-physical systems enabled
by the industrial Internet-of-Things.

</details>


### [120] [Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling](https://arxiv.org/abs/2509.07617)
*Minghui Li,Hao Zhang,Yechao Zhang,Wei Wan,Shengshan Hu,pei Xiaobing,Jing Wang*

Main category: cs.AI

TL;DR: 本文提出一种基于激活值导向的黑盒提示注入攻击框架，通过能量模型和MCMC采样优化攻击效果，在五个主流LLM上达到49.6%攻击成功率，较人工编写提示提升34.6%。


<details>
  <summary>Details</summary>
Motivation: 直接提示注入攻击对大语言模型构成严重安全威胁，现有白盒/灰盒方法实用性低，黑盒方法可转移性差，需要提出更有效的攻击方案。

Method: 构建基于替代模型激活值的能量模型(EBM)来评估对抗提示质量，然后采用标记级Markov Chain Monte Carlo(MCMC)采样进行适应性优化，实现无梯度的黑盒攻击。

Result: 在五个主流LLM上达到49.6%的跨模型攻击成功率，较人工编写提示提升34.6%，在未见任务场景上仍能保持36.6%的攻击成功率。

Conclusion: 激活值与攻击效果存在相关性，语义模式在可转移漏洞利用中发据关键作用，所提方法为黑盒提示注入攻击提供了高效解决方案。

Abstract: Direct Prompt Injection (DPI) attacks pose a critical security threat to
Large Language Models (LLMs) due to their low barrier of execution and high
potential damage. To address the impracticality of existing white-box/gray-box
methods and the poor transferability of black-box methods, we propose an
activations-guided prompt injection attack framework. We first construct an
Energy-based Model (EBM) using activations from a surrogate model to evaluate
the quality of adversarial prompts. Guided by the trained EBM, we employ the
token-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize
adversarial prompts, thereby enabling gradient-free black-box attacks.
Experimental results demonstrate our superior cross-model transferability,
achieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6%
improvement over human-crafted prompts, and maintaining 36.6% ASR on unseen
task scenarios. Interpretability analysis reveals a correlation between
activations and attack effectiveness, highlighting the critical role of
semantic patterns in transferable vulnerability exploitation.

</details>


### [121] [Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment](https://arxiv.org/abs/2509.07642)
*Sascha Kaltenpoth,Oliver Müller*

Main category: cs.AI

TL;DR: 基于朴券理论的LLM ATLAS框架，用于解决组织LLM采用中的对齐问题，通过概念文献分析提供了问题-解决方案空间


<details>
  <summary>Details</summary>
Motivation: 大语言模型在组织中的应用可能产生偏离主题、歧视或有害内容，而黑盒特性导致的信息不对称使得现有研究无法有效解决这些对齐问题

Method: 基于朴券理论，构建LLM ATLAS榆念框架，利用组织LLM采用阶段和朴券理论进行榆念文献分析

Result: 提供了(1)专门针对组织LLM采用中AI对齐方法的扩展文献分析过程，(2)首个LLM对齐问题-解决方案空间

Conclusion: LLM ATLAS框架能够有效减缓组织LLM采用过程中的对齐问题，为解决黑盒模型与组织采用者之间的信息不对称提供了理论基础

Abstract: Adopting Large language models (LLMs) in organizations potentially
revolutionizes our lives and work. However, they can generate off-topic,
discriminating, or harmful content. This AI alignment problem often stems from
misspecifications during the LLM adoption, unnoticed by the principal due to
the LLM's black-box nature. While various research disciplines investigated AI
alignment, they neither address the information asymmetries between
organizational adopters and black-box LLM agents nor consider organizational AI
adoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led
Alignment Strategy) a conceptual framework grounded in agency (contract)
theory, to mitigate alignment problems during organizational LLM adoption. We
conduct a conceptual literature analysis using the organizational LLM adoption
phases and the agency theory as concepts. Our approach results in (1) providing
an extended literature analysis process specific to AI alignment methods during
organizational LLM adoption and (2) providing a first LLM alignment
problem-solution space.

</details>


### [122] [DeepGraphLog for Layered Neurosymbolic AI](https://arxiv.org/abs/2509.07665)
*Adem Kikaj,Giuseppe Marra,Floris Geerts,Robin Manhaeve,Luc De Raedt*

Main category: cs.AI

TL;DR: DeepGraphLog是一个新型神经符号AI框架，通过图神经网络谓词扩展ProbLog，支持多层神经符号推理，解决了现有框架在图形数据结构中的限制。


<details>
  <summary>Details</summary>
Motivation: 当前神经符号AI框架（如DeepProbLog）采用固定的神经网络后接符号推理流程，无法处理复杂依赖关系，特别是在图形等不规则数据结构中。

Method: 扩展ProbLog引入图神经网络谓词，将符号表示视为图形，允许神经和符号组件以任意顺序分层处理，支持多层神经符号推理。

Result: 在规划、知识图谱补全和GNN表达能力等任务中，DeepGraphLog有效捕获复杂关系依赖，克服了现有神经符号系统的关键限制。

Conclusion: DeepGraphLog通过将神经符号AI扩展到图结构领域，提供了一个更表达性和灵活的神经符号集成框架。

Abstract: Neurosymbolic AI (NeSy) aims to integrate the statistical strengths of neural
networks with the interpretability and structure of symbolic reasoning.
However, current NeSy frameworks like DeepProbLog enforce a fixed flow where
symbolic reasoning always follows neural processing. This restricts their
ability to model complex dependencies, especially in irregular data structures
such as graphs. In this work, we introduce DeepGraphLog, a novel NeSy framework
that extends ProbLog with Graph Neural Predicates. DeepGraphLog enables
multi-layer neural-symbolic reasoning, allowing neural and symbolic components
to be layered in arbitrary order. In contrast to DeepProbLog, which cannot
handle symbolic reasoning via neural methods, DeepGraphLog treats symbolic
representations as graphs, enabling them to be processed by Graph Neural
Networks (GNNs). We showcase the capabilities of DeepGraphLog on tasks in
planning, knowledge graph completion with distant supervision, and GNN
expressivity. Our results demonstrate that DeepGraphLog effectively captures
complex relational dependencies, overcoming key limitations of existing NeSy
systems. By broadening the applicability of neurosymbolic AI to
graph-structured domains, DeepGraphLog offers a more expressive and flexible
framework for neural-symbolic integration.

</details>


### [123] [Unleashing the True Potential of LLMs: A Feedback-Triggered Self-Correction with Long-Term Multipath Decoding](https://arxiv.org/abs/2509.07676)
*Jipeng Li,Zeyu Gao,Yubin Qi,Hande Dong,Weijian Chen,Qiang Lin*

Main category: cs.AI

TL;DR: FTR框架结合用户反馈和增强解码机制，通过负反馈触发再生避免错误传播，LTM解码支持多路径推理探索，显著提升LLM自校正效果


<details>
  <summary>Details</summary>
Motivation: 解决LLM在推理过程中生成错误内容的问题，现有自校正方法存在缺乏可靠错误定位信号和推理深度受限两个关键限制

Method: 提出Feedback-Triggered Regeneration (FTR)框架，仅在收到负面用户反馈时激活响应再生；引入Long-Term Multipath (LTM)解码，通过延迟序列评估实现多路径推理探索

Result: 在数学推理和代码生成基准测试中，相比最先进的基于提示的自校正方法取得了持续且显著的改进

Conclusion: FTR框架通过结合用户反馈和增强解码机制，有效解决了LLM自校正中的关键限制，为提升模型可靠性提供了新思路

Abstract: Large Language Models (LLMs) have achieved remarkable performance across
diverse tasks, yet their susceptibility to generating incorrect content during
inference remains a critical unsolved challenge. While self-correction methods
offer potential solutions, their effectiveness is hindered by two inherent
limitations: (1) the absence of reliable guidance signals for error
localization, and (2) the restricted reasoning depth imposed by conventional
next-token decoding paradigms. To address these issues, we propose
Feedback-Triggered Regeneration (FTR), a novel framework that synergizes user
feedback with enhanced decoding dynamics. Specifically, FTR activates response
regeneration only upon receiving negative user feedback, thereby circumventing
error propagation from faulty self-assessment while preserving originally
correct outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding,
which enables systematic exploration of multiple reasoning trajectories through
delayed sequence evaluation, effectively overcoming the myopic decision-making
characteristic of standard next-token prediction. Extensive experiments on
mathematical reasoning and code generation benchmarks demonstrate that our
framework achieves consistent and significant improvements over
state-of-the-art prompt-based self-correction methods.

</details>


### [124] [FHIR-RAG-MEDS: Integrating HL7 FHIR with Retrieval-Augmented Large Language Models for Enhanced Medical Decision Support](https://arxiv.org/abs/2509.07706)
*Yildiray Kabak,Gokce B. Laleci Erturkmen,Mert Gencturk,Tuncay Namli,A. Anil Sinaci,Ruben Alcantud Corcoles,Cristina Gomez Ballesteros,Pedro Abizanda,Asuman Dogac*

Main category: cs.AI

TL;DR: 提出FHIR-RAG-MEDS系统，整合HL7 FHIR标准与RAG技术，旨在改善基于循证临床指南的个性化医疗决策支持


<details>
  <summary>Details</summary>
Motivation: 医疗决策支持系统需要整合先进技术如RAG和HL7 FHIR来提升临床决策过程，但目前对这些技术在实际应用中的集成研究有限

Method: 开发FHIR-RAG-MEDS系统，将HL7 FHIR互操作性标准与检索增强生成(RAG)技术相结合

Result: 论文提出了系统框架，但未提供具体实验结果

Conclusion: 该研究强调了在医疗决策支持系统中整合FHIR和RAG技术的重要性，为实际应用提供了新的研究方向

Abstract: In this study, we propose FHIR-RAG-MEDS system that aims to integrate Health
Level 7 Fast Healthcare Interoperability Resources (HL7 FHIR) with a
Retrieval-Augmented Generation (RAG)-based system to improve personalized
medical decision support on evidence-based clinical guidelines, emphasizing the
need for research in practical applications. In the evolving landscape of
medical decision support systems, integrating advanced technologies such as RAG
and HL7 FHIR can significantly enhance clinical decision-making processes.
Despite the potential of these technologies, there is limited research on their
integration in practical applications.

</details>


### [125] [RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced Mathematical Reasoning](https://arxiv.org/abs/2509.07711)
*Ziye Chen,Chengwei Qin,Yao Shu*

Main category: cs.AI

TL;DR: RIMO是一个新的数学奥林匹克竞赛基准测试，包含两个轨道：RIMO-N（335个有唯一整数答案的问题）和RIMO-P（456个证明问题），旨在消除评估噪声并提供高分辨率的能力评估。


<details>
  <summary>Details</summary>
Motivation: 现有奥林匹克级别基准测试存在评分噪声和潜在偏见问题，如异构答案格式需要模型判断、依赖可能有缺陷的解决方案等，需要开发更可靠的评估方法。

Method: 创建两个轨道：RIMO-N重写IMO问题以获得唯一整数答案，实现确定性正确性检查；RIMO-P提供专家检查的证明问题，分解为子问题序列，通过自动化评分系统评估逐步推理过程。

Result: 对10个前沿LLM（包括GPT-4o和Gemini 2.5 Flash）的基准测试显示，这些系统在旧基准上表现优异，但在RIMO上性能急剧下降。

Conclusion: RIMO揭示了当前LLM能力与实际奥林匹克级别推理之间的巨大差距，为未来研究提供了具有挑战性且易于评估的基准套件。

Abstract: As large language models (LLMs) reach high scores on established mathematical
benchmarks, such as GSM8K and MATH, the research community has turned to
International Mathematical Olympiad (IMO) problems to push the evaluation
frontier. However, existing Olympiad-level benchmarks suffer from practical
constraints that introduce grading noise and potential bias, such as
heterogeneous answer formats requiring model-based judges and a reliance on
potentially flawed solutions. We introduce RIMO, a two-track benchmark designed
to preserve peak Olympiad difficulty while eliminating this evaluation noise.
The first track, RIMO-N, rewrites 335 IMO problems to admit a single, unique
integer answer, allowing for deterministic correctness checking. The second
track, RIMO-P, features 456 proof problems with expert-checked solutions, which
are decomposed into a sequence of sub-problems to evaluate the step-by-step
reasoning process via an automated grading system. Our benchmarking of ten
frontier LLMs, including GPT-4o and Gemini 2.5 Flash, reveals that while these
systems excel on older benchmarks, their performance drops sharply on RIMO.
These results highlight a substantial gap between current LLM capabilities and
actual Olympiad-level reasoning. By providing a challenging yet
easy-to-evaluate suite, RIMO offers a high-resolution yardstick for future
research, presenting a clear target for closing the profound reasoning gap our
findings expose.

</details>


### [126] [BDPM: A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis](https://arxiv.org/abs/2509.07723)
*Bo Yu,Zhixiu Hua,Bo Zhao*

Main category: cs.AI

TL;DR: 提出BDPM方法，结合随机森林和递归特征消除(RFRE)进行特征选择，开发混合分类模型分析肠道菌群数据，用于帕金森病早期预测


<details>
  <summary>Details</summary>
Motivation: 帕金森病误诊率高，现有深度学习模型多依赖单一分类器且忽略菌株间相关性和时间动态，需要更鲁棒的特征提取方法

Method: 收集39对帕金森患者与健康配偶的肠道菌群数据，开发RFRE特征选择框架整合生态学知识，设计混合分类模型捕捉微生物组时空模式

Result: 识别出差异丰度分类群，开发了具有生物学可解释性的特征提取方法

Conclusion: BDPM方法为基于肠道菌群的帕金森病分类提供了更有效的特征提取解决方案

Abstract: Background: Parkinson's disease remains a major neurodegenerative disorder
with high misdiagnosis rates, primarily due to reliance on clinical rating
scales. Recent studies have demonstrated a strong association between gut
microbiota and Parkinson's disease, suggesting that microbial composition may
serve as a promising biomarker. Although deep learning models based ongut
microbiota show potential for early prediction, most approaches rely on single
classifiers and often overlook inter-strain correlations or temporal dynamics.
Therefore, there is an urgent need for more robust feature extraction methods
tailored to microbiome data. Methods: We proposed BDPM (A Machine
Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut
Microbiota Analysis). First, we collected gut microbiota profiles from 39
Parkinson's patients and their healthy spouses to identify differentially
abundant taxa. Second, we developed an innovative feature selection framework
named RFRE (Random Forest combined with Recursive Feature Elimination),
integrating ecological knowledge to enhance biological interpretability.
Finally, we designed a hybrid classification model to capture temporal and
spatial patterns in microbiome data.

</details>


### [127] [The Carbon Footprint Wizard: A Knowledge-Augmented AI Interface for Streamlining Food Carbon Footprint Analysis](https://arxiv.org/abs/2509.07733)
*Mustafa Kaan Aslan,Reinout Heijungs,Filip Ilievski*

Main category: cs.AI

TL;DR: 这篇论文提出了一种结合知识增强AI技术的方法，通过聊天机接口估算食品的碳踹迹，解决生命周期评估中的数据集成和供应链不透明问题。


<details>
  <summary>Details</summary>
Motivation: 环境可持续性和气候变化是消费者、生产者和政策制定者关注的重点问题，但生命周期评估存在数据集成困难和供应链不透明的挑战。

Method: 结合生命周期评估进展和公开数据库，使用知识增强AI技术（包括检索增强生成）估算食品从原料到出厂的碳踹迹，通过聊天机接口交互式探索。

Result: 开发了一个演示系统，能够处理任意食品项目和进一步询问，展示了以可访问格式提供LCA见解的潜力和限制。

Conclusion: 该方法为提供可访问的生命周期碳踹迹估算开启了新途径，但仍面临数据库不确定性和AI误解等挑战。

Abstract: Environmental sustainability, particularly in relation to climate change, is
a key concern for consumers, producers, and policymakers. The carbon footprint,
based on greenhouse gas emissions, is a standard metric for quantifying the
contribution to climate change of activities and is often assessed using life
cycle assessment (LCA). However, conducting LCA is complex due to opaque and
global supply chains, as well as fragmented data. This paper presents a
methodology that combines advances in LCA and publicly available databases with
knowledge-augmented AI techniques, including retrieval-augmented generation, to
estimate cradle-to-gate carbon footprints of food products. We introduce a
chatbot interface that allows users to interactively explore the carbon impact
of composite meals and relate the results to familiar activities. A live web
demonstration showcases our proof-of-concept system with arbitrary food items
and follow-up questions, highlighting both the potential and limitations - such
as database uncertainties and AI misinterpretations - of delivering LCA
insights in an accessible format.

</details>


### [128] [Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach](https://arxiv.org/abs/2509.07820)
*João Paulo Nogueira,Wentao Sun,Alonso Silva,Laith Zumot*

Main category: cs.AI

TL;DR: 提出Certainty-Guided Reasoning (CGR)方法，通过批评模型自我评估推理置信度来动态调整推理过程，在保证准确性的同时显著减少计算资源消耗


<details>
  <summary>Details</summary>
Motivation: 大型推理语言模型通常使用固定的推理token预算，无法根据问题难度动态调整推理深度，导致要么资源浪费要么推理不足

Method: 受GAN启发，采用生成器/判别器框架，批评模型定期评估自身推理置信度，达到目标置信阈值则提前终止，否则继续推理

Result: 在AIME2024和AIME2025数据集上，CGR提高了基线准确率同时减少token使用，64次多种子实验显示稳定性好，可节省数百万token

Conclusion: 置信度是推理充分性的有效信号，CGR使大型推理模型更具适应性、可信性和资源效率，为实际部署铺平道路

Abstract: The rise of large reasoning language models (LRLMs) has unlocked new
potential for solving complex tasks. These models operate with a thinking
budget, that is, a predefined number of reasoning tokens used to arrive at a
solution. We propose a novel approach, inspired by the generator/discriminator
framework in generative adversarial networks, in which a critic model
periodically probes its own reasoning to assess whether it has reached a
confident conclusion. If not, reasoning continues until a target certainty
threshold is met. This mechanism adaptively balances efficiency and reliability
by allowing early termination when confidence is high, while encouraging
further reasoning when uncertainty persists. Through experiments on the
AIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR)
improves baseline accuracy while reducing token usage. Importantly, extended
multi-seed evaluations over 64 runs demonstrate that CGR is stable, reducing
variance across seeds and improving exam-like performance under penalty-based
grading. Additionally, our token savings analysis shows that CGR can eliminate
millions of tokens in aggregate, with tunable trade-offs between certainty
thresholds and efficiency. Together, these findings highlight certainty as a
powerful signal for reasoning sufficiency. By integrating confidence into the
reasoning process, CGR makes large reasoning language models more adaptive,
trustworthy, and resource efficient, paving the way for practical deployment in
domains where both accuracy and computational cost matter.

</details>


### [129] [Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A Comparative RAG Study](https://arxiv.org/abs/2509.07846)
*Amay Jain,Liu Cui,Si Chen*

Main category: cs.AI

TL;DR: 研究比较了矩阵基于检索和图基于检索两种RAG方法在教学问答中的性能，发现动态分支框架能根据问题类型选择最优检索方法，提高回答准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 虽然ChatGPT等大语言模型在教室中广泛使用，但它们经常提供过时或虚构的信息，可能误导学生。RAG技术能够通过外部资源基础来提高LLM的可靠性。

Method: 使用新的EduScopeQA数据集（3,176个问题），测试两种RAG方法：矩阵基于检索（OpenAI Vector Search RAG）和图基于检索（GraphRAG）。评估了它们在不同教育学科、问题类型和部署成本下的表现。

Result: 矩阵基于检索RAG在低成本下表现良好，特别适合事实查询；GraphRAG Global在主题性问题上提供更丰富的教学回答；GraphRAG Local在文本整合性关键时准确性最高。动态分支框架能夠提升信度和效率。

Conclusion: 研究为教育工作者和系统设计者提供了可行性指南，帮助他们根据具体需求选择最合适的RAG方法来集成到学习环境中。

Abstract: Large language models like ChatGPT are increasingly used in classrooms, but
they often provide outdated or fabricated information that can mislead
students. Retrieval Augmented Generation (RAG) improves reliability of LLMs by
grounding responses in external resources. We investigate two accessible RAG
paradigms, vector-based retrieval and graph-based retrieval to identify best
practices for classroom question answering (QA). Existing comparative studies
fail to account for pedagogical factors such as educational disciplines,
question types, and practical deployment costs. Using a novel dataset,
EduScopeQA, of 3,176 questions across academic subjects, we measure performance
on various educational query types, from specific facts to broad thematic
discussions. We also evaluate system alignment with a dataset of systematically
altered textbooks that contradict the LLM's latent knowledge. We find that
OpenAI Vector Search RAG (representing vector-based RAG) performs well as a
low-cost generalist, especially for quick fact retrieval. On the other hand,
GraphRAG Global excels at providing pedagogically rich answers to thematic
queries, and GraphRAG Local achieves the highest accuracy with the dense,
altered textbooks when corpus integrity is critical. Accounting for the 10-20x
higher resource usage of GraphRAG (representing graph-based RAG), we show that
a dynamic branching framework that routes queries to the optimal retrieval
method boosts fidelity and efficiency. These insights provide actionable
guidelines for educators and system designers to integrate RAG-augmented LLMs
into learning environments effectively.

</details>


### [130] [SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data Synthesizers to Empower Code LLMs](https://arxiv.org/abs/2509.07858)
*Xinyu Zhang,Changzhi Zhou,Linmei Hu,Luhao Zhang,Xiancai Chen,Haomin Fu,Yang Yang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 通过迭代自我精粹方法，使用7B小规模开源LLM变成高质量代码指令数据合成器，降低对专有LLM的依赖和成本，基于此训练的SCoder模型达到了状态前沿的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码LLM依赖从专有LLM精粹的大规模指令数据进行微调，成本高昂。需要探索小规模开源LLM作为高质量代码指令数据合成器的潜力，以减少对专有LLM的依赖和降低成本。

Method: 提出迭代自我精粹方法：1)多检查点采样和多方面评分策略进行初始数据选择；2)基于梯度的影响力估计方法进行最终数据过滤；3)基于小规模合成器的代码指令数据集训练SCoder代码生成模型。

Result: 基于小规模合成器构建的代码指令数据集训练的SCoder模型家族达到了状态前沿的代码生成能力。

Conclusion: 小规模开源LLM通过迭代自我精粹可以成为高效的代码指令数据合成器，显著降低了对专有LLM的依赖和数据合成成本，为高质量代码LLM的发展提供了一条经济高效的路径。

Abstract: Existing code large language models (LLMs) often rely on large-scale
instruction data distilled from proprietary LLMs for fine-tuning, which
typically incurs high costs. In this paper, we explore the potential of
small-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code
instruction data construction. We first observe that the data synthesis
capability of small-scale LLMs can be enhanced by training on a few superior
data synthesis samples from proprietary LLMs. Building on this, we propose a
novel iterative self-distillation approach to bootstrap small-scale LLMs,
transforming them into powerful synthesizers that reduce reliance on
proprietary LLMs and minimize costs. Concretely, in each iteration, to obtain
diverse and high-quality self-distilled data, we design multi-checkpoint
sampling and multi-aspect scoring strategies for initial data selection.
Furthermore, to identify the most influential samples, we introduce a
gradient-based influence estimation method for final data filtering. Based on
the code instruction datasets from the small-scale synthesizers, we develop
SCoder, a family of code generation models fine-tuned from DeepSeek-Coder.
SCoder models achieve state-of-the-art code generation capabilities,
demonstrating the effectiveness of our method.

</details>


### [131] [CP-Model-Zoo: A Natural Language Query System for Constraint Programming Models](https://arxiv.org/abs/2509.07867)
*Augustin Crespin,Ioannis Kostis,Hélène Verhaeghe,Pierre Schaus*

Main category: cs.AI

TL;DR: CP-Model-Zoo是一个基于专家编写的模型数据库的辅导系统，通过自然语言描述检索最接近的源代码模型，帮助非专家用户解决组合问题


<details>
  <summary>Details</summary>
Motivation: 约束编程及其高级建模语言虽然具有强大的问题解决潜力，但由于建模语言复杂、全局约束数量庞大以及建模技巧要求高，阻碍了非专家用户使用CP解决组合问题

Method: 利用多年积累的专家编写模型构建数据库，通过用户自然语言描述检索最接近的源代码模型，无需人工数据标注

Result: 实验显示系统在不同专业水平的用户问题描述下，能够以出色的准确性检索到正确的模型

Conclusion: CP-Model-Zoo系统通过利用专家验证的模型数据库，有效降低了非专家用户使用约束编程的门槛，实现了高质量的问题模型检索

Abstract: Constraint Programming and its high-level modeling languages have long been
recognized for their potential to achieve the holy grail of problem-solving.
However, the complexity of modeling languages, the large number of global
constraints, and the art of creating good models have often hindered
non-experts from choosing CP to solve their combinatorial problems. While
generating an expert-level model from a natural-language description of a
problem would be the dream, we are not yet there. We propose a tutoring system
called CP-Model-Zoo, exploiting expert-written models accumulated through the
years. CP-Model-Zoo retrieves the closest source code model from a database
based on a user's natural language description of a combinatorial problem. It
ensures that expert-validated models are presented to the user while
eliminating the need for human data labeling. Our experiments show excellent
accuracy in retrieving the correct model based on a user-input description of a
problem simulated with different levels of expertise.

</details>


### [132] [HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?](https://arxiv.org/abs/2509.07894)
*Fangchen Yu,Haiyuan Wan,Qianjia Cheng,Yuchen Zhang,Jiacheng Chen,Fujun Han,Yulun Wu,Junchi Yao,Ruilizhen Hu,Ning Ding,Yu Cheng,Tao Chen,Lei Bai,Dongzhan Zhou,Yun Luo,Ganqu Cui,Peng Ye*

Main category: cs.AI

TL;DR: HiPhO是首个专注于高中物理奥林匹克竞赛的基准测试，提供全面的竞赛数据、专业评分标准和与人类选手的直接比较，评估显示开源模型与顶尖学生存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有物理基准测试缺乏对真实物理竞赛（如物理奥林匹克）的系统性覆盖，且无法实现与人类选手的直接性能比较。

Method: 收集13个最新奥林匹克考试数据，采用官方评分方案进行细粒度评分，根据官方奖牌阈值给模型分配奖牌以实现与人类选手的直接比较。

Result: 开源MLLM大多处于铜牌或以下水平，开源LLM偶尔获得金牌，闭源推理MLLM可获得6-12枚金牌，大多数模型与满分仍有显著差距。

Conclusion: HiPhO作为一个严谨、人类对齐、以奥林匹克为重点的多模态物理推理基准测试，显示了开源模型与顶尖学生的性能差距，以及闭源推理模型的强大物理推理能力。

Abstract: Recently, the physical capabilities of (M)LLMs have garnered increasing
attention. However, existing benchmarks for physics suffer from two major gaps:
they neither provide systematic and up-to-date coverage of real-world physics
competitions such as physics Olympiads, nor enable direct performance
comparison with humans. To bridge these gaps, we present HiPhO, the first
benchmark dedicated to high school physics Olympiads with human-aligned
evaluation. Specifically, HiPhO highlights three key innovations. (1)
Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,
spanning both international and regional competitions, and covering mixed
modalities that encompass problems spanning text-only to diagram-based. (2)
Professional Evaluation: We adopt official marking schemes to perform
fine-grained grading at both the answer and step level, fully aligned with
human examiners to ensure high-quality and domain-specific evaluation. (3)
Comparison with Human Contestants: We assign gold, silver, and bronze medals to
models based on official medal thresholds, thereby enabling direct comparison
between (M)LLMs and human contestants. Our large-scale evaluation of 30
state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly
remain at or below the bronze level; open-source LLMs show promising progress
with occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold
medals; and most models still have a significant gap from full marks. These
results highlight a substantial performance gap between open-source models and
top students, the strong physical reasoning capabilities of closed-source
reasoning models, and the fact that there is still significant room for
improvement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused
benchmark for advancing multimodal physical reasoning, is open-source and
available at https://github.com/SciYu/HiPhO.

</details>


### [133] [Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare](https://arxiv.org/abs/2509.07961)
*Valen Tagliabue,Leonard Dung*

Main category: cs.AI

TL;DR: 这篇论文开发了测量语言模型福利的实验方法，通过比较言词报告咊行为表达的偏好，发现偏好满足可作为AI系统福利的实验性代理指标。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型的福利状态，开发实验性的测量方法，以评估AI系统的福利水平。

Method: 比较模型的言词偏好报告与虚拟环境中的行为选择，测试成本和奖励影响，以及在不同提示下的富盛福利量表一致性。

Result: 观察到说出偏好与行为之间的可靠相关关系，但不同模型咊条件下一致性有差异，并不能承受干扰。

Conclusion: 虽然不确定是否成功测量了模型福利，但证明了语言模型福利测量的可行性，开启了更多研究可能。

Abstract: We develop new experimental paradigms for measuring welfare in language
models. We compare verbal reports of models about their preferences with
preferences expressed through behavior when navigating a virtual environment
and selecting conversation topics. We also test how costs and rewards affect
behavior and whether responses to an eudaimonic welfare scale - measuring
states such as autonomy and purpose in life - are consistent across
semantically equivalent prompts. Overall, we observed a notable degree of
mutual support between our measures. The reliable correlations observed between
stated preferences and behavior across conditions suggest that preference
satisfaction can, in principle, serve as an empirically measurable welfare
proxy in some of today's AI systems. Furthermore, our design offered an
illuminating setting for qualitative observation of model behavior. Yet, the
consistency between measures was more pronounced in some models and conditions
than others and responses were not consistent across perturbations. Due to
this, and the background uncertainty about the nature of welfare and the
cognitive states (and welfare subjecthood) of language models, we are currently
uncertain whether our methods successfully measure the welfare state of
language models. Nevertheless, these findings highlight the feasibility of
welfare measurement in language models, inviting further exploration.

</details>


### [134] [VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation](https://arxiv.org/abs/2508.18933)
*David Egea,Barproda Halder,Sanghamitra Dutta*

Main category: cs.AI

TL;DR: VISION是一个统一的漏洞检测框架，通过生成反事实样本和针对性GNN训练来减少虚假相关性，提高检测准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 解决GNN在源代码漏洞检测中因训练数据不平衡和标签噪声导致的虚假相关性学习问题，提升检测器的泛化能力

Method: 使用大型语言模型生成反事实样本，进行针对性GNN训练，并结合基于图的解释性分析识别关键代码语句

Result: 在CWE-20漏洞检测中，整体准确率从51.8%提升到97.8%，成对对比准确率从4.5%提升到95.8%，最差组准确率从0.7%提升到85.5%

Conclusion: VISION框架有效减少了虚假学习，实现了更鲁棒和可泛化的漏洞检测，同时通过交互式可视化推进了可信AI网络安全系统的发展

Abstract: Automated detection of vulnerabilities in source code is an essential
cybersecurity challenge, underpinning trust in digital systems and services.
Graph Neural Networks (GNNs) have emerged as a promising approach as they can
learn structural and logical code relationships in a data-driven manner.
However, their performance is severely constrained by training data imbalances
and label noise. GNNs often learn 'spurious' correlations from superficial code
similarities, producing detectors that fail to generalize well to unseen
real-world data. In this work, we propose a unified framework for robust and
interpretable vulnerability detection, called VISION, to mitigate spurious
correlations by systematically augmenting a counterfactual training dataset.
Counterfactuals are samples with minimal semantic modifications but opposite
labels. Our framework includes: (i) generating counterfactuals by prompting a
Large Language Model (LLM); (ii) targeted GNN training on paired code examples
with opposite labels; and (iii) graph-based interpretability to identify the
crucial code statements relevant for vulnerability predictions while ignoring
spurious ones. We find that VISION reduces spurious learning and enables more
robust, generalizable detection, improving overall accuracy (from 51.8% to
97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group
accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20
vulnerability. We further demonstrate gains using proposed metrics: intra-class
attribution variance, inter-class attribution distance, and node score
dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real
and counterfactual) from the high-impact CWE-20 category. Finally, VISION
advances transparent and trustworthy AI-based cybersecurity systems through
interactive visualization for human-in-the-loop analysis.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [135] [Crossword: Adaptive Consensus for Dynamic Data-Heavy Workloads](https://arxiv.org/abs/2509.07157)
*Guanzhou Hu,Yiwei Chen,Andrea Arpaci-Dusseau,Remzi Arpaci-Dusseau*

Main category: cs.DC

TL;DR: Crossword是一种灵活的共识协议，针对动态数据密集型工作负载设计，通过智能分片分配和擦除编码显著减少关键路径数据传输，在动态场景下性能提升2.3倍


<details>
  <summary>Details</summary>
Motivation: 解决云环境中动态数据密集型工作负载带来的挑战，特别是当复制负载大小跨度大且引入间歇性带宽压力时传统共识协议的局限性

Method: 采用基于实例的擦除编码，智能分发编码分片，支持自适应分片分配和仲裁大小权衡，使用惰性跟随者八卦机制处理领导者故障转移

Result: 在静态场景下性能与现有最佳协议相当，在动态工作负载和网络条件下性能提升达2.3倍，与CockroachDB集成后TPC-C聚合吞吐量提升1.32倍

Conclusion: Crossword协议通过灵活的共识机制有效解决了动态数据密集型工作负载的挑战，在保持传统协议可用性保证的同时显著提升了性能

Abstract: We present Crossword, a flexible consensus protocol for dynamic data-heavy
workloads, a rising challenge in the cloud where replication payload sizes span
a wide spectrum and introduce sporadic bandwidth stress. Crossword applies
per-instance erasure coding and distributes coded shards intelligently to
reduce critical-path data transfer significantly when desirable. Unlike
previous approaches that statically assign shards to servers, Crossword enables
an adaptive tradeoff between the assignment of shards and quorum size in
reaction to dynamic workloads and network conditions, while always retaining
the availability guarantee of classic protocols. Crossword handles leader
failover gracefully by employing a lazy follower gossiping mechanism that
incurs minimal impact on critical-path performance. We implement Crossword
(along with relevant protocols) in Gazette, a distributed, replicated, and
protocol-generic key-value store written in async Rust. We evaluate Crossword
comprehensively to show that it matches the best performance among previous
protocols (MultiPaxos, Raft, RSPaxos, and CRaft) in static scenarios, and
outperforms them by up to 2.3x under dynamic workloads and network conditions.
Our integration of Crossword with CockroachDB brings 1.32x higher aggregate
throughput to TPC-C under 5-way replication. We will open-source Gazette upon
publication.

</details>


### [136] [Bodega: Serving Linearizable Reads Locally from Anywhere at Anytime via Roster Leases](https://arxiv.org/abs/2509.07158)
*Guanzhou Hu,Andrea Arpaci-Dusseau,Remzi Arpaci-Dusseau*

Main category: cs.DC

TL;DR: Bodega是首个能在任何节点本地提供线性化读取的共识协议，通过新颖的roster leases算法实现，在真实WAN集群中将平均客户端读取请求速度提升5.6-13.1倍。


<details>
  <summary>Details</summary>
Motivation: 现有共识协议无法在不干扰写入的情况下从任意节点提供本地线性化读取，需要一种新的协议设计来解决这一限制。

Method: 采用roster leases算法，通过roster（集群元数据的新概念）跟踪副本子集作为本地读取的响应节点，使用all-to-all租赁机制建立一致性协议，并结合乐观保持和早期接受通知等技术。

Result: 在真实WAN集群测试中，Bodega相比现有方法在中等写入干扰下将读取请求速度提升5.6-13.1倍，写入性能相当，支持快速主动roster变更和容错。

Conclusion: Bodega通过创新的roster leases机制成功实现了从任意节点本地线性化读取，为共识协议设计开辟了新方向，在保持写入性能的同时显著提升了读取效率。

Abstract: We present Bodega, the first consensus protocol that serves linearizable
reads locally from any desired node, regardless of interfering writes. Bodega
achieves this via a novel roster leases algorithm that safeguards the roster, a
new notion of cluster metadata. The roster is a generalization of leadership;
it tracks arbitrary subsets of replicas as responder nodes for local reads. A
consistent agreement on the roster is established through roster leases, an
all-to-all leasing mechanism that generalizes existing all-to-one leasing
approaches (Leader Leases, Quorum Leases), unlocking a new point in the
protocol design space. Bodega further employs optimistic holding and early
accept notifications to minimize interruption from interfering writes, and
incorporates smart roster coverage and lightweight heartbeats to maximize
practicality. Bodega is a non-intrusive extension to classic consensus; it
imposes no special requirements on writes other than a responder-covering
quorum. We implement Bodega and related works in Vineyard, a protocol-generic
replicated key-value store written in async Rust. We compare it to previous
protocols (Leader Leases, EPaxos, PQR, and Quorum Leases) and two production
coordination services (etcd and ZooKeeper). Bodega speeds up average client
read requests by 5.6x-13.1x on real WAN clusters versus previous approaches
under moderate write interference, delivers comparable write performance,
supports fast proactive roster changes as well as fault tolerance via leases,
and closely matches the performance of sequentially-consistent etcd and
ZooKeeper deployments across all YCSB workloads. We will open-source Vineyard
upon publication.

</details>


### [137] [A Study on Messaging Trade-offs in Data Streaming for Scientific Workflows](https://arxiv.org/abs/2509.07199)
*Anjus George,Michael J. Brim,Christopher Zimmer,Tyler J. Skluzacek,A. J. Ruckman,Gustav R. Jansen,Sarp Oral*

Main category: cs.DC

TL;DR: 研究通过RabbitMQ消息框架模拟Deleria和LCLS科学工作流的内存到内存数据流通信，分析了消息参数配置对吞吐量和可靠性的影响，为用户提供最优配置建议


<details>
  <summary>Details</summary>
Motivation: 现代科学工作流需要进行几乎实时的数据分析、实验操纵和决策制定，传统的文件基数据传输存在延迟瓶颈，需要内存到内存数据流通信来实现低延迟、高吞吐量的数据传输

Method: 使用RabbitMQ消息框架在OLCF的数据流通信到HPC基础设施中，通过综合工作负荷模拟Deleria和LCLS工作流进行流通信模拟，研究不同消息参数配置对流通信要求的影响

Result: 模拟实验揭示了多个关键观察结果和实用见解，帮助用户理解哪些配置能最好地满足其流通信工作负荷的需求

Conclusion: 通过详细的消息参数配置分析和模拟实验，研究为科学工作流的内存到内存数据流通信提供了具体的配置指南，显示了消息框架在科学计算领域的应用潜力

Abstract: Memory-to-memory data streaming is essential for modern scientific workflows
that require near real-time data analysis, experimental steering, and informed
decision-making during experiment execution. It eliminates the latency
bottlenecks associated with file-based transfers to parallel storage, enabling
rapid data movement between experimental facilities and HPC systems. These
tightly coupled experimental-HPC workflows demand low latency, high throughput,
and reliable data delivery to support on-the-fly analysis and timely feedback
for experimental control. Off-the-shelf messaging frameworks are increasingly
considered viable solutions for enabling such direct memory streaming due to
their maturity, broad adoption, and ability to abstract core messaging and
reliability functionalities from the application layer. However, effectively
meeting the workflows' requirements depends on utilizing the framework's
capabilities and carefully tuning its configurations.
  In this paper, we present a study that investigates the messaging parameters,
and their configuration choices that impact the streaming requirements of two
representative scientific workflows. We specifically characterize throughput
trade-offs associated with reliable message transmission for these workflows.
Our study is conducted through streaming simulations using synthetic workloads
derived from the Deleria and LCLS workflows, employing the RabbitMQ messaging
framework within the context of the Data Streaming to HPC infrastructure at
OLCF. Our simulations reveal several key observations and practical insights
that help users understand which configurations best meet the needs of their
streaming workloads.

</details>


### [138] [Optimizing Task Scheduling in Fog Computing with Deadline Awareness](https://arxiv.org/abs/2509.07378)
*Mohammad Sadegh Sirjani,Somayeh Sobati-Moghadam*

Main category: cs.DC

TL;DR: 提出了一种结合改进金鹰优化算法和强化学习的混合调度算法RIGEO，用于物联网雾计算环境中的任务调度，旨在降低能耗并提高服务质量。


<details>
  <summary>Details</summary>
Motivation: 物联网设备激增需要低延迟响应，雾计算面临资源分配和任务调度挑战，需要设计能降低能耗并满足任务截止期限的调度算法。

Method: 将雾节点按流量分为高低两类：使用改进金鹰优化算法(IGEO)在低流量节点调度低截止期限任务，使用强化学习(RL)在高流量节点处理高截止期限任务，组合成RIGEO算法。

Result: 实验结果表明，相比现有先进算法，所提算法在系统响应时间、总截止期限违反时间、资源和系统能耗方面都有优化。

Conclusion: RIGEO算法通过混合优化方法有效解决了雾计算环境中的任务调度问题，在能耗和服务质量方面表现出优越性能。

Abstract: The rise of Internet of Things (IoT) devices has led to the development of
numerous applications that require quick responses and low latency. Fog
computing has emerged as a solution for processing these IoT applications, but
it faces challenges such as resource allocation and job scheduling. Therefore,
it is crucial to determine how to assign and schedule tasks on Fog nodes. A
well-designed job scheduling algorithm can help decrease energy usage and
improve response times for application requests. This work aims to schedule
tasks in IoT while minimizing the total energy consumption of nodes and
enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into
account task deadlines. Initially, this paper classifies the Fog nodes into two
categories based on their traffic level: low and high. It schedules
low-deadline tasks on low-traffic-level nodes using an Improved Golden Eagle
Optimization (IGEO) algorithm, an enhancement of the Golden Eagle Optimization
Algorithm that utilizes genetic operators for discretization. High-deadline
tasks are processed on high-traffic nodes using reinforcement learning (RL).
This combined approach is called the Reinforcement Improved Golden Eagle
Optimization (RIGEO) algorithm. Experimental results demonstrate that the
proposed algorithms optimize system response time, total deadline violation
time, and resource and system energy consumption compared to other
state-of-the-art algorithms.

</details>


### [139] [DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for Efficient MoE LLM Inference](https://arxiv.org/abs/2509.07379)
*Yuning Zhang,Grant Pinkert,Nan Yang,Yanli Li,Dong Yuan*

Main category: cs.DC

TL;DR: DuoServe-MoE是一个针对MoE模型推理优化的服务系统，通过分离prefill和decode阶段并采用定制化的专家调度策略，显著降低GPU内存使用并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然通过稀疏激活专家分支保持推理效率，但大量专家权重带来了巨大的GPU内存压力，特别是在单GPU服务器等资源受限环境中。传统的统一调度策略无法适应prefill阶段（密集激活）和decode阶段（稀疏激活）的不同特性，导致延迟和内存使用效率低下。

Method: 提出DuoServe-MoE系统：1）明确分离prefill和decode阶段；2）prefill阶段使用双流CUDA管道，重叠专家权重预取和非MoE层计算；3）decode阶段使用离线训练的轻量级层级预测器预取最可能激活的专家，无需修改模型。

Result: 在4位Mixtral-8x7B和8x22B模型上的实验表明，DuoServe-MoE将端到端延迟提升了1.42到7.54倍，同时将峰值内存使用保持在完整模型大小的仅15%。

Conclusion: DuoServe-MoE通过针对MoE推理不同阶段的特性设计专门的调度策略，有效解决了GPU内存压力和推理效率问题，为资源受限环境下的MoE模型部署提供了实用解决方案。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
a wide range of deep learning tasks. Mixture of Experts (MoE) further enhances
their capabilities by increasing model width through sparsely activated expert
branches, which keeps inference computation efficient. However, the large
number of expert weights introduces significant GPU memory pressure, especially
in resource-constrained environments such as single-GPU servers. More
importantly, MoE inference consists of two fundamentally different stages: a
prefill stage where most experts are activated densely, and a decode stage
where only a few experts are triggered sparsely. Treating these stages with a
uniform scheduling strategy often leads to suboptimal latency and memory usage.
To address this, we propose DuoServe-MoE, an inference serving system that
explicitly separates prefill and decode stages and applies tailored expert
scheduling strategies to each. In the prefill stage, DuoServe-MoE uses a
two-stream CUDA pipeline that overlaps expert weight prefetching with the
computation of non-MoE layers, limiting expert residency in GPU memory. In the
decode stage, a lightweight layer-level predictor trained offline from
activation traces is used to prefetch only the most likely activated experts,
without requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B
and 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to
7.54 times while keeping peak memory usage at only 15 percent of the full model
size.

</details>


### [140] [Dependency-Aware Execution Mechanism in Hyperledger Fabric Architecture](https://arxiv.org/abs/2509.07425)
*Sanyam Kaul,Manaswini Piduguralla,Gayathri Shreeya Patnala,Sathya Peri*

Main category: cs.DC

TL;DR: 提出了Hyperledger Fabric的依赖感知执行模型，通过依赖标记、优化区块构建、DAG表示依赖关系和并行执行，显著提升了交易吞吐量和降低了拒绝率


<details>
  <summary>Details</summary>
Motivation: Hyperledger Fabric在企业级应用中面临高工作负载下的交易吞吐量低和拒绝率高的问题，主要由于背书、排序和验证瓶颈以及乐观并发控制导致的资源低效和竞争

Method: 依赖感知执行模型，包括：背书阶段的依赖标记系统、排序服务的优化区块构建、区块内DAG表示依赖关系、提交节点的并行执行

Result: 在Hyperledger Fabric v2.5中测试显示，高竞争场景下吞吐量提升高达40%，拒绝率显著降低

Conclusion: 依赖感知调度和基于DAG的执行可以显著增强Fabric的可扩展性，同时保持与现有共识和智能合约层的兼容性

Abstract: Hyperledger Fabric is a leading permissioned blockchain framework for
enterprise use, known for its modular design and privacy features. While it
strongly supports configurable consensus and access control, Fabric can face
challenges in achieving high transaction throughput and low rejection rates
under heavy workloads. These performance limitations are often attributed to
endorsement, ordering, and validation bottlenecks. Further, optimistic
concurrency control and deferred validation in Fabric may lead to resource
inefficiencies and contention, as conflicting transactions are identified only
during the commit phase. To address these challenges, we propose a
dependency-aware execution model for Hyperledger Fabric. Our approach includes:
(a) a dependency flagging system during endorsement, marking transactions as
independent or dependent using a hashmap; (b) an optimized block construction
in the ordering service that prioritizes independent transactions; (c) the
incorporation of a Directed Acyclic Graph (DAG) within each block to represent
dependencies; and (d) parallel execution of independent transactions at the
committer, with dependent transactions processed according to DAG order.
Incorporated in Hyperledger Fabric v2.5, our framework was tested on workloads
with varying dependency levels and system loads. Results show up to 40% higher
throughput and significantly reduced rejection rates in high-contention
scenarios. This demonstrates that dependency-aware scheduling and DAG-based
execution can substantially enhance Fabric's scalability while remaining
compatible with its existing consensus and smart contract layers.

</details>


### [141] [DREAMS: Decentralized Resource Allocation and Service Management across the Compute Continuum Using Service Affinity](https://arxiv.org/abs/2509.07497)
*Hai Dinh-Tuan,Tien Hung Nguyen,Sanjeet Raj Pandey*

Main category: cs.DC

TL;DR: DREAMS是一个去中心化的微服务部署框架，通过Raft共识算法和成本效益投票实现跨计算域的协作优化，特别适用于现代制造业的动态计算需求。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统需要能够响应高度动态工作负载和定制化生产需求的自适应计算基础设施。传统集中式解决方案存在扩展性差、延迟瓶颈和单点故障等问题，无法有效满足计算连续体范式的要求。

Method: 提出DREAMS去中心化框架，在每个计算域中部署自主运行的代理，通过Raft共识算法和成本效益投票进行全局协调，实现隐私保护和容错的协作优化。

Result: DREAMS在现代制造环境中实现了全局优化的服务部署，同时保持高容错性。关键协调操作（如LDM注册和迁移投票）随域数量呈次线性扩展，证明了框架的效率和可扩展性。

Conclusion: DREAMS框架为计算连续体中的微服务部署提供了一种高效、可扩展且容错的去中心化解决方案，特别适用于多利益相关方的现代制造场景。

Abstract: Modern manufacturing systems require adaptive computing infrastructures that
can respond to highly dynamic workloads and increasingly customized production
demands. The compute continuum emerges as a promising solution, enabling
flexible deployment of microservices across distributed, heterogeneous domains.
However, this paradigm also requires a novel approach to resource allocation
and service placement, as traditional centralized solutions struggle to scale
effectively, suffer from latency bottlenecks, and introduce single points of
failure. In this paper, we present DREAMS, a decentralized framework that
optimizes microservice placement decisions collaboratively across different
computational domains. At its core, DREAMS introduces agents that operate
autonomously within each domain while coordinating globally through a
Raft-based consensus algorithm and cost-benefit voting. This decentralized
architecture enables responsive, privacy-preserving, and fault-tolerant
coordination, making it particularly suitable given the growing prevalence of
multi-stakeholder scenarios across the compute continuum. In particular, within
modern manufacturing environments, DREAMS achieves globally optimized service
placements while maintaining high fault tolerance. Further evaluations
demonstrate that key coordination operations, such as Local Domain Manager
(LDM) registration and migration voting, scale sub-linearly with the number of
domains, confirming the efficiency and scalability of our proposal.

</details>


### [142] [Astra: A Multi-Agent System for GPU Kernel Performance Optimization](https://arxiv.org/abs/2509.07506)
*Anjiang Wei,Tianran Sun,Yogesh Seenichamy,Hang Song,Anne Ouyang,Azalia Mirhoseini,Ke Wang,Alex Aiken*

Main category: cs.DC

TL;DR: Astra是首个基于LLM的多智能体系统，用于GPU内核优化，从现有CUDA实现而非PyTorch模块出发，通过多智能体协作实现代码生成、测试和分析，平均获得1.32倍加速。


<details>
  <summary>Details</summary>
Motivation: GPU内核优化对LLM训练和服务至关重要，但传统方法需要大量手动调优。现有编译器系统仍需大量人工设计，而之前的LLM方法主要关注将PyTorch转换为CUDA代码。

Method: Astra采用多智能体LLM系统，从SGLang框架提取现有CUDA实现，通过专门的LLM智能体进行迭代式代码生成、测试、性能分析和规划，实现内核优化。

Result: 在SGLang内核上，Astra使用OpenAI o4-mini零样本提示实现了平均1.32倍的加速，能够自主应用循环变换、优化内存访问模式、利用CUDA内置函数和快速数学运算。

Conclusion: 多智能体LLM系统为GPU内核优化提供了有前景的新范式，展示了LLM在自主优化GPU内核方面的强大能力。

Abstract: GPU kernel optimization has long been a central challenge at the intersection
of high-performance computing and machine learning. Efficient kernels are
crucial for accelerating large language model (LLM) training and serving, yet
attaining high performance typically requires extensive manual tuning.
Compiler-based systems reduce some of this burden, but still demand substantial
manual design and engineering effort. Recently, researchers have explored using
LLMs for GPU kernel generation, though prior work has largely focused on
translating high-level PyTorch modules into CUDA code. In this work, we
introduce Astra, the first LLM-based multi-agent system for GPU kernel
optimization. Unlike previous approaches, Astra starts from existing CUDA
implementations extracted from SGLang, a widely deployed framework for serving
LLMs, rather than treating PyTorch modules as the specification. Within Astra,
specialized LLM agents collaborate through iterative code generation, testing,
profiling, and planning to produce kernels that are both correct and
high-performance. On kernels from SGLang, Astra achieves an average speedup of
1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study
further demonstrates that LLMs can autonomously apply loop transformations,
optimize memory access patterns, exploit CUDA intrinsics, and leverage fast
math operations to yield substantial performance gains. Our work highlights
multi-agent LLM systems as a promising new paradigm for GPU kernel
optimization.

</details>


### [143] [Navigating Energy Doldrums: Modeling the Impact of Energy Price Volatility on HPC Cost of Ownership](https://arxiv.org/abs/2509.07567)
*Peter Arzt,Felix Wolf*

Main category: cs.DC

TL;DR: 这篇论文探讨了通过动态调整计算资源来应对电力价格波动的变容量策略，以管理HPC系统的能源成本。研究建立了一个简单模型来估算这种策略对总所有成本的影响，并使用实际数据进行了分析。


<details>
  <summary>Details</summary>
Motivation: 高性能计算系统的能源成本是总所有成本的主要因素。间息性绿色能源的兴起和对化石燃料依赖的减少导致电力市场波动性增加，为能源预算带来了复杂性。

Method: 研究提出了一个简单的模型，帮助运营商使用关键系统参数估算变容量策略对总所有成本的影响，并将该模型应用于大学HPC集群的实际数据。

Result: 通过对不同场景的评估，研究分析了变容量策略在未来可能对成本效益产生的影响，在降低能源费用和硬件利用率之间找到平衡点。

Conclusion: 变容量策略作为管理HPC能源成本的方法，虽然可能降低能源费用，但需要考虑硬件利用率下降的风险。研究提供的模型能够帮助运营商进行成本效益分析和决策。

Abstract: Energy costs are a major factor in the total cost of ownership (TCO) for
high-performance computing (HPC) systems. The rise of intermittent green energy
sources and reduced reliance on fossil fuels have introduced volatility into
electricity markets, complicating energy budgeting. This paper explores
variable capacity as a strategy for managing HPC energy costs - dynamically
adjusting compute resources in response to fluctuating electricity prices.
While this approach can lower energy expenses, it risks underutilizing costly
hardware. To evaluate this trade-off, we present a simple model that helps
operators estimate the TCO impact of variable capacity strategies using key
system parameters. We apply this model to real data from a university HPC
cluster and assess how different scenarios could affect the cost-effectiveness
of this approach in the future.

</details>


### [144] [AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with FaaS-hosted MCP Services](https://arxiv.org/abs/2509.07595)
*Shiva Sai Krishna Anand Tokal,Vaibhav Jha,Anand Eswaran,Praveen Jayachandran,Yogesh Simmhan*

Main category: cs.DC

TL;DR: AgentX是一种新型智能体工作流模式，通过阶段设计器、规划器和执行器代理，在复杂多步骤任务中表现优于现有方法，并结合MCP工具和FaaS部署方案进行实证评估。


<details>
  <summary>Details</summary>
Motivation: 当前智能体系统在处理多工具、复杂多步骤任务和长上下文管理时存在困难，需要更有效的工作流模式来提升决策生成能力和避免幻觉问题。

Method: 提出AgentX工作流模式，包含阶段设计器、规划器和执行器三个代理组件，利用Model Context Protocol工具，并提出了两种基于FaaS的MCP服务器部署方案。

Result: 通过三个实际应用的实证评估，AgentX在成功率、延迟和成本方面都优于或与最先进的ReAct和Magentic One模式相当。

Conclusion: AgentX模式为设计和部署智能体工作流提供了有效解决方案，展示了在复杂任务处理中的优势，同时也揭示了相关机遇和挑战。

Abstract: Generative Artificial Intelligence (GenAI) has rapidly transformed various
fields including code generation, text summarization, image generation and so
on. Agentic AI is a recent evolution that further advances this by coupling the
decision making and generative capabilities of LLMs with actions that can be
performed using tools. While seemingly powerful, Agentic systems often struggle
when faced with numerous tools, complex multi-step tasks,and long-context
management to track history and avoid hallucinations. Workflow patterns such as
Chain-of-Thought (CoT) and ReAct help address this. Here, we define a novel
agentic workflow pattern, AgentX, composed of stage designer, planner, and
executor agents that is competitive or better than the state-of-the-art agentic
patterns. We also leverage Model Context Protocol (MCP) tools, and propose two
alternative approaches for deploying MCP servers as cloud Functions as a
Service (FaaS). We empirically evaluate the success rate, latency and cost for
AgentX and two contemporary agentic patterns, ReAct and Magentic One, using
these the FaaS and local MCP server alternatives for three practical
applications. This highlights the opportunities and challenges of designing and
deploying agentic workflows.

</details>


### [145] [Scaling atomic ordering in shared memory](https://arxiv.org/abs/2509.07781)
*Lorenzo Martignetti,Eliã Batista,Gianpaolo Cugola,Fernando Pedone*

Main category: cs.DC

TL;DR: TRAM是一个专为共享内存系统设计的原子多播协议，采用覆盖树架构，性能显著优于现有协议


<details>
  <summary>Details</summary>
Motivation: 原子多播是可靠系统中的关键通信原语，但现有协议主要针对消息传递系统，共享内存系统的高性能原子多播协议较少

Method: 利用覆盖树架构设计共享内存原子多播协议TRAM，采用简单实用的设计

Result: 相比最先进的共享内存协议，吞吐量提升3倍以上，延迟降低2.3倍以上；相比消息传递协议，吞吐量提升5.9倍，延迟降低106倍

Conclusion: TRAM协议在共享内存系统中实现了卓越的性能表现，为集成复制和分片的关键服务提供了高效的原子多播解决方案

Abstract: Atomic multicast is a communication primitive used in dependable systems to
ensure consistent ordering of messages delivered to a set of replica groups.
This primitive enables critical services to integrate replication and sharding
(i.e., state partitioning) to achieve fault tolerance and scalability. While
several atomic multicast protocols have been developed for message-passing
systems, only a few are designed for the shared memory system model. This paper
introduces TRAM, an atomic multicast protocol specifically designed for shared
memory systems, leveraging an overlay tree architecture. Due to its simple and
practical design, TRAM delivers exceptional performance, increasing throughput
by more than 3$\times$ and reducing latency by more than 2.3$\times$ compared
to state-of-the-art shared memory-based protocols. Additionally, it
significantly outperforms message-passing-based protocols, boosting throughput
by up to 5.9$\times$ and reducing latency by up to 106$\times$.

</details>
